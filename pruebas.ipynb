{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\roger\\\\Projects\\\\sgbd1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. [Teoria] Introduccion.pdf', 'app.py', 'pruebas.ipynb']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import walk\n",
    "\n",
    "f = []\n",
    "for (dirpath, dirnames, filenames) in walk(os.getcwd()):\n",
    "    f.extend(filenames)\n",
    "    break\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"\\n\", \" \")\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "\n",
    "def convert_pdf_to_text(doc_pdf,id):\n",
    "    from io import StringIO\n",
    "    from pdfminer.converter import TextConverter\n",
    "    from pdfminer.layout import LAParams\n",
    "    from pdfminer.pdfdocument import PDFDocument\n",
    "    from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "    from pdfminer.pdfpage import PDFPage\n",
    "    from pdfminer.pdfparser import PDFParser\n",
    "\n",
    "    output_string = StringIO()\n",
    "    with open(doc_pdf, 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    with open((\"archivos_txt/file-text-\" + str(id) + \".txt\"), \"w\") as txtfile:\n",
    "        print(\"String Variable: {}\".format(normalize(output_string.getvalue().lower())), file=txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u03c9' in position 3261: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\roger\\Projects\\sgbd1\\pruebas.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/roger/Projects/sgbd1/pruebas.ipynb#ch0000004?line=0'>1</a>\u001b[0m convert_pdf_to_text(\u001b[39m'\u001b[39;49m\u001b[39m1. [Teoria] Introduccion.pdf\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m5242\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\roger\\Projects\\sgbd1\\pruebas.ipynb Cell 5\u001b[0m in \u001b[0;36mconvert_pdf_to_text\u001b[1;34m(doc_pdf, id)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/roger/Projects/sgbd1/pruebas.ipynb#ch0000004?line=30'>31</a>\u001b[0m         interpreter\u001b[39m.\u001b[39mprocess_page(page)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/roger/Projects/sgbd1/pruebas.ipynb#ch0000004?line=31'>32</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m((\u001b[39m\"\u001b[39m\u001b[39marchivos_txt/file-text-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m txtfile:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/roger/Projects/sgbd1/pruebas.ipynb#ch0000004?line=32'>33</a>\u001b[0m     \u001b[39mprint\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mString Variable: \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(normalize(output_string\u001b[39m.\u001b[39;49mgetvalue()\u001b[39m.\u001b[39;49mlower())), file\u001b[39m=\u001b[39;49mtxtfile)\n",
      "File \u001b[1;32mc:\\Users\\roger\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:19\u001b[0m, in \u001b[0;36mIncrementalEncoder.encode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_encode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,encoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\u03c9' in position 3261: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "convert_pdf_to_text('1. [Teoria] Introduccion.pdf',5242)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/a73/a110/a116/a114/a111 /a100/a117/a99/a99/a105/a243/a110\n",
      "/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n",
      "/a68/a105/a115/a116/a114/a105/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114\n",
      "/a68/a105/a115/a116/a114/a105/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114\n",
      "/a69/a106/a101/a109/a112/a108/a111 /a52/a58 /a68/a105/a115/a116/a114/a105/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114 /a109/a111 /a100/a101/a108/a111 /a98/a105/a110/a111/a109/a105/a97/a108\n",
      "/a67/a111/a110/a115/a105/a100/a101/a114/a101 /a101/a108 /a109/a111 /a100/a101/a108/a111 /a98/a105/a110/a111/a109/a105/a97/a108 /a100/a101/a108 /a101/a106/a101/a109/a112/a108/a111 /a50/a46 /a72/a97/a108/a108/a97 /a114 /a108/a97 /a100/a105/a115/a116/a114/a105/a45\n",
      "/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114 /a112/a97 /a114/a97 ~y=eyi /a44 /a117/a110 /a110/a117/a101/a118/a111 /a101/a110/a115/a97 /a121 /a111 /a100/a101\n",
      "/a66/a101/a114/a110/a111/a117/a108/a108/a105/a46\n",
      "/a69/a106/a101/a109/a112/a108/a111 /a53/a58 /a68/a105/a115/a116/a114/a105/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114 /a109/a111 /a100/a101/a108/a111 /a100/a101 /a80 /a111/a105/a115/a115/a111/a110\n",
      "/a67/a111/a110/a115/a105/a100/a101/a114/a101 /a101/a108 /a109/a111 /a100/a101/a108/a111 /a100/a101 /a80 /a111/a105/a115/a115/a111/a110 /a100/a101/a108 /a101/a106/a101/a109/a112/a108/a111 /a51/a46 /a72/a97/a108/a108/a97 /a114 /a108/a97 /a100/a105/a115/a116/a114/a105/a45\n",
      "/a98/a117/a99/a105/a243/a110 /a112 /a114/a101/a100/a105/a99/a116/a105/a118/a97 /a112 /a111/a115/a116/a101/a114/a105/a111 /a114 /a112/a97 /a114/a97 ~y=Pm\n",
      "i= /a49eyi /a44 /a101/a108 /a110/a250/a109/a101/a114/a111 /a116/a111/a116/a97/a108\n",
      "/a100/a101 /a101/a118/a101/a110/a116/a111/a115 /a100/a101 /a105/a110/a116/a101/a114/a233/a115 /a101/a110 /a117/a110/a97 /a110/a117/a101/a118/a97 /a109/a117/a101/a115/a116/a114/a97 ey /a49;\u0001\u0001\u0001;eym /a46\n",
      "/a68/a114 /a67/a97 /a114/a108/a111/a115 /a76/a243/a112 /a101/a122 /a100/a101 /a67/a97/a115/a116/a105/a108/a108/a97 /a86/a225/a115/a113/a117/a101/a122 /a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdffileobj=open('1. [Teoria] Introduccion.pdf','rb')\n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    "x=pdfreader.numPages\n",
    "pageobj=pdfreader.getPage(x-1)\n",
    "text=pageobj.extractText()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "/a73/a110/a116/a114/a111 /a100/a117/a99/a99/a105/a243/a110\n",
      "/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n",
      "/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n",
      "/a73/a110/a116/a114/a111 /a100/a117/a99/a99/a105/a243/a110\n",
      "/a68/a114 /a67/a97 /a114/a108/a111/a115 /a76/a243/a112 /a101/a122 /a100/a101 /a67/a97/a115/a116/a105/a108/a108/a97 /a86/a225/a115/a113/a117/a101/a122\n",
      "/a85/a110/a105/a118/a101/a114/a115/a105/a100/a97/a100 /a78/a97/a99/a105/a111/a110/a97/a108 /a65/a103/a114/a97 /a114/a105/a97 /a76/a97 /a77/a111/a108/a105/a110/a97\n",
      "/a50/a48/a50/a50/a45/a49\n",
      "/a68/a114 /a67/a97 /a114/a108/a111/a115 /a76/a243/a112 /a101/a122 /a100/a101 /a67/a97/a115/a116/a105/a108/a108/a97 /a86/a225/a115/a113/a117/a101/a122 /a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n"
     ]
    }
   ],
   "source": [
    "# importing required modules \n",
    "import PyPDF2 \n",
    "    \n",
    "# creating a pdf file object \n",
    "pdfFileObj = open('1. [Teoria] Introduccion.pdf', 'rb') \n",
    "    \n",
    "# creating a pdf reader object \n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj) \n",
    "    \n",
    "# printing number of pages in pdf file \n",
    "print(pdfReader.numPages) \n",
    "    \n",
    "# creating a page object \n",
    "pageObj = pdfReader.getPage(0) \n",
    "    \n",
    "# extracting text from page \n",
    "print(pageObj.extractText()) \n",
    "    \n",
    "# closing the pdf file object \n",
    "pdfFileObj.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/a73/a110/a116/a114/a111 /a100/a117/a99/a99/a105/a243/a110\n",
      "/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n",
      "/a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n",
      "/a73/a110/a116/a114/a111 /a100/a117/a99/a99/a105/a243/a110\n",
      "/a68/a114 /a67/a97 /a114/a108/a111/a115 /a76/a243/a112 /a101/a122 /a100/a101 /a67/a97/a115/a116/a105/a108/a108/a97 /a86/a225/a115/a113/a117/a101/a122\n",
      "/a85/a110/a105/a118/a101/a114/a115/a105/a100/a97/a100 /a78/a97/a99/a105/a111/a110/a97/a108 /a65/a103/a114/a97 /a114/a105/a97 /a76/a97 /a77/a111/a108/a105/a110/a97\n",
      "/a50/a48/a50/a50/a45/a49\n",
      "/a68/a114 /a67/a97 /a114/a108/a111/a115 /a76/a243/a112 /a101/a122 /a100/a101 /a67/a97/a115/a116/a105/a108/a108/a97 /a86/a225/a115/a113/a117/a101/a122 /a69/a115/a116/a97/a100/a237/a115/a116/a105/a99/a97 /a66/a97 /a121 /a101/a115/a105/a97/a110/a97\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "fhandle = open(r'1. [Teoria] Introduccion.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(fhandle)\n",
    "pagehandle = pdfReader.getPage(0)\n",
    "print(pagehandle.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scott M. Lynch\n",
      "\n",
      "Introduction to Applied Bayesian\n",
      "Statistics and Estimation for\n",
      "Social Scientists\n",
      "\n",
      "c©2006 SPRINGER SCIENCE+BUSINESS MEDIA, LLC. All rights\n",
      "\n",
      "reserved. No part of this work may be reproduced in any form without the written\n",
      "\n",
      "permission of SPRINGER SCIENCE+BUSINESS MEDIA, LLC.\n",
      "\n",
      "April 24, 2007\n",
      "\n",
      "Springer\n",
      "\n",
      "Berlin Heidelberg NewYork\n",
      "HongKong London\n",
      "Milan Paris Tokyo\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "For my Barbara\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Preface\n",
      "\n",
      "This book was written slowly over the course of the last five years. During\n",
      "that time, a number of advances have been made in Bayesian statistics and\n",
      "Markov chain Monte Carlo (MCMC) methods, but, in my opinion, the market\n",
      "still lacks a truly introductory book written explicitly for social scientists\n",
      "that thoroughly describes the actual process of Bayesian analysis using these\n",
      "methods. To be sure, a variety of introductory books are available that cover\n",
      "the basics of the Bayesian approach to statistics (e.g., Gill 2002 and Gelman\n",
      "et al. 1995) and several that cover the foundation of MCMC methods (e.g.,\n",
      "beginning with Gilks et al. 1996). Yet, a highly applied book showing how to\n",
      "use MCMC methods to complete a Bayesian analysis involving typical social\n",
      "science models applied to typical social science data is still sorely lacking. The\n",
      "goal of this book is to fill this niche.\n",
      "\n",
      "The Bayesian approach to statistics has a long history in the discipline of\n",
      "statistics, but prior to the 1990s, it held a marginal, almost cult-like status in\n",
      "the discipline and was almost unheard of in social science methodology. The\n",
      "primary reasons for the marginal status of the Bayesian approach include (1)\n",
      "philosophical opposition to the use of “prior distributions” in particular and\n",
      "the subjective approach to probability in general, and (2) the lack of com-\n",
      "puting power for completing realistic Bayesian analyses. In the 1990s, several\n",
      "events occurred simultaneously to overcome these concerns. First, the explo-\n",
      "sion in computing power nullified the second limitation of conducting Bayesian\n",
      "analyses, especially with the development of sampling based methods (e.g.,\n",
      "MCMC methods) for estimating parameters of Bayesian models. Second, the\n",
      "growth in availability of longitudinal (panel) data and the rise in the use of\n",
      "hierarchical modeling made the Bayesian approach more appealing, because\n",
      "Bayesian statistics offers a natural approach to constructing hierarchical mod-\n",
      "els. Third, there has been a growing recognition both that the enterprise of\n",
      "statistics is a subjective process in general and that the use of prior distribu-\n",
      "tions need not influence results substantially. Additionally, in many problems,\n",
      "the use of a prior distribution turns out to be advantageous.\n",
      "\n",
      "\n",
      "\n",
      "viii Preface\n",
      "\n",
      "The publication of Gelfand and Smith’s 1990 paper describing the use of\n",
      "MCMC simulation methods for summarizing Bayesian posterior distributions\n",
      "was the watershed event that launched MCMC methods into popularity in\n",
      "statistics. Following relatively closely on the heels of this article, Gelman et\n",
      "al.’s (1995) book, Bayesian Data Analysis, and Gilks et al.’s (1996) book,\n",
      "Markov Chain Monte Carlo in Practice, placed the Bayesian approach in\n",
      "general, and the application of MCMC methods to Bayesian statistical models,\n",
      "squarely in the mainstream of statistics. I consider these books to be classics\n",
      "in the field and rely heavily on them throughout this book.\n",
      "\n",
      "Since the mid-1990s, there has been an explosion in advances in Bayesian\n",
      "statistics and especially MCMC methodology. Many improvements in the re-\n",
      "cent past have been in terms of (1) monitoring and improving the performance\n",
      "of MCMC algorithms and (2) the development of more refined and complex\n",
      "Bayesian models and MCMC algorithms tailored to specific problems. These\n",
      "advances have largely escaped mainstream social science.\n",
      "\n",
      "In my view, these advances have gone largely unnoticed in social science,\n",
      "because purported introductory books on Bayesian statistics and MCMC\n",
      "methods are not truly introductory for this audience. First, the mathematics\n",
      "in introductory books is often too advanced for a mainstream social science\n",
      "audience, which begs the question: “introductory for whom?” Many social sci-\n",
      "entists do not have the probability theory and mathematical statistics back-\n",
      "ground to follow many of these books beyond the first chapter. This is not to\n",
      "say that the material is impossible to follow, only that more detail may be\n",
      "needed to make the text and examples more readable for a mainstream social\n",
      "science audience.\n",
      "\n",
      "Second, many examples in introductory-level Bayesian books are at best\n",
      "foreign and at worst irrelevant to social scientists. The probability distribu-\n",
      "tions that are used in many examples are not typical probability distributions\n",
      "used by social scientists (e.g., Cauchy), and the data sets that are used in ex-\n",
      "amples are often atypical of social science data. Specifically, many books use\n",
      "small data sets with a limited number of covariates, and many of the models\n",
      "are not typical of the regression-based approaches used in social science re-\n",
      "search. This fact may not seem problematic until, for example, one is faced\n",
      "with a research question requiring a multivariate regression model for 10,000\n",
      "observations measured on 5 outcomes with 10 or more covariates. Nonethe-\n",
      "less, research questions involving large-scale data sets are not uncommon in\n",
      "social science research, and methods shown that handle a sample of size 100\n",
      "measured on one or two outcomes with a couple of covariates simply may not\n",
      "be directly transferrable to a larger data set context. In such cases, the ana-\n",
      "lyst without a solid understanding of the linkage between the model and the\n",
      "estimation routine may be unable to complete the analysis. Thus, some dis-\n",
      "cussion tailored to the practicalities of real social science data and computing\n",
      "is warranted.\n",
      "\n",
      "Third, there seems to be a disjunction between introductory books on\n",
      "Bayesian theory and introductory books on applied Bayesian statistics. One\n",
      "\n",
      "\n",
      "\n",
      "Preface ix\n",
      "\n",
      "of the greatest frustrations for me, while I was learning the basics of Bayesian\n",
      "statistics and MCMC estimation methods, was (and is) the lack of a book\n",
      "that links the theoretical aspects of Bayesian statistics and model develop-\n",
      "ment with the application of modern estimation methods. Some examples in\n",
      "extant books may be substantively interesting, but they are often incomplete\n",
      "in the sense that discussion is truncated after model development without\n",
      "adequate guidance regarding how to estimate parameters. Often, suggestions\n",
      "are made concerning how to go about implementing only certain aspects of\n",
      "an estimation routine, but for a person with no experience doing this, these\n",
      "suggestions are not enough.\n",
      "\n",
      "In an attempt to remedy these issues, this book takes a step back from\n",
      "the most recent advances in Bayesian statistics and MCMC methods and tries\n",
      "to bridge the gap between Bayesian theory and modern Bayesian estimation\n",
      "methods, as well as to bridge the gap between Bayesian statistics books writ-\n",
      "ten as “introductory” texts for statisticians and the needs of a mainstream\n",
      "social science audience. To accomplish this goal, this book presents very little\n",
      "that is new. Indeed, most of the material in this book is now “old-hat” in\n",
      "statistics, and many references are a decade old (In fact, a second edition of\n",
      "Gelman et al.’s 1995 book is now available). However, the trade-off for not pre-\n",
      "senting much new material is that this book explains the process of Bayesian\n",
      "statistics and modern parameter estimation via MCMC simulation methods in\n",
      "great depth. Throughout the book, I painstakingly show the modeling process\n",
      "from model development, through development of an MCMC algorithm to es-\n",
      "timate its parameters, through model evaluation, and through summarization\n",
      "and inference.\n",
      "\n",
      "Although many introductory books begin with the assumption that the\n",
      "reader has a solid grasp of probability theory and mathematical statistics, I\n",
      "do not make that assumption. Instead, this book begins with an exposition of\n",
      "the probability theory needed to gain a solid understanding of the statistical\n",
      "analysis of data. In the early chapters, I use contrived examples applied to\n",
      "(sometimes) contrived data so that the forest is not lost for the trees: The\n",
      "goal is to provide an understanding of the issue at hand rather than to get\n",
      "lost in the idiosyncratic features of real data. In the latter chapters, I show a\n",
      "Bayesian approach (or approaches) to estimating some of the most common\n",
      "models in social science research, including the linear regression model, gen-\n",
      "eralized linear models (specifically, dichotomous and ordinal probit models),\n",
      "hierarchical models, and multivariate models.\n",
      "\n",
      "A consequence of this choice of models is that the parameter estimates\n",
      "obtained via the Bayesian approach are often very consistent with those that\n",
      "could be obtained via a classical approach. This may make a reader ask,\n",
      "“then what’s the point?” First, there are many cases in which a Bayesian\n",
      "approach and a classical approach will not coincide, but from my perspective,\n",
      "an introductory text should establish a foundation that can be built upon,\n",
      "rather than beginning in unfamiliar territory. Second, there are additional\n",
      "benefits to taking a Bayesian approach beyond the simple estimation of model\n",
      "\n",
      "\n",
      "\n",
      "x Preface\n",
      "\n",
      "parameters. Specificially, the Bayesian approach allows for greater flexibility in\n",
      "evaluating model fit, comparing models, producing samples of parameters that\n",
      "are not directly estimated within a model, handling missing data, “tweaking”\n",
      "a model in ways that cannot be done using canned routines in existing software\n",
      "(e.g., freeing or imposing constraints), and making predictions/forecasts that\n",
      "capture greater uncertainty than classical methods. I discuss each of these\n",
      "benefits in the examples throughout the latter chapters.\n",
      "\n",
      "Throughout the book I thoroughly flesh out each example, beginning\n",
      "with the development of the model and continuing through to developing an\n",
      "MCMC algorithm (generally in R) to estimate it, estimating it using the algo-\n",
      "rithm, and presenting and summarizing the results. These programs should be\n",
      "straightforward, albeit perhaps tedious, to replicate, but some programming\n",
      "is inherently required to conduct Bayesian analyses. However, once such pro-\n",
      "gramming skills are learned, they are incredibly freeing to the researcher and\n",
      "thus well worth the investment to acquire them. Ultimately, the point is that\n",
      "the examples are thoroughly detailed; nothing is left to the imagination or\n",
      "to guesswork, including the mathematical contortions of simplifying posterior\n",
      "distributions to make them recognizable as known distributions.\n",
      "\n",
      "A key feature of Bayesian statistics, and a point of contention for oppo-\n",
      "nents, is the use of a prior distribution. Indeed, one of the most complex things\n",
      "about Bayesian statistics is the development of a model that includes a prior\n",
      "and yields a “proper” posterior distribution. In this book, I do not concentrate\n",
      "much effort on developing priors. Often, I use uniform priors on most param-\n",
      "eters in a model, or I use “reference” priors. Both types of priors generally\n",
      "have the effect of producing results roughly comparable with those obtained\n",
      "via maximum likelihood estimation (although not in interpretation!). My goal\n",
      "is not to minimize the importance of choosing appropriate priors, but instead\n",
      "it is not to overcomplicate an introductory exposition of Bayesian statistics\n",
      "and model estimation. The fact is that most Bayesian analyses explicitly at-\n",
      "tempt to minimize the effect of the prior. Most published applications to date\n",
      "have involved using uniform, reference, or otherwise “noninformative” priors\n",
      "in an effort to avoid the “subjectivity” criticism that historically has been\n",
      "levied against Bayesians by classical statisticians. Thus, in most Bayesian so-\n",
      "cial science research, the prior has faded in its importance in differentiating\n",
      "the classical and Bayesian paradigms. This is not to say that prior distribu-\n",
      "tions are unimportant—for some problems they may be very important or\n",
      "useful—but it is to say that it is not necessary to dwell on them.\n",
      "\n",
      "The book consists of a total of 11 chapters plus two appendices covering\n",
      "(1) calculus and matrix algebra and (2) the basic concepts of the Central Limit\n",
      "Theorem. The book is suited for a highly applied one-semester graduate level\n",
      "social science course. Each chapter, including the appendix but excluding\n",
      "the introduction, contains a handful of exercises at the end that test the\n",
      "understanding of the material in the chapter at both theoretical and applied\n",
      "levels. In the exercises, I have traded quantity for quality: There are relatively\n",
      "few exercises, but each one was chosen to address the essential material in\n",
      "\n",
      "\n",
      "\n",
      "Preface xi\n",
      "\n",
      "the chapter. The first half of the book (Chapters 1-6) is primarily theoretical\n",
      "and provides a generic introduction to the theory and methods of Bayesian\n",
      "statistics. These methods are then applied to common social science models\n",
      "and data in the latter half of the book (Chapters 7-11). Chapters 2-4 can\n",
      "each be covered in a week of classes, and much of this material, especially in\n",
      "Chapters 2 and 3, should be review material for most students. Chapters 5\n",
      "and 6 will most likely each require more than a week to cover, as they form\n",
      "the nuts and bolts of MCMC methods and evaluation. Subsequent chapters\n",
      "should each take 1-2 weeks of class time. The models themselves should be\n",
      "familiar, but the estimation of them via MCMC methods will not be and may\n",
      "be difficult for students without some programming and applied data analysis\n",
      "experience. The programming language used throughout the book is R, a\n",
      "freely available and common package used in applied statistics, but I introduce\n",
      "the program WinBugs in the chapter on hierarchical modeling. Overall, R and\n",
      "WinBugs are syntactically similar, and so the introduction of WinBugs is not\n",
      "problematic. From my perspective, the main benefit of WinBugs is that some\n",
      "derivations of conditional distributions that would need to be done in order\n",
      "to write an R program are handled automatically by WinBugs. This feature\n",
      "is especially useful in hierarchical models. All programs used in this book, as\n",
      "well as most data, and hints and/or solutions to the exercises can be found\n",
      "on my Princeton University website at: www.princeton.edu/∼slynch.\n",
      "\n",
      "Acknowledgements\n",
      "\n",
      "I have a number of people to thank for their help during the writing of this\n",
      "book. First, I want to thank German Rodriguez and Bruce Western (both at\n",
      "Princeton) for sharing their advice, guidance, and statistical knowledge with\n",
      "me as I worked through several sections of the book. Second, I thank my\n",
      "friend and colleague J. Scott Brown for reading through virtually all chap-\n",
      "ters and providing much-needed feedback over the course of the last several\n",
      "years. Along these same lines, I thank Chris Wildeman and Steven Shafer for\n",
      "reading through a number of chapters and suggesting ways to improve ex-\n",
      "amples and the general presentation of material. Third, I thank my statistics\n",
      "thesis advisor, Valen Johnson, and my mentor and friend, Ken Bollen, for all\n",
      "that they have taught me about statistics. (They cannot be held responsible\n",
      "for the fact that I may not have learned well, however). For their constant\n",
      "help and tolerance, I thank Wayne Appleton and Bob Jackson, the senior\n",
      "computer folks at Princeton University and Duke University, without whose\n",
      "support this book could not have been possible. For their general support\n",
      "and friendship over a period including, but not limited to, the writing of this\n",
      "book, I thank Linda George, Angie O’Rand, Phil Morgan, Tom Espenshade,\n",
      "Debby Gold, Mark Hayward, Eileen Crimmins, Ken Land, Dan Beirute, Tom\n",
      "Rice, and John Moore. I also thank my son, Tyler, and my wife, Barbara, for\n",
      "listening to me ramble incessantly about statistics and acting as a sounding\n",
      "\n",
      "\n",
      "\n",
      "xii Preface\n",
      "\n",
      "board during the writing of the book. Certainly not least, I thank Bill Mc-\n",
      "Cabe for helping to identify an egregious error on page 364. Finally, I want to\n",
      "thank my editor at Springer, John Kimmel, for his patience and advice, and\n",
      "I acknowledge support from NICHD grant R03HD050374-01 for much of the\n",
      "work in Chapter 10 on multivariate models.\n",
      "\n",
      "Despite having all of these sources of guidance and support, all the errors\n",
      "in the book remain my own.\n",
      "\n",
      "Princeton University Scott M. Lynch\n",
      "April 2007\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii\n",
      "\n",
      "Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xvii\n",
      "\n",
      "List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxv\n",
      "\n",
      "List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .xxviii\n",
      "\n",
      "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n",
      "1.1 Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "1.2 A note on programming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "1.3 Symbols used throughout the book . . . . . . . . . . . . . . . . . . . . . . . . 6\n",
      "\n",
      "2 Probability Theory and Classical Statistics . . . . . . . . . . . . . . . . 9\n",
      "2.1 Rules of probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.2 Probability distributions in general . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "\n",
      "2.2.1 Important quantities in distributions . . . . . . . . . . . . . . . . . 17\n",
      "2.2.2 Multivariate distributions . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "2.2.3 Marginal and conditional distributions . . . . . . . . . . . . . . . 23\n",
      "\n",
      "2.3 Some important distributions in social science . . . . . . . . . . . . . . . 25\n",
      "2.3.1 The binomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n",
      "2.3.2 The multinomial distribution . . . . . . . . . . . . . . . . . . . . . . . 27\n",
      "2.3.3 The Poisson distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n",
      "2.3.4 The normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "2.3.5 The multivariate normal distribution . . . . . . . . . . . . . . . . . 30\n",
      "2.3.6 t and multivariate t distributions . . . . . . . . . . . . . . . . . . . . 33\n",
      "\n",
      "2.4 Classical statistics in social science . . . . . . . . . . . . . . . . . . . . . . . . . 33\n",
      "2.5 Maximum likelihood estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n",
      "\n",
      "2.5.1 Constructing a likelihood function . . . . . . . . . . . . . . . . . . . 36\n",
      "2.5.2 Maximizing a likelihood function . . . . . . . . . . . . . . . . . . . . 38\n",
      "2.5.3 Obtaining standard errors . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "\n",
      "\n",
      "\n",
      "xiv Contents\n",
      "\n",
      "2.5.4 A normal likelihood example . . . . . . . . . . . . . . . . . . . . . . . . 41\n",
      "2.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "2.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "\n",
      "2.7.1 Probability exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n",
      "2.7.2 Classical inference exercises . . . . . . . . . . . . . . . . . . . . . . . . . 45\n",
      "\n",
      "3 Basics of Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n",
      "3.1 Bayes’ Theorem for point probabilities . . . . . . . . . . . . . . . . . . . . . 47\n",
      "3.2 Bayes’ Theorem applied to probability distributions . . . . . . . . . . 50\n",
      "\n",
      "3.2.1 Proportionality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n",
      "3.3 Bayes’ Theorem with distributions: A voting example . . . . . . . . 53\n",
      "\n",
      "3.3.1 Specification of a prior: The beta distribution . . . . . . . . . 54\n",
      "3.3.2 An alternative model for the polling data: A gamma\n",
      "\n",
      "prior/ Poisson likelihood approach . . . . . . . . . . . . . . . . . . . 60\n",
      "3.4 A normal prior–normal likelihood example with σ2 known . . . . 62\n",
      "\n",
      "3.4.1 Extending the normal distribution example . . . . . . . . . . . 65\n",
      "3.5 Some useful prior distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\n",
      "\n",
      "3.5.1 The Dirichlet distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 69\n",
      "3.5.2 The inverse gamma distribution . . . . . . . . . . . . . . . . . . . . . 69\n",
      "3.5.3 Wishart and inverse Wishart distributions . . . . . . . . . . . . 70\n",
      "\n",
      "3.6 Criticism against Bayesian statistics . . . . . . . . . . . . . . . . . . . . . . . 70\n",
      "3.7 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n",
      "3.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n",
      "\n",
      "4 Modern Model Estimation Part 1: Gibbs Sampling . . . . . . . . 77\n",
      "4.1 What Bayesians want and why . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n",
      "4.2 The logic of sampling from posterior densities . . . . . . . . . . . . . . . 78\n",
      "4.3 Two basic sampling methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n",
      "\n",
      "4.3.1 The inversion method of sampling . . . . . . . . . . . . . . . . . . . 81\n",
      "4.3.2 The rejection method of sampling . . . . . . . . . . . . . . . . . . . 84\n",
      "\n",
      "4.4 Introduction to MCMC sampling . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "4.4.1 Generic Gibbs sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\n",
      "4.4.2 Gibbs sampling example using the inversion method . . . 89\n",
      "4.4.3 Example repeated using rejection sampling . . . . . . . . . . . 93\n",
      "4.4.4 Gibbs sampling from a real bivariate density . . . . . . . . . . 96\n",
      "4.4.5 Reversing the process: Sampling the parameters given\n",
      "\n",
      "the data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n",
      "4.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n",
      "4.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n",
      "\n",
      "5 Modern Model Estimation Part 2: Metroplis–Hastings\n",
      "Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n",
      "5.1 A generic MH algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108\n",
      "\n",
      "5.1.1 Relationship between Gibbs and MH sampling . . . . . . . . 113\n",
      "\n",
      "\n",
      "\n",
      "Contents xv\n",
      "\n",
      "5.2 Example: MH sampling when conditional densities are\n",
      "difficult to derive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n",
      "\n",
      "5.3 Example: MH sampling for a conditional density with an\n",
      "unknown form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full\n",
      "multiparameter model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n",
      "5.4.1 The conditionals for µx and µy . . . . . . . . . . . . . . . . . . . . . . 122\n",
      "5.4.2 The conditionals for σ2x, σ\n",
      "\n",
      "2\n",
      "y, and ρ . . . . . . . . . . . . . . . . . . . 123\n",
      "\n",
      "5.4.3 The complete MH algorithm . . . . . . . . . . . . . . . . . . . . . . . . 124\n",
      "5.4.4 A matrix approach to the bivariate normal distribution\n",
      "\n",
      "problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n",
      "5.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n",
      "5.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n",
      "\n",
      "6 Evaluating Markov Chain Monte Carlo (MCMC)\n",
      "Algorithms and Model Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n",
      "6.1 Why evaluate MCMC algorithm performance? . . . . . . . . . . . . . . 132\n",
      "6.2 Some common problems and solutions . . . . . . . . . . . . . . . . . . . . . . 132\n",
      "6.3 Recognizing poor performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "\n",
      "6.3.1 Trace plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n",
      "6.3.2 Acceptance rates of MH algorithms . . . . . . . . . . . . . . . . . . 141\n",
      "6.3.3 Autocorrelation of parameters . . . . . . . . . . . . . . . . . . . . . . 146\n",
      "6.3.4 “R̂” and other calculations . . . . . . . . . . . . . . . . . . . . . . . . . 147\n",
      "\n",
      "6.4 Evaluating model fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n",
      "6.4.1 Residual analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154\n",
      "6.4.2 Posterior predictive distributions . . . . . . . . . . . . . . . . . . . . 155\n",
      "\n",
      "6.5 Formal comparison and combining models . . . . . . . . . . . . . . . . . . 159\n",
      "6.5.1 Bayes factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "6.5.2 Bayesian model averaging . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n",
      "\n",
      "6.6 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n",
      "6.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163\n",
      "\n",
      "7 The Linear Regression Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165\n",
      "7.1 Development of the linear regression model . . . . . . . . . . . . . . . . . 165\n",
      "7.2 Sampling from the posterior distribution for the model\n",
      "\n",
      "parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n",
      "7.2.1 Sampling with an MH algorithm . . . . . . . . . . . . . . . . . . . . 168\n",
      "7.2.2 Sampling the model parameters using Gibbs sampling . . 169\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than others? . . . . . . . 174\n",
      "7.3.1 Results and comparison of the algorithms . . . . . . . . . . . . 175\n",
      "7.3.2 Model evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n",
      "\n",
      "7.4 Incorporating missing data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n",
      "7.4.1 Types of missingness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n",
      "7.4.2 A generic Bayesian approach when data are MAR:\n",
      "\n",
      "The “niceness” example revisited . . . . . . . . . . . . . . . . . . . . 186\n",
      "\n",
      "\n",
      "\n",
      "xvi Contents\n",
      "\n",
      "7.5 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191\n",
      "7.6 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192\n",
      "\n",
      "8 Generalized Linear Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193\n",
      "8.1 The dichotomous probit model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 195\n",
      "\n",
      "8.1.1 Model development and parameter interpretation . . . . . . 195\n",
      "8.1.2 Sampling from the posterior distribution for the model\n",
      "\n",
      "parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n",
      "8.1.3 Simulating from truncated normal distributions . . . . . . . 200\n",
      "8.1.4 Dichotomous probit model example: Black–white\n",
      "\n",
      "differences in mortality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n",
      "8.2 The ordinal probit model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n",
      "\n",
      "8.2.1 Model development and parameter interpretation . . . . . . 218\n",
      "8.2.2 Sampling from the posterior distribution for the\n",
      "\n",
      "parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n",
      "8.2.3 Ordinal probit model example: Black–white differences\n",
      "\n",
      "in health . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n",
      "8.3 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n",
      "8.4 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "\n",
      "9 Introduction to Hierarchical Models . . . . . . . . . . . . . . . . . . . . . . . 231\n",
      "9.1 Hierarchical models in general . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232\n",
      "\n",
      "9.1.1 The voting example redux . . . . . . . . . . . . . . . . . . . . . . . . . . 233\n",
      "9.2 Hierarchical linear regression models . . . . . . . . . . . . . . . . . . . . . . . 240\n",
      "\n",
      "9.2.1 Random effects: The random intercept model . . . . . . . . . 241\n",
      "9.2.2 Random effects: The random coefficient model . . . . . . . . 251\n",
      "9.2.3 Growth models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n",
      "\n",
      "9.3 A note on fixed versus random effects models and other\n",
      "terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\n",
      "\n",
      "9.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268\n",
      "9.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n",
      "\n",
      "10 Introduction to Multivariate Regression Models . . . . . . . . . . . 271\n",
      "10.1 Multivariate linear regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n",
      "\n",
      "10.1.1 Model development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271\n",
      "10.1.2 Implementing the algorithm . . . . . . . . . . . . . . . . . . . . . . . . 275\n",
      "\n",
      "10.2 Multivariate probit models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n",
      "10.2.1 Model development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278\n",
      "10.2.2 Step 2: Simulating draws from truncated multivariate\n",
      "\n",
      "normal distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283\n",
      "10.2.3 Step 3: Simulation of thresholds in the multivariate\n",
      "\n",
      "probit model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n",
      "10.2.4 Step 5: Simulating the error covariance matrix . . . . . . . . 295\n",
      "10.2.5 Implementing the algorithm . . . . . . . . . . . . . . . . . . . . . . . . 297\n",
      "\n",
      "\n",
      "\n",
      "Contents xvii\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of\n",
      "multistate life tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303\n",
      "10.3.1 Model specification and simulation . . . . . . . . . . . . . . . . . . 307\n",
      "10.3.2 Life table generation and other posterior inferences . . . . 310\n",
      "\n",
      "10.4 Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315\n",
      "10.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n",
      "\n",
      "11 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319\n",
      "\n",
      "A Background Mathematics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "A.1 Summary of calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "\n",
      "A.1.1 Limits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323\n",
      "A.1.2 Differential calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n",
      "A.1.3 Integral calculus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n",
      "A.1.4 Finding a general rule for a derivative . . . . . . . . . . . . . . . . 329\n",
      "\n",
      "A.2 Summary of matrix algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n",
      "A.2.1 Matrix notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n",
      "A.2.2 Matrix operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\n",
      "\n",
      "A.3 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "A.3.1 Calculus exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "A.3.2 Matrix algebra exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335\n",
      "\n",
      "B The Central Limit Theorem, Confidence Intervals, and\n",
      "Hypothesis Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "B.1 A simulation study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337\n",
      "B.2 Classical inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338\n",
      "\n",
      "B.2.1 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n",
      "B.2.2 Confidence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n",
      "B.2.3 Some final notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n",
      "\n",
      "References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n",
      "\n",
      "Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Figures\n",
      "\n",
      "2.1 Sample Venn diagram: Outer box is sample space; and circles\n",
      "are events A and B. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "\n",
      "2.2 Two uniform distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "2.3 Histogram of the importance of being able to express\n",
      "\n",
      "unpopular views in a free society (1 = Not very important...6\n",
      "= One of the most important things). . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "\n",
      "2.4 Sample probability density function: A linear density. . . . . . . . . . 17\n",
      "2.5 Sample probability density function: A bivariate plane density. . 20\n",
      "2.6 Three-dimensional bar chart for GSS data with “best” planar\n",
      "\n",
      "density superimposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "2.7 Representation of bivariate cumulative distribution function:\n",
      "\n",
      "Area under bivariate plane density from 0 to 1 in both\n",
      "dimensions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "\n",
      "2.8 Some binomial distributions (with parameter n = 10). . . . . . . . . . 27\n",
      "2.9 Some Poisson distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n",
      "2.10 Some normal distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n",
      "2.11 Two bivariate normal distributions. . . . . . . . . . . . . . . . . . . . . . . . . . 32\n",
      "2.12 The t(0, 1, 1), t(0, 1, 10), and t(0, 1, 120) distributions (with an\n",
      "\n",
      "N(0, 1) distribution superimposed). . . . . . . . . . . . . . . . . . . . . . . . . . 34\n",
      "2.13 Binomial (top) and Bernoulli (bottom) likelihood functions\n",
      "\n",
      "for the OH presidential poll data. . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n",
      "2.14 Finding the MLE: Likelihood and log-likelihood functions for\n",
      "\n",
      "the OH presidential poll data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n",
      "\n",
      "3.1 Three beta distributions with mean α/(α + β) = .5. . . . . . . . . . . 56\n",
      "3.2 Prior, likelihood, and posterior for polling data example: The\n",
      "\n",
      "likelihood function has been normalized as a density for the\n",
      "parameter K. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n",
      "\n",
      "3.3 Posterior for polling data example: A vertical line at K = .5 is\n",
      "included to show the area needed to be computed to estimate\n",
      "the probability that Kerry would win Ohio. . . . . . . . . . . . . . . . . . . 59\n",
      "\n",
      "\n",
      "\n",
      "xx List of Figures\n",
      "\n",
      "3.4 Some examples of the gamma distribution. . . . . . . . . . . . . . . . . . . 61\n",
      "\n",
      "4.1 Convergence of sample means on the true beta distribution\n",
      "mean across samples sizes: Vertical line shows sample size of\n",
      "5,000; dashed horizontal lines show approximate confidence\n",
      "band of sample estimates for samples of size n = 5, 000; and\n",
      "solid horizontal line shows the true mean. . . . . . . . . . . . . . . . . . . . 81\n",
      "\n",
      "4.2 Example of the inversion method: Left-hand figures show the\n",
      "sequence of draws from the U(0, 1) density (upper left) and\n",
      "the sequence of draws from the density f(x) = (1/40)(2x + 3)\n",
      "density (lower left); and the right-hand figures show these\n",
      "draws in histogram format, with true density functions\n",
      "superimposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n",
      "\n",
      "4.3 The three-step process of rejection sampling. . . . . . . . . . . . . . . . . . 86\n",
      "4.4 Sample of 1,000 draws from density using rejection sampling\n",
      "\n",
      "with theoretical density superimposed. . . . . . . . . . . . . . . . . . . . . . . 87\n",
      "4.5 Results of Gibbs sampler using the inversion method for\n",
      "\n",
      "sampling from conditional densities. . . . . . . . . . . . . . . . . . . . . . . . . 92\n",
      "4.6 Results of Gibbs sampler using the inversion method for\n",
      "\n",
      "sampling from conditional densities: Two-dimensional view\n",
      "after 5, 25, 100, and 2,000 iterations. . . . . . . . . . . . . . . . . . . . . . . . . 93\n",
      "\n",
      "4.7 Results of Gibbs sampler using rejection sampling to sample\n",
      "from conditional densities. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n",
      "\n",
      "4.8 Results of Gibbs sampler using rejection sampling to sample\n",
      "from conditional densities: Two-dimensional view after 5, 25,\n",
      "100, and 2,000 iterations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n",
      "\n",
      "4.9 Results of Gibbs sampler for standard bivariate normal\n",
      "distribution with correlation r = .5: Two-dimensional view\n",
      "after 10, 50, 200, and 2,000 iterations. . . . . . . . . . . . . . . . . . . . . . . . 100\n",
      "\n",
      "4.10 Results of Gibbs sampler for standard bivariate normal\n",
      "distribution: Upper left and right graphs show marginal\n",
      "distributions for x and y (last 1,500 iterations); lower left\n",
      "graph shows contour plot of true density; and lower right\n",
      "graph shows contour plot of true density with Gibbs samples\n",
      "superimposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n",
      "\n",
      "4.11 Samples from posterior densities for a mean and variance\n",
      "parameter for NHIS years of schooling data under two Gibbs\n",
      "sampling approaches: The solid lines are the results for the\n",
      "marginal-for-σ2-but conditional-for-µ approach; and the\n",
      "dashed lines are the results for the full conditionals approach. . . 104\n",
      "\n",
      "5.1 Example of symmetric proposals centered over the previous\n",
      "and candidate values of the parameters. . . . . . . . . . . . . . . . . . . . . . 110\n",
      "\n",
      "5.2 Example of asymmetric proposals centered at the mode over\n",
      "the previous and candidate values of the parameters. . . . . . . . . . . 111\n",
      "\n",
      "\n",
      "\n",
      "List of Figures xxi\n",
      "\n",
      "5.3 Example of asymmetry in proposals due to a boundary\n",
      "constraint on the parameter space. . . . . . . . . . . . . . . . . . . . . . . . . . 113\n",
      "\n",
      "5.4 Trace plot and histogram of m parameter from linear density\n",
      "model for the 2000 GSS free speech data. . . . . . . . . . . . . . . . . . . . . 117\n",
      "\n",
      "5.5 Trace plot and histogram of ρ parameter from bivariate\n",
      "normal density model for the 2000 GSS free speech and\n",
      "political participation data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n",
      "\n",
      "5.6 Trace plot and histogram of ρ from bivariate normal model for\n",
      "the 2000 GSS free speech and political participation data. . . . . . 126\n",
      "\n",
      "6.1 Trace plot for first 1,000 iterations of an MH algorithm\n",
      "sampling parameters from planar density for GSS free speech\n",
      "and political participation data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\n",
      "\n",
      "6.2 Trace plot for first 4,000 iterations of an MH algorithm\n",
      "sampling parameters from planar density for GSS free speech\n",
      "and political participation data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137\n",
      "\n",
      "6.3 Trace plot for all 50,000 iterations of an MH algorithm\n",
      "sampling parameters from planar density for GSS free speech\n",
      "and political participation data with means superimposed. . . . . . 138\n",
      "\n",
      "6.4 Trace plot of cumulative and batch means for MH algorithm\n",
      "sampling parameters from planar density for GSS free speech\n",
      "and political participation data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n",
      "\n",
      "6.5 Trace plot of cumulative standard deviation of m2 from MH\n",
      "algorithm sampling parameters from planar density for GSS\n",
      "free speech and political participation data. . . . . . . . . . . . . . . . . . . 140\n",
      "\n",
      "6.6 Posterior density for m2 parameter from planar density MH\n",
      "algorithm and two proposal densities: U(−.0003, .0003) and\n",
      "U(−.003, .003). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n",
      "\n",
      "6.7 Trace plot of the first 1,000 iterations of an MH algorithm for\n",
      "the planar density with a (relatively) broad U(−.003, .003)\n",
      "proposal density. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n",
      "\n",
      "6.8 Posterior density for m2 parameter from planar density MH\n",
      "algorithm and two proposal densities: U(−.003, .003) and\n",
      "N(0, .00085). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n",
      "\n",
      "6.9 Two-dimensional trace plot for initial run of MH algorithm\n",
      "sampling parameters from planar density for GSS free speech\n",
      "and political participation data: Two possible bivariate normal\n",
      "proposal densities are superimposed. . . . . . . . . . . . . . . . . . . . . . . . . 145\n",
      "\n",
      "6.10 Trace plot for 5000 iterations of an MH algorithm sampling\n",
      "parameters from planar density for GSS free speech and\n",
      "political participation data: Bivariate normal proposal density\n",
      "with correlation −.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n",
      "\n",
      "\n",
      "\n",
      "xxii List of Figures\n",
      "\n",
      "6.11 Autocorrelation plots of parameter m2 from MH algorithms\n",
      "for the planar density: Upper figure is for the MH algorithm\n",
      "with independent uniform proposals for m1 and m2; and\n",
      "lower figure is for the MH algorithm with a bivariate normal\n",
      "proposal with correlation −.9. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148\n",
      "\n",
      "6.12 Histogram of marginal posterior density for m2 parameter:\n",
      "Dashed reference line is the posterior mean. . . . . . . . . . . . . . . . . . . 149\n",
      "\n",
      "6.13 Trace plot of first 600 sampled values of ρ from MH algorithms\n",
      "with three different starting values (GSS political participation\n",
      "and free speech data). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n",
      "\n",
      "6.14 Two-dimensional trace plot of sampled values of σ2x and σ\n",
      "2\n",
      "y\n",
      "\n",
      "from MH algorithms with three different starting values (GSS\n",
      "political participation and free speech data). . . . . . . . . . . . . . . . . . 152\n",
      "\n",
      "6.15 Trace plot of the scale reduction factor R̂ for ρ, σ2x, and σ\n",
      "2\n",
      "y\n",
      "\n",
      "across the first 500 iterations of the MH algorithms. . . . . . . . . . . 153\n",
      "6.16 Posterior predictive distributions for the ratio of the mean to\n",
      "\n",
      "the median in the bivariate normal distribution and planar\n",
      "distribution models: Vertical reference line is the observed\n",
      "value in the original data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\n",
      "\n",
      "6.17 Correlations between observed cell counts and posterior\n",
      "predictive distribution cell counts for the bivariate normal and\n",
      "planar distribution models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n",
      "\n",
      "7.1 Scale reduction factors by iteration for all regression\n",
      "parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176\n",
      "\n",
      "7.2 Trace plot of error variance parameter in three different\n",
      "MCMC algorithms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 177\n",
      "\n",
      "7.3 Posterior predictive distributions: (1) The distribution of all\n",
      "replicated samples and the distribution of the original data;\n",
      "(2) the distribution of the ratio of the sample mean of y to\n",
      "the median, with the observed ratio superimposed; and (3)\n",
      "the distribution of the ranges of predicted values with the\n",
      "observed range superimposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180\n",
      "\n",
      "7.4 Posterior predictive distributions (solid lines) and observed\n",
      "values (dashed vertical lines) for persons 1,452 and 1,997. . . . . . . 181\n",
      "\n",
      "7.5 Distribution of age, education, and income for entire sample\n",
      "and for 31 potential outliers identified from posterior\n",
      "predictive simulation (reference lines at means). . . . . . . . . . . . . . . 183\n",
      "\n",
      "7.6 Examples of types of missingness: No missing, missing are not\n",
      "OAR, and missing are not MAR. . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n",
      "\n",
      "8.1 Depiction of simulation from a truncated normal distribution. . . 201\n",
      "8.2 Depiction of rejection sampling from the tail region of a\n",
      "\n",
      "truncated normal distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n",
      "\n",
      "\n",
      "\n",
      "List of Figures xxiii\n",
      "\n",
      "8.3 Trace plot and autocorrelation function (ACF) plot for\n",
      "intercept parameter in dichotomous probit model example\n",
      "(upper plots show samples thinned to every 10th sample; lower\n",
      "plot shows the ACF after thinning to every 24th post-burn-in\n",
      "sample). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n",
      "\n",
      "8.4 Sampled latent health scores for four sample members from\n",
      "the probit model example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n",
      "\n",
      "8.5 Model-predicted traits and actual latent traits for four\n",
      "observations in probit model (solid line = model-predicted\n",
      "latent traits from sample values of β; dashed line = latent\n",
      "traits simulated from the Gibbs sampler). . . . . . . . . . . . . . . . . . . . 212\n",
      "\n",
      "8.6 Latent residuals for four observations in probit model\n",
      "(reference line at 0). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n",
      "\n",
      "8.7 Distribution of black and age-by-black parameters (Cases 2\n",
      "and 3 = both values are positive or negative, respectively;\n",
      "Cases 1 and 4 = values are of opposite signs. Case 4 is the\n",
      "typically-seen/expected pattern). . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n",
      "\n",
      "8.8 Distribution of black-white crossover ages for ages > 0 and <\n",
      "200 (various summary measures superimposed as reference\n",
      "lines). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216\n",
      "\n",
      "8.9 Depiction of latent distribution for Y ∗ and Y with thresholds\n",
      "superimposed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n",
      "\n",
      "8.10 Trace plot for threshold parameters in ordinal probit model. . . . 224\n",
      "8.11 Distributions of latent health scores for five persons with\n",
      "\n",
      "different observed values of y. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n",
      "8.12 Distributions of R2 and pseudo-R2 from OLS and probit\n",
      "\n",
      "models, respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n",
      "\n",
      "9.1 Two-dimensional trace plot of α(0) and α(1) parameters\n",
      "(dashed lines at posterior means for each parameter). . . . . . . . . . 249\n",
      "\n",
      "9.2 Scatterplots of four persons’ random intercepts and slopes\n",
      "from growth curve model of health (posterior means\n",
      "superimposed as horizontal and vertical dashed lines). . . . . . . . . . 261\n",
      "\n",
      "9.3 Predicted trajectories and observed health for four persons:\n",
      "The solid lines are the predicted trajectories based on the\n",
      "posterior means of the random intercepts and slopes from\n",
      "Figure 9.2; and the dashed lines are the predicted trajectories\n",
      "based on the individuals’ covariate profiles and posterior\n",
      "means of the parameters in Table 9.4 . . . . . . . . . . . . . . . . . . . . . . . 265\n",
      "\n",
      "\n",
      "\n",
      "xxiv List of Figures\n",
      "\n",
      "10.1 Depiction of a bivariate outcome space for continuous latent\n",
      "variables Z1 and Z2: The observed variables Y1 and Y2 are\n",
      "formed by the imposition of the vectors of thresholds in each\n",
      "dimension; and the darkened area is the probability that an\n",
      "individual’s response falls in the [2, 3] cell, conditional on the\n",
      "distribution for Z. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282\n",
      "\n",
      "10.2 Comparison of truncated bivariate normal simulation using\n",
      "naive simulation and conditional/decomposition simulation:\n",
      "Upper plots show the true contours (solid lines) of the bivariate\n",
      "normal distribution with sampled values superimposed (dots);\n",
      "and lower plots show histograms of the values sampled from\n",
      "the two dimensions under the two alternative sampling\n",
      "approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n",
      "\n",
      "10.3 Truncated bivariate normal distribution: Marginal distribution\n",
      "for x1 when truncation of x2 is ignored. . . . . . . . . . . . . . . . . . . . . . 288\n",
      "\n",
      "10.4 Comparison of truncated bivariate normal distribution\n",
      "simulation using naive simulation and two iterations of Gibbs\n",
      "sampling: Upper plots show the true contours (solid lines)\n",
      "of the bivariate normal distribution with sampled values\n",
      "superimposed (dots); and lower plots show histograms of\n",
      "the values sampled from the two dimensions under the two\n",
      "alternative approaches to sampling. . . . . . . . . . . . . . . . . . . . . . . . . 290\n",
      "\n",
      "10.5 Gibbs sampling versus Cowles’ algorithm for sampling\n",
      "threshold parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n",
      "\n",
      "10.6 Posterior predictive distributions for the probability a\n",
      "30-year-old white male from the South self-identifies as a\n",
      "conservative Republican across time. . . . . . . . . . . . . . . . . . . . . . . . . 304\n",
      "\n",
      "10.7 Representation of state space for a three state model. . . . . . . . . . 305\n",
      "10.8 Two-dimensional outcome for capturing a three-state state\n",
      "\n",
      "space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306\n",
      "10.9 Trace plots of life table quantities computed from Gibbs\n",
      "\n",
      "samples of bivariate probit model parameters. . . . . . . . . . . . . . . . . 315\n",
      "10.10Histograms of life table quantitities computed from Gibbs\n",
      "\n",
      "samples of bivariate probit model parameters. . . . . . . . . . . . . . . . . 316\n",
      "\n",
      "A.1 Generic depiction of a curve and a tangent line at an arbitrary\n",
      "point. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324\n",
      "\n",
      "A.2 Finding successive approximations to the area under a curve\n",
      "using rectangles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328\n",
      "\n",
      "B.1 Distributions of sample means for four sample sizes (n = 1, 2,\n",
      "30, and 100). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n",
      "\n",
      "B.2 Empirical versus theoretical standard deviations of sample\n",
      "means under the CLT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340\n",
      "\n",
      "B.3 Sampling distributions of variances in the simulation. . . . . . . . . . 341\n",
      "\n",
      "\n",
      "\n",
      "List of Figures xxv\n",
      "\n",
      "B.4 z-based confidence intervals for the mean. . . . . . . . . . . . . . . . . . . . 343\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of Tables\n",
      "\n",
      "1.1 Some Symbols Used Throughout the Text . . . . . . . . . . . . . . . . . . . 7\n",
      "\n",
      "2.1 Cross-tabulation of importance of expressing unpopular views\n",
      "with importance of political participation. . . . . . . . . . . . . . . . . . . . 21\n",
      "\n",
      "3.1 CNN/USAToday/Gallup 2004 presidential election polls. . . . . . . 57\n",
      "\n",
      "4.1 Cell counts and marginals for a hypothetical bivariate\n",
      "dichotomous distribution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n",
      "\n",
      "6.1 Posterior predictive tests for bivariate normal and planar\n",
      "distribution models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n",
      "\n",
      "7.1 Descriptive statistics for variables in OLS regression example\n",
      "(2002 and 2004 GSS data, n = 2, 696). . . . . . . . . . . . . . . . . . . . . . . 175\n",
      "\n",
      "7.2 Results of linear regression of measures of “niceness” on three\n",
      "measures of region. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178\n",
      "\n",
      "7.3 Results of regression of empathy on region with and without\n",
      "missing data: Missing data assumed to be MAR. . . . . . . . . . . . . . 190\n",
      "\n",
      "8.1 Link functions and corrresponding generalized linear models\n",
      "(individual subscripts omitted). . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194\n",
      "\n",
      "8.2 Descriptive statistics for NHANES/NHEFS data used in\n",
      "dichotomous probit model example (baseline n = 3, 201). . . . . . . 207\n",
      "\n",
      "8.3 Gibbs sampling results for dichotomous probit model\n",
      "predicting mortality. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214\n",
      "\n",
      "8.4 Maximum likelihood and Gibbs sampling results for ordinal\n",
      "probit model example. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226\n",
      "\n",
      "8.5 Various summary measures for the black-white health\n",
      "crossover age. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 229\n",
      "\n",
      "\n",
      "\n",
      "xxviii List of Tables\n",
      "\n",
      "9.1 Results of hierarchical model for voting example under\n",
      "different gamma hyperprior specifications. . . . . . . . . . . . . . . . . . . . 240\n",
      "\n",
      "9.2 Results of hierarchical model for two-wave panel of income\n",
      "and Internet use data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n",
      "\n",
      "9.3 Results of “growth” model for two-wave panel of income and\n",
      "Internet use data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n",
      "\n",
      "9.4 Results of growth curve model of health across time. . . . . . . . . . . 262\n",
      "\n",
      "10.1 Results of multivariate regression of measures of “niceness” on\n",
      "three measures of region. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n",
      "\n",
      "10.2 Political orientation and party affiliation. . . . . . . . . . . . . . . . . . . . . 279\n",
      "10.3 Multivariate probit regression model of political orientation\n",
      "\n",
      "and party affiliation on covariates (GSS data, n = 37, 028). . . . . 300\n",
      "10.4 Results of multivariate probit regression of health and\n",
      "\n",
      "mortality on covariates. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310\n",
      "\n",
      "B.1 Percentage of confidence intervals capturing the true mean in\n",
      "simulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343\n",
      "\n",
      "\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "The fundamental goal of statistics is to summarize large amounts of data with\n",
      "a few numbers that provide us with some sort of insight into the process that\n",
      "generated the data we observed. For example, if we were interested in learn-\n",
      "ing about the income of individuals in American society, and we asked 1,000\n",
      "individuals “What is your income?,” we would probably not be interested in\n",
      "reporting the income of all 1,000 persons. Instead, we would more likely be\n",
      "interested in a few numbers that summarized this information—like the mean,\n",
      "median, and variance of income in the sample—and we would want to be able\n",
      "to use these sample summaries to say something about income in the popu-\n",
      "lation. In a nutshell, “statistics” is the process of constructing these sample\n",
      "summaries and using them to infer something about the population, and it\n",
      "is the inverse of probabilistic reasoning. Whereas determining probabilities\n",
      "or frequencies of events—like particular incomes—is a deductive process of\n",
      "computing probabilities given certain parameters of probability distributions\n",
      "(like the mean and variance of a normal distribution), statistical reasoning is\n",
      "an inductive process of “guessing” best choices for parameters, given the data\n",
      "that have been observed, and making some statement about how close our\n",
      "“guess” is to the real population parameters of interest. Bayesian statistics\n",
      "and classical statistics involving maximum likelihood estimation constitute\n",
      "two different approaches to obtaining “guesses” for parameters and for mak-\n",
      "ing inferences about them. This book provides a detailed introduction to the\n",
      "Bayesian approach to statistics and compares and contrasts it with the clas-\n",
      "sical approach under a variety of statistical models commonly used in social\n",
      "science research.\n",
      "\n",
      "Regardless of the approach one takes to statistics, the process of statistics\n",
      "involves (1) formulating a research question, (2) collecting data, (3) developing\n",
      "a probability model for the data, (4) estimating the model, and (5) summa-\n",
      "rizing the results in an appropriate fashion in order to answer the research\n",
      "question—a process often called “statistical inference.” This book generally\n",
      "assumes that a research question has been formulated and that a random\n",
      "sample of data has already been obtained. Therefore, this book focuses on\n",
      "\n",
      "\n",
      "\n",
      "2 1 Introduction\n",
      "\n",
      "model development, estimation, and summarization/inference. Under a clas-\n",
      "sical approach to statistics, model estimation is often performed using canned\n",
      "procedures within statistical software packages like SAS R©, STATA R©, and\n",
      "SPSS R©. Under the Bayesian approach, on the other hand, model estimation\n",
      "is often performed using software/programs that the researcher has developed\n",
      "using more general programming languages like R, C, or C++. Therefore, a\n",
      "substantial portion of this book is devoted to explaining the mechanics of\n",
      "model estimation in a Bayesian context. Although I often use the term “es-\n",
      "timation” throughout the book, the modern Bayesian approach to statistics\n",
      "typically involves simulation of model parameters from their “posterior dis-\n",
      "tributions,” and so “model estimation” is actually a misnomer.\n",
      "\n",
      "In brief, the modern Bayesian approach to model development, estimation,\n",
      "and inference involves the following steps:\n",
      "\n",
      "1. Specification of a “likelihood function” (or “sampling density”) for the\n",
      "data, given the model parameters.\n",
      "\n",
      "2. Specification of a “prior distribution” for the model parameters.\n",
      "3. Derivation of the “posterior distribution” for the model parameters, given\n",
      "\n",
      "the likelihood function and prior distribution.\n",
      "4. Simulation of parameters to obtain a sample from the “posterior distri-\n",
      "\n",
      "bution” of the parameters.\n",
      "5. Summarization of these parameter samples using basic descriptive statis-\n",
      "\n",
      "tical calculations.\n",
      "\n",
      "Although this process and its associated terminology may seem foreign at\n",
      "the moment, the goal of this book is to thoroughly describe and illustrate\n",
      "these steps. The first step—as well as the associated parameter estimation\n",
      "method of maximum likelihood—is perhaps well understood by most quanti-\n",
      "tative researchers in the social sciences. The subsequent steps, on the other\n",
      "hand, are not, especially Step 4. Yet advances in Step 4 have led to the re-\n",
      "cent explosion in the use of Bayesian methods. Specifically, the development\n",
      "of Markov chain Monte Carlo (MCMC) sampling methods, coupled with ex-\n",
      "ponential growth in computing capabilities, has made the use of Bayesian\n",
      "statistics more feasible because of their relative simplicity compared with tra-\n",
      "ditional numerical methods. When approximation methods of estimation were\n",
      "more common, such methods generally relied on normality assumptions and\n",
      "asymptotic arguments for which Bayesians often criticize classical statistics.\n",
      "With the advent of MCMC sampling methods, however, more complicated\n",
      "and realistic applications can be undertaken, and there is no inherent reliance\n",
      "on asymptotic arguments and assumptions. This has allowed the benefits of\n",
      "taking a Bayesian approach over a classical approach to be realized.\n",
      "\n",
      "\n",
      "\n",
      "1.1 Outline 3\n",
      "\n",
      "1.1 Outline\n",
      "\n",
      "For this book, I assume only a familiarity with (1) classical social science\n",
      "statistics and (2) matrix algebra and basic calculus. For those without such\n",
      "a background, or for whom basic concepts from these subjects are not fresh\n",
      "in memory, there are two appendices at the end of the book. Appendix A\n",
      "covers the basic ideas of calculus and matrix algebra needed to understand\n",
      "the concepts of, and notation for, mathematical statistics. Appendix B briefly\n",
      "reviews the Central Limit Theorem and its importance for classical hypothesis\n",
      "testing using a simulation study.\n",
      "\n",
      "The first several chapters of this book lay a foundation for understanding\n",
      "the Bayesian paradigm of statistics and some basic modern methods of esti-\n",
      "mating Bayesian models. Chapter 2 provides a review of (or introduction to)\n",
      "probability theory and probability distributions (see DeGroot 1986, for an ex-\n",
      "cellent background in probability theory; see Billingsley 1995 and Chung and\n",
      "AitSahlia 2003, for more advanced discussion, including coverage of Measure\n",
      "theory). Within this chapter, I develop several simple probability distributions\n",
      "that are used in subsequent chapters as examples before jumping into more\n",
      "complex real-world models. I also discuss a number of real univariate and\n",
      "multivariate distributions that are commonly used in social science research.\n",
      "\n",
      "Chapter 2 also reviews the classical approach to statistical inference from\n",
      "the development of a likelihood function through the steps of estimating the\n",
      "parameters involved in it. Classical statistics is actually a combination of at\n",
      "least two different historical strains in statistics: one involving Fisherian maxi-\n",
      "mum likelihood estimation and the other involving Fisherian and Neyman and\n",
      "Pearsonian hypothesis testing and confidence interval construction (DeGroot\n",
      "1986; Edwards 1992; see Hubbard and Bayarri 2003 for a discussion of the\n",
      "confusion regarding the two approaches). The approach commonly followed\n",
      "today is a hybrid of these traditions, and I lump them both under the term\n",
      "“classical statistics.” This chapter spells out the usual approach to deriving\n",
      "parameter estimates and conducting hypothesis tests under this paradigm.\n",
      "\n",
      "Chapter 3 develops Bayes’ Theorem and discusses the Bayesian paradigm\n",
      "of statistics in depth. Specifically, I spend considerable time discussing the\n",
      "concept of prior distributions, the classical statistical critique of their use,\n",
      "and the Bayesian responses. I begin the chapter with examples that use a\n",
      "point-estimate approach to applying Bayes’ Theorem. Next, I turn to more\n",
      "realistic examples involving probability distributions rather than points es-\n",
      "timates. For these examples, I use real distributions (binomial, poisson, and\n",
      "normal for sampling distributions and beta, gamma, and inverse gamma for\n",
      "prior distributions). Finally, in this chapter, I discuss several additional prob-\n",
      "ability distributions that are not commonly used in social science research but\n",
      "are commonly used as prior distributions by Bayesians.\n",
      "\n",
      "Chapter 4 introduces the rationale for MCMC methods, namely that sam-\n",
      "pling quantities from distributions can help us produce summaries of them\n",
      "that allow us to answer our research questions. The chapter then describes\n",
      "\n",
      "\n",
      "\n",
      "4 1 Introduction\n",
      "\n",
      "some basic methods of sampling from arbitrary distributions and then de-\n",
      "velops the Gibbs sampler as a fundamental method for sampling from high-\n",
      "dimensional distributions that are common in social science research.\n",
      "\n",
      "Chapter 5 introduces an alternative MCMC sampling method that can\n",
      "be used when Gibbs sampling cannot be easily employed: the Metropolis-\n",
      "Hastings algorithm. In both Chapters 4 and 5, I apply these sampling methods\n",
      "to distributions and problems that were used in Chapters 2 and 3 in order to\n",
      "exemplify the complete process of performing a Bayesian analysis up to, but\n",
      "not including, assessing MCMC algorithm performance and evaluating model\n",
      "fit.\n",
      "\n",
      "Chapter 6 completes the exemplification of a Bayesian analysis by showing\n",
      "(1) how to monitor and assess MCMC algorithm performance and (2) how to\n",
      "evaluate model fit and compare models. The first part of the chapter is al-\n",
      "most entirely devoted to technical issues concerning MCMC implementation.\n",
      "A researcher must know that his/her estimation method is performing ac-\n",
      "ceptably, and s/he must know how to use the output to produce appropriate\n",
      "estimates. These issues are generally nonissues for most classical statistical\n",
      "analyses, because generic software exists for most applications. However, they\n",
      "are important issues for Bayesian analyses, which typically involve software\n",
      "that is developed by the researcher him/herself. A benefit to this additional\n",
      "step in the process of analysis—evaluating algorithm performance—is that it\n",
      "requires a much more intimate relationship with the data and model assump-\n",
      "tions than a classical analysis, which may have the potential to lull researchers\n",
      "into a false sense of security about the validity of parameter estimates and\n",
      "model assumptions.\n",
      "\n",
      "The second part of the chapter is largely substantive. All researchers, clas-\n",
      "sical or Bayesian, need to determine whether their models fit the data at hand\n",
      "and whether one model is better than another. I attempt to demonstrate that\n",
      "the Bayesian paradigm offers considerably more information and flexibility\n",
      "than a classical approach in making these determinations. Although I cannot\n",
      "and do not cover all the possibilities, in this part of the chapter, I introduce\n",
      "a number of approaches to consider.\n",
      "\n",
      "The focus of the remaining chapters (7-10) is substantive and applied.\n",
      "These chapters are geared to developing and demonstrating MCMC algo-\n",
      "rithms for specific models that are common in social science research. Chap-\n",
      "ter 7 shows a Bayesian approach to the linear regression model. Chapter 8\n",
      "shows a Bayesian approach to generalized linear models, specifically the di-\n",
      "chotomous and ordinal probit models. Chapter 9 introduces a Bayesian ap-\n",
      "proach to hierarchical models. Finally, Chapter 10 introduces a Bayesian ap-\n",
      "proach to multivariate models. The algorithms developed in these chapters,\n",
      "although fairly generic, should not be considered endpoints for use by re-\n",
      "searchers. Instead, they should be considered as starting points for the devel-\n",
      "opment of algorithms tailored to user-specific problems.\n",
      "\n",
      "In contrast to the use of sometimes contrived examples in the first six\n",
      "chapters, almost all examples in the latter chapters concern real probability\n",
      "\n",
      "\n",
      "\n",
      "1.2 A note on programming 5\n",
      "\n",
      "distributions, real research questions, and real data. To that end, some ad-\n",
      "ditional beneficial aspects of Bayesian analysis are introduced, including the\n",
      "ability to obtain posterior distributions for parameters that are not directly\n",
      "estimated as part of a model, and the ease with which missing data can be\n",
      "handled.\n",
      "\n",
      "1.2 A note on programming\n",
      "\n",
      "Throughout the text, I present R programs for virtually all MCMC algo-\n",
      "rithms in order to demystify the linkage between model development and\n",
      "estimation. R is a freely available, downloadable programming package and is\n",
      "extremely well suited to Bayesian analyses (www.r-project.org). However, R\n",
      "is only one possible programming language in which MCMC algorithms can\n",
      "be written. Another package I use in the chapter on hierarchical modeling is\n",
      "WinBugs. WinBugs is a freely available, downloadable software package that\n",
      "performs Gibbs sampling with relative ease (www.mrc-bsu.cam.ac.uk/bugs).\n",
      "I strongly suggest learning how to use WinBugs if you expect to routinely\n",
      "conduct Bayesian analyses. The syntax of WinBugs is very similar to R, and\n",
      "so the learning curve is not steep once R is familiar. The key advantage to\n",
      "WinBugs over R is that WinBugs derives conditional distributions for Gibbs\n",
      "sampling for you; the user simply has to specify the model. In R, on the other\n",
      "hand, the conditional distributions must be derived mathematically by the\n",
      "user and then programmed. The key advantage of R over WinBugs, however,\n",
      "is that R—as a generic programming language—affords the user greater flex-\n",
      "ibility in reading data from files, modeling data, and writing output to files.\n",
      "For learning how to program in R, I recommend downloading the various\n",
      "documentation available when you download the software. I also recommend\n",
      "Venables and Ripley’s books for S and S-Plus R© programming (1999, 2000).\n",
      "The S and S-Plus languages are virtually identical to R, but they are not\n",
      "freely available.\n",
      "\n",
      "I even more strongly recommend learning a generic programming language\n",
      "like C or C++. Although I show R programs throughout the text, I have\n",
      "used UNIX-based C extensively in my own work, because programs tend to\n",
      "run much faster in UNIX-based C than in any other language. First, UNIX\n",
      "systems are generally faster than other systems. Second, C++ is the language\n",
      "in which many software packages are written. Thus, writing a program in a\n",
      "software package’s language when that language itself rests on a foundation\n",
      "in C/C++ makes any algorithm in that language inherently slower than it\n",
      "would be if it were written directly in C/C++.\n",
      "\n",
      "C and C++ are not difficult languages to learn. In fact, if you can pro-\n",
      "gram in R, you can program in C, because the syntax for many commands\n",
      "is close to identical. Furthermore, if you can program in SAS or STATA, you\n",
      "can learn C very easily. The key differences between database programming\n",
      "languages like SAS and generic programming languages like C are in terms\n",
      "\n",
      "\n",
      "\n",
      "6 1 Introduction\n",
      "\n",
      "of how elements in arrays are handled. In C, each element in an array must\n",
      "be handled; in database and statistics programming languages, commands\n",
      "typically apply to an entire column (variable) at once. For example, recoding\n",
      "gender in a database or statistics package requires only a single command\n",
      "that is systematically applied to all observations automatically. In a generic\n",
      "programming language, on the other hand, one has to apply the command\n",
      "repeatedly to every row (usually using a loop). R combines both features, as\n",
      "I exemplify throughout the examples in the text: Elements in arrays may be\n",
      "handled one-at-a-time, or they may be handled all at once.\n",
      "\n",
      "Another difference between generic programming languages like C and\n",
      "database packages is that generic languages do not have many functions built\n",
      "into them. For example, simulating variates from normal distributions in R,\n",
      "SAS, and STATA is easy because these languages have built-in functions that\n",
      "can be used to do so. In a generic language like C, on the other hand, one must\n",
      "either write the function oneself or find a function in an existing library. Once\n",
      "again, although this may seem like a drawback, it takes very little time to\n",
      "amass a collection of functions which can be used in all subsequent programs.\n",
      "\n",
      "If you choose to learn C, I recommend two books: Teach Yourself C in 24\n",
      "Hours (Zhang 1997) and Numerical Recipes in C (Press et al. 2002). Teach\n",
      "Yourself C is easy to read and will show you practically everything you need\n",
      "to know about the language, with the exception of listing all the built-in\n",
      "functions that C-compilers possess. Numerical Recipes provides a number of\n",
      "algorithms/functions for conducting various mathematical operations, such as\n",
      "Cholesky decomposition, matrix inversion, etc.\n",
      "\n",
      "1.3 Symbols used throughout the book\n",
      "\n",
      "A number of mathematical symbols that may be unfamiliar are used through-\n",
      "out this book. The meanings of most symbols are discussed upon their first\n",
      "appearance, but here I provide a nonexhaustive summary table for reference.\n",
      "Parts of this table may not be helpful until certain sections of the book have\n",
      "been read; the table is a summary, and so some expressions/terms are used\n",
      "here but are defined within the text (e.g., density function).\n",
      "\n",
      "As for general notation that is not described elsewhere, I use lowercase\n",
      "letters, generally from the end of the alphabet (e.g., x), to represent random\n",
      "variables and uppercase versions of these letters to represent vectors of random\n",
      "variables or specific values of a random variable (e.g., X). I violate this general\n",
      "rule only in a few cases for the sake of clarity. Greek letters are reserved for\n",
      "distribution parameters (which, from a Bayesian view, can also be viewed as\n",
      "random variables). In some cases, I use p() to represent probabilities; in other\n",
      "cases, for the sake of clarity, I use pr().\n",
      "\n",
      "\n",
      "\n",
      "1.3 Symbols used throughout the book 7\n",
      "\n",
      "Table 1.1. Some Symbols Used Throughout the Text\n",
      "\n",
      "Symbol\n",
      "or Ex-\n",
      "pression\n",
      "\n",
      "Meaning Explanation\n",
      "\n",
      "∀ “for all” Used to summarily define all ele-\n",
      "ments in a set.\n",
      "\n",
      "∈ “in” or “is an element of” Set notation symbol that means\n",
      "that the item to its left is a mem-\n",
      "ber of the set to its right.Pn\n",
      "\n",
      "i=1\n",
      "xi Repeated, discrete summa-\n",
      "\n",
      "tion\n",
      "Shorthand for representing that all\n",
      "elements xi are to be summed to-\n",
      "gether.\n",
      "\n",
      "f(x) Generic continuous func-\n",
      "tion\n",
      "\n",
      "Used to represent a generic function\n",
      "of the random variable x. Mostly\n",
      "used to represent an algebraic prob-\n",
      "ability density function for a contin-\n",
      "uous variable.\n",
      "\n",
      "p(x) Generic discrete function\n",
      "or probability\n",
      "\n",
      "Used to represent a generic function\n",
      "of a discrete random variable x. Also\n",
      "used to represent “the probability of\n",
      "x.” In a discrete sample space, these\n",
      "are equivalent, given that the func-\n",
      "tion yields the probability for a spec-\n",
      "ified value of x.R b\n",
      "\n",
      "a\n",
      "f(x)dx Integration (continuous\n",
      "\n",
      "summation)\n",
      "Calculus symbol that is the contin-\n",
      "uous analog to\n",
      "\n",
      "P\n",
      ". Implies continu-\n",
      "\n",
      "ous summation of the function f(x)\n",
      "over the interval [a, b]. In multiple\n",
      "dimensional problems, multiple inte-\n",
      "gration may be used (e.g.,\n",
      "\n",
      "R b\n",
      "a\n",
      "\n",
      "R d\n",
      "c\n",
      "\n",
      ". . .).\n",
      "See Appendix A.\n",
      "\n",
      "F (x) Cumulative distribution\n",
      "function\n",
      "\n",
      "Used to represent\n",
      "R X\n",
      "−∞ f(x)dx.\n",
      "\n",
      "∝ “is proportional to” Used to indicate that the object\n",
      "to its left is proportional to (dif-\n",
      "fers only by a multiplicative con-\n",
      "stant compared to) the object on its\n",
      "right.\n",
      "\n",
      "∼ “is distributed as” Used to indicate that the random\n",
      "variable or parameter on its left fol-\n",
      "lows the distribution on its right.\n",
      "\n",
      "A⊗B Kronecker product A special type of matrix multiplica-\n",
      "tion in which each element of A is\n",
      "replaced by itself multiplied by the\n",
      "entirety of B : AijB, ∀i, j.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "\n",
      "Probability Theory and Classical Statistics\n",
      "\n",
      "Statistical inference rests on probability theory, and so an in-depth under-\n",
      "standing of the basics of probability theory is necessary for acquiring a con-\n",
      "ceptual foundation for mathematical statistics. First courses in statistics for\n",
      "social scientists, however, often divorce statistics and probability early with\n",
      "the emphasis placed on basic statistical modeling (e.g., linear regression) in\n",
      "the absence of a grounding of these models in probability theory and prob-\n",
      "ability distributions. Thus, in the first part of this chapter, I review some\n",
      "basic concepts and build statistical modeling from probability theory. In the\n",
      "second part of the chapter, I review the classical approach to statistics as it\n",
      "is commonly applied in social science research.\n",
      "\n",
      "2.1 Rules of probability\n",
      "\n",
      "Defining “probability” is a difficult challenge, and there are several approaches\n",
      "for doing so. One approach to defining probability concerns itself with the\n",
      "frequency of events in a long, perhaps infinite, series of trials. From that per-\n",
      "spective, the reason that the probability of achieving a heads on a coin flip is\n",
      "1/2 is that, in an infinite series of trials, we would see heads 50% of the time.\n",
      "This perspective grounds the classical approach to statistical theory and mod-\n",
      "eling. Another perspective on probability defines probability as a subjective\n",
      "representation of uncertainty about events. When we say that the probability\n",
      "of observing heads on a single coin flip is 1/2, we are really making a series\n",
      "of assumptions, including that the coin is fair (i.e., heads and tails are in\n",
      "fact equally likely), and that in prior experience or learning we recognize that\n",
      "heads occurs 50% of the time. This latter understanding of probability grounds\n",
      "Bayesian statistical thinking. From that view, the language and mathematics\n",
      "of probability is the natural language for representing uncertainty, and there\n",
      "are subjective elements that play a role in shaping probabilistic statements.\n",
      "\n",
      "Although these two approaches to understanding probability lead to dif-\n",
      "ferent approaches to statistics, some fundamental axioms of probability are\n",
      "\n",
      "\n",
      "\n",
      "10 2 Probability Theory and Classical Statistics\n",
      "\n",
      "important and agreed upon. We represent the probability that a particular\n",
      "event, E, will occur as p(E). All possible events that can occur in a single trial\n",
      "or experiment constitute a sample space (S), and the sum of the probabilities\n",
      "of all possible events in the sample space is 11:∑\n",
      "\n",
      "∀E∈S\n",
      "\n",
      "p(E) = 1. (2.1)\n",
      "\n",
      "As an example that highlights this terminology, a single coin flip is a\n",
      "trial/experiment with possible events “Heads” and “Tails,” and therefore has\n",
      "a sample space of S = {Heads, Tails}. Assuming the coin is fair, the probabil-\n",
      "ities of each event are 1/2, and—as used in social science—the record of the\n",
      "outcome of the coin-flipping process can be considered a “random variable.”\n",
      "\n",
      "We can extend the idea of the probability of observing one event in one trial\n",
      "(e.g., one head in one coin toss) to multiple trials and events (e.g., two heads\n",
      "in two coin tosses). The probability assigned to multiple events, say A and\n",
      "B, is called a “joint” probability, and we denote joint probabilities using the\n",
      "disjunction symbol from set notation (∩) or commas, so that the probability\n",
      "of observing events A and B is simply p(A,B). When we are interested in the\n",
      "occurrence of event A or event B, we use the union symbol (∪), or simply the\n",
      "word “or”: p(A ∪B) ≡ p(A or B).\n",
      "\n",
      "The “or” in probability is somewhat different than the “or” in common\n",
      "usage. Typically, in English, when we use the word “or,” we are referring\n",
      "to the occurrence of one or another event, but not both. In the language of\n",
      "logic and probability, when we say “or” we are referring to the occurrence of\n",
      "either event or both events. Using a Venn diagram clarifies this concept (see\n",
      "Figure 2.1).\n",
      "\n",
      "In the diagram, the large rectangle denotes the sample space. Circles A\n",
      "and B denote events A and B, respectively. The overlap region denotes the\n",
      "joint probability p(A,B). p(A or B) is the region that is A only, B only, and\n",
      "the disjunction region. A simple rule follows:\n",
      "\n",
      "p(A or B) = p(A) + p(B)− p(A,B). (2.2)\n",
      "\n",
      "p(A,B) is subtracted, because it is added twice when summing p(A) and p(B).\n",
      "There are two important rules for joint probabilities. First:\n",
      "\n",
      "p(A,B) = p(A)p(B) (2.3)\n",
      "\n",
      "iff (if and only if) A and B are independent events. In probability theory,\n",
      "independence means that event A has no bearing on the occurrence of event\n",
      "B. For example, two coin flips are independent events, because the outcome\n",
      "of the first flip has no bearing on the outcome of the second flip. Second, if A\n",
      "and B are not independent, then:\n",
      "\n",
      "1 If the sample space is continuous, then integration, rather than summation, is\n",
      "used. We will discuss this issue in greater depth shortly.\n",
      "\n",
      "\n",
      "\n",
      "2.1 Rules of probability 11\n",
      "\n",
      "●\n",
      "\n",
      "A B\n",
      "\n",
      "A∩B\n",
      "\n",
      "A∪BNot A∪B\n",
      "\n",
      "Fig. 2.1. Sample Venn diagram: Outer box is sample space; and circles are events\n",
      "A and B.\n",
      "\n",
      "p(A,B) = p(A|B)p(B). (2.4)\n",
      "\n",
      "Expressed another way:\n",
      "\n",
      "p(A|B) =\n",
      "p(A,B)\n",
      "p(B)\n",
      "\n",
      ". (2.5)\n",
      "\n",
      "Here, the “|” represents a conditional and is read as “given.” This last rule can\n",
      "be seen via Figure 2.1. p(A|B) refers to the region that contains A, given that\n",
      "we know B is already true. Knowing that B is true implies a reduction in the\n",
      "total sample space from the entire rectangle to the circle B only. Thus, p(A)\n",
      "is reduced to the (A,B) region, given the reduced space B, and p(A|B) is the\n",
      "proportion of the new sample space, B, which includes A. Returning to the\n",
      "rule above, which states p(A,B) = p(A)p(B) iff A and B are independent,\n",
      "if A and B are independent, then knowing B is true in that case does not\n",
      "reduce the sample space. In that case, then p(A|B) = p(A), which leaves us\n",
      "with the first rule.\n",
      "\n",
      "\n",
      "\n",
      "12 2 Probability Theory and Classical Statistics\n",
      "\n",
      "Although we have limited our discussion to two events, these rules gener-\n",
      "alize to more than two events. For example, the probability of observing three\n",
      "independent events A, B, and C, is p(A,B,C) = p(A)p(B)p(C). More gener-\n",
      "ally, the joint probability of n independent events, E1, E2 . . . En, is\n",
      "\n",
      "∏n\n",
      "i=1 p(Ei),\n",
      "\n",
      "where the\n",
      "∏\n",
      "\n",
      "symbol represents repeated multiplication. This result is very\n",
      "useful in statistics in constructing likelihood functions. See DeGroot (1986)\n",
      "for additional generalizations. Surprisingly, with basic generalizations, these\n",
      "basic probability rules are all that are needed to develop the most common\n",
      "probability models that are used in social science statistics.\n",
      "\n",
      "2.2 Probability distributions in general\n",
      "\n",
      "The sample space for a single coin flip is easy to represent using set notation\n",
      "as we did above, because the space consists of only two possible events (heads\n",
      "or tails). Larger sample spaces, like the sample space for 100 coin flips, or\n",
      "the sample space for drawing a random integer between 1 and 1,000,000,\n",
      "however, are more cumbersome to represent using set notation. Consequently,\n",
      "we often use functions to assign probabilities or relative frequencies to all\n",
      "events in a sample space, where these functions contain “parameters” that\n",
      "govern the shape and scale of the curve defined by the function, as well as\n",
      "expressions containing the random variable to which the function applies.\n",
      "These functions are called “probability density functions,” if the events are\n",
      "continuously distributed, or “probability mass functions,” if the events are\n",
      "discretely distributed. By continuous, I mean that all values of a random\n",
      "variable x are possible in some region (like x = 1.2345); by discrete, I mean\n",
      "that only some values of x are possible (like all integers between 1 and 10).\n",
      "These functions are called “density” and “mass” functions because they tell us\n",
      "where the most (and least) likely events are concentrated in a sample space.\n",
      "We often abbreviate both types of functions using “pdf,” and we denote a\n",
      "random variable x that has a particular distribution g(.) using the generic\n",
      "notation: x ∼ g(.), where the “∼” is read “is distributed as,” the g denotes a\n",
      "particular distribution, and the “.” contains the parameters of the distribution\n",
      "g.\n",
      "\n",
      "If x ∼ g(.), then the pdf itself is expressed as f(x) = . . . , where\n",
      "the “. . .” is the particular algebraic function that returns the relative fre-\n",
      "quency/probability associated with each value of x. For example, one of the\n",
      "most common continuous pdfs in statistics is the normal distribution, which\n",
      "has two parameters—a mean (µ) and variance (σ2). If a variable x has prob-\n",
      "abilities/relative frequencies that follow a normal distribution, then we say\n",
      "x ∼ N(µ, σ2), and the pdf is:\n",
      "\n",
      "f(x) =\n",
      "1\n",
      "\n",
      "√\n",
      "2πσ2\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "(x− µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 13\n",
      "\n",
      "We will discuss this particular distribution in considerable detail through-\n",
      "out the book; the point is that the pdf is simply an algebraic function that,\n",
      "given particular values for the parameters µ and σ2, assigns relative frequen-\n",
      "cies for all events x in the sample space.\n",
      "\n",
      "I use the term “relative frequencies” rather than “probabilities” in dis-\n",
      "cussing continuous distributions, because in continuous distributions, techni-\n",
      "cally, 0 probability is associated with any particular value of x. An infinite\n",
      "number of real numbers exist between any two numbers. Given that we com-\n",
      "monly express the probability for an event E as the number of ways E can\n",
      "be realized divided by the number of possible equally likely events that can\n",
      "occur, when the sample space is continuous, the denominator is infinite. The\n",
      "result is that the probability for any particular event is 0. Therefore, instead\n",
      "of discussing the probability of a particular event, we may discuss the proba-\n",
      "bility of observing an event within a specified range. For this reason, we need\n",
      "to define the cumulative distribution function.\n",
      "\n",
      "Formally, we define a “distribution function” or “cumulative distribution\n",
      "function,” often denoted “cdf,” as the sum or integral of a mass or density\n",
      "function from the smallest possible value for x in the sample space to some\n",
      "value X, and we represent the cdf using the uppercase letter or symbol that\n",
      "we used to represent the corresponding pdf. For example, for a continuous pdf\n",
      "f(x), in which x can take all real values (x ∈ R),\n",
      "\n",
      "p(x < X) = F (x < X) =\n",
      "∫ X\n",
      "−∞\n",
      "\n",
      "f(x) dx. (2.6)\n",
      "\n",
      "For a discrete distribution, integration is replaced with summation and\n",
      "the “<” symbol is replaced with “≤,” because some probability is associated\n",
      "with every discrete value of x in the sample space.\n",
      "\n",
      "Virtually any function can be considered a probability density function,\n",
      "so long as the function is real-valued and it integrates (or sums) to 1 over\n",
      "the sample space (the region of allowable values). The latter requirement is\n",
      "necessary in order to keep consistent with the rule stated in the previous\n",
      "section that the sum of all possible events in a sample space equals 1. It is\n",
      "often the case, however, that a given function will not integrate to 1, hence\n",
      "requiring the inclusion of a “normalizing constant” to bring the integral to\n",
      "1. For example, the leading term outside the exponential expression in the\n",
      "normal density function (1/\n",
      "\n",
      "√\n",
      "2πσ2) is a normalizing constant. A normalized\n",
      "\n",
      "density—one that integrates to 1—or a density that can integrate to 1 with\n",
      "an appropriate normalizing constant is called a “proper” density function. In\n",
      "contrast, a density that cannot integrate to 1 (or a finite value), is called “im-\n",
      "proper.” In Bayesian statistics, the propriety of density functions is important,\n",
      "as we will discuss throughout the remainder of the book.\n",
      "\n",
      "Many of the most useful pdfs in social science statistics appear compli-\n",
      "cated, but as a simple first example, suppose we have some random variable\n",
      "x that can take any value in the interval (a, b) with equal probability. This\n",
      "\n",
      "\n",
      "\n",
      "14 2 Probability Theory and Classical Statistics\n",
      "\n",
      "is called a uniform distribution and is commonly denoted as U(a, b), where a\n",
      "and b are the lower and upper bounds of the interval in which x can fall. If\n",
      "x ∼ U(a, b), then\n",
      "\n",
      "f(x) =\n",
      "{\n",
      "\n",
      "c if a < x < b\n",
      "0 otherwise.\n",
      "\n",
      "(2.7)\n",
      "\n",
      "What is c? c is a constant, which shows that any value in the interval (a, b)\n",
      "is equally likely to occur. In other words, regardless of which value of x one\n",
      "chooses, the height of the curve is the same. The constant must be determined\n",
      "so that the area under the curve/line is 1. A little calculus shows that this\n",
      "constant must be 1/(b− a). That is, if:∫ b\n",
      "\n",
      "a\n",
      "\n",
      "c dx = 1,\n",
      "\n",
      "then\n",
      "c x|ba = 1,\n",
      "\n",
      "and so\n",
      "c =\n",
      "\n",
      "1\n",
      "(b− a)\n",
      "\n",
      ".\n",
      "\n",
      "Because the uniform density function does not depend on x, it is a rectangle.\n",
      "Figure 2.2 shows two uniform densities: the U(−1.5, .5) and the U(0, 1) den-\n",
      "sities. Notice that the heights of the two densities differ; they differ because\n",
      "their widths vary, and the total area under the curve must be 1.\n",
      "\n",
      "The uniform distribution is not explicitly used very often in social science\n",
      "research, largely because very few phenomena in the social sciences follow\n",
      "such a distribution. In order for something to follow this distribution, values\n",
      "at the extreme ends of the distribution must occur as often as values in the\n",
      "center, and such simply is not the case with most social science variables.\n",
      "However, the distribution is important in mathematical statistics generally,\n",
      "and Bayesian statistics more specifically, for a couple of reasons. First, random\n",
      "samples from other distributions are generally simulated from draws from uni-\n",
      "form distributions—especially the standard uniform density [U(0, 1)]. Second,\n",
      "uniform distributions are commonly used in Bayesian statistics as priors on\n",
      "parameters when little or no information exists to construct a more informa-\n",
      "tive prior (see subsequent chapters).\n",
      "\n",
      "More often than not, variables of interest in the social sciences follow\n",
      "distributions that are either peaked in the center and taper at the extremes,\n",
      "or they are peaked at one end of the distribution and taper away from that end\n",
      "(i.e., they are skewed). As an example of a simple distribution that exhibits\n",
      "the latter pattern, consider a density in which larger values are linearly more\n",
      "(or less) likely than smaller ones on the interval (r, s):\n",
      "\n",
      "f(x) =\n",
      "{\n",
      "\n",
      "c(mx + b) if r < x < s\n",
      "0 otherwise.\n",
      "\n",
      "(2.8)\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 15\n",
      "\n",
      "−2 −1 0 1 2\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "U(−1.5,.5) density\n",
      "\n",
      "U(0,1) density\n",
      "\n",
      "Fig. 2.2. Two uniform distributions.\n",
      "\n",
      "This density function is a line, with r and s as the left and right bound-\n",
      "aries, respectively. As with the uniform density, c is a constant—a normalizing\n",
      "constant—that must be determined in order for the density to integrate to 1.\n",
      "For this generic linear density, the normalizing constant is (see Exercises):\n",
      "\n",
      "c =\n",
      "2\n",
      "\n",
      "(s− r)[m(s + r) + 2b]\n",
      ".\n",
      "\n",
      "In this density, the relative frequency of any particular value of x depends on\n",
      "x, as well as on the parameters m and b. If m is positive, then larger values\n",
      "of x occur more frequently than smaller values. If m is negative, then smaller\n",
      "values of x occur more frequently than larger values.\n",
      "\n",
      "What type of variable might follow a distribution like this in social sci-\n",
      "ence research? I would argue that many attitudinal items follow this sort of\n",
      "distribution, especially those with ceiling or floor effects. For example, in the\n",
      "2000 General Social Survey (GSS) special topic module on freedom, a ques-\n",
      "tion was asked regarding the belief in the importance of being able to express\n",
      "unpopular views in a democracy. Figure 2.3 shows the histogram of responses\n",
      "for this item with a linear density superimposed. A linear density appears to\n",
      "\n",
      "\n",
      "\n",
      "16 2 Probability Theory and Classical Statistics\n",
      "\n",
      "fit fairly well (of course, the data are discrete, whereas the density function is\n",
      "continuous).\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "Importance of Ability to Express Unpopular Views\n",
      "\n",
      "P\n",
      "e\n",
      "\n",
      "rc\n",
      "e\n",
      "\n",
      "n\n",
      "t\n",
      "\n",
      "Linear Density Fit\n",
      "\n",
      "Fig. 2.3. Histogram of the importance of being able to express unpopular views in\n",
      "a free society (1 = Not very important...6 = One of the most important things).\n",
      "\n",
      "To be sure, we commonly treat such attitudinal items as being normally\n",
      "distributed and model them accordingly, but they may follow a linear distri-\n",
      "bution as well as, or better than, a normal distribution. Ultimately, this is a\n",
      "question we will address later in the book under model evaluation.\n",
      "\n",
      "Figure 2.4 shows a particular, arbitrary case of the linear density in which\n",
      "m = 2, b = 3; the density is bounded on the interval (0, 5); and thus c = 1/40.\n",
      "So:\n",
      "\n",
      "f(x) =\n",
      "{\n",
      "\n",
      "(1/40)(2x + 3) 0 < x < 5\n",
      "0 otherwise.\n",
      "\n",
      "(2.9)\n",
      "\n",
      "Notice that the inclusion of the normalizing constant ultimately alters the\n",
      "slope and intercept if it is distributed through: The slope becomes 1/20 and\n",
      "the intercept becomes 3/40. This change is not a problem, and it highlights\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 17\n",
      "\n",
      "the notion of “relative frequency”: The relative frequency of values of x are\n",
      "unaffected. For example, the ratio of the height of the original function at\n",
      "x = 5 and x = 0 is 13/3, whereas the ratio of the new function at the same\n",
      "values is 13/40\n",
      "\n",
      "3/40\n",
      "= 13/3.\n",
      "\n",
      "−1 0 1 2 3 4 5 6\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "X\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "=\n",
      "(1\n",
      "\n",
      "/4\n",
      "0\n",
      "\n",
      ")(\n",
      "2\n",
      "\n",
      "x+\n",
      "3\n",
      "\n",
      ")\n",
      "\n",
      "Area in here=1\n",
      "\n",
      "Fig. 2.4. Sample probability density function: A linear density.\n",
      "\n",
      "2.2.1 Important quantities in distributions\n",
      "\n",
      "We generally want to summarize information concerning a probability dis-\n",
      "tribution using summary statistics like the mean and variance, and these\n",
      "quantities can be computed from pdfs using integral calculus for continuous\n",
      "distributions and summation for discrete distributions. The mean is defined\n",
      "as:\n",
      "\n",
      "µx =\n",
      "∫\n",
      "\n",
      "x∈S\n",
      "x× f(x)dx, (2.10)\n",
      "\n",
      "if the distribution is continuous, and:\n",
      "\n",
      "\n",
      "\n",
      "18 2 Probability Theory and Classical Statistics\n",
      "\n",
      "µx =\n",
      "∑\n",
      "x∈S\n",
      "\n",
      "x× p(x), (2.11)\n",
      "\n",
      "if the distribution is discrete. The mean is often called the “expectation” or\n",
      "expected value of x and is denoted as E(x). The variance is defined as:\n",
      "\n",
      "σ2x =\n",
      "∫\n",
      "\n",
      "x∈S\n",
      "(x− µx)2 × f(x)dx, (2.12)\n",
      "\n",
      "if the distribution is continuous, and:\n",
      "\n",
      "σ2x =\n",
      "1\n",
      "n\n",
      "\n",
      "∑\n",
      "x∈S\n",
      "\n",
      "(x− µx)2, (2.13)\n",
      "\n",
      "if the distribution is discrete. Using the expectation notation introduced for\n",
      "the mean, the variance is sometimes referred to as E((x − µx)2); in other\n",
      "words, the variance is the expected value of the squared deviation from the\n",
      "mean.2\n",
      "\n",
      "Quantiles, including the median, can also be computed using integral cal-\n",
      "culus. The median of a continuous distribution, for example, is obtained by\n",
      "finding Q that satisfies:\n",
      "\n",
      ".5 =\n",
      "∫ Q\n",
      "−∞\n",
      "\n",
      "f(x)dx. (2.14)\n",
      "\n",
      "Returning to the examples in the previous section, the mean of the U(a, b)\n",
      "distribution is:\n",
      "\n",
      "E(x) = µx =\n",
      "∫ b\n",
      "\n",
      "a\n",
      "\n",
      "x×\n",
      "(\n",
      "\n",
      "1\n",
      "b− a\n",
      "\n",
      ")\n",
      "dx =\n",
      "\n",
      "b + a\n",
      "2\n",
      "\n",
      ",\n",
      "\n",
      "and the variance is:\n",
      "\n",
      "E((x− µx)2) =\n",
      "∫ b\n",
      "\n",
      "a\n",
      "\n",
      "1\n",
      "b− a\n",
      "\n",
      "(x− µx)2 dx =\n",
      "(b− a)2\n",
      "\n",
      "12\n",
      ".\n",
      "\n",
      "For the linear density with the arbitrary parameter values introduced in\n",
      "Equation 2.9 (f(x) = (1/40)(2x + 3)), the mean is:\n",
      "\n",
      "µx =\n",
      "∫ 5\n",
      "\n",
      "0\n",
      "\n",
      "x× (1/40)(2x + 3)dx = (1/240)(4x3 + 9x2)dx\n",
      "∣∣∣∣5\n",
      "0\n",
      "\n",
      "= 3.02.\n",
      "\n",
      "The variance is:\n",
      "\n",
      "2 The sample mean, unlike the population distribution mean shown here, is esti-\n",
      "mated with (n − 1) in the denominator rather than with n. This is a correction\n",
      "factor for the known bias in estimating the population variance from sample data.\n",
      "It becomes less important asymptotically (as n →∞.)\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 19\n",
      "\n",
      "Var(x) =\n",
      "∫ 5\n",
      "\n",
      "0\n",
      "\n",
      "(x− 3.02)2 × (1/40)(2x + 3)dx = 1.81.\n",
      "\n",
      "Finally, the median can be found by solving for Q in:\n",
      "\n",
      ".5 =\n",
      "∫ Q\n",
      "\n",
      "0\n",
      "\n",
      "(1/40)(2x + 3)dx.\n",
      "\n",
      "This yields:\n",
      "20 = Q2 + 3Q,\n",
      "\n",
      "which can be solved using the quadratic formula from algebra. The quadratic\n",
      "formula yields two real roots—3.22 and −6.22—only one of which is within\n",
      "the “support” of the distribution (3.22); that is, only one has a value that\n",
      "falls in the domain of the distribution.\n",
      "\n",
      "In addition to finding particular quantiles of the distribution (like quartile\n",
      "cutpoints, deciles, etc.), we may also like to determine the probability associ-\n",
      "ated with a given range of the variable. For example, in the U(0,1) distribution,\n",
      "what is the probability that a random value drawn from this distribution will\n",
      "fall between .2 and .6? Determining this probability also involves calculus3:\n",
      "\n",
      "p(.2 < x < .6) =\n",
      "∫ .6\n",
      "\n",
      ".2\n",
      "\n",
      "1\n",
      "1− 0\n",
      "\n",
      "dx = x\n",
      "∣∣∣∣.6\n",
      ".2\n",
      "\n",
      "= .4.\n",
      "\n",
      "An alternative, but equivalent, way of conceptualizing probabilities for regions\n",
      "of a density is in terms of the cdf. That is, p(.2 < x < .6) = F (x = .6)−F (x =\n",
      ".2), where F is\n",
      "\n",
      "∫X\n",
      "0\n",
      "\n",
      "f(x)dx [the cumulative distribution function of f(x)].\n",
      "\n",
      "2.2.2 Multivariate distributions\n",
      "\n",
      "In social science research, we routinely need distributions that represent more\n",
      "than one variable simultaneously. For example, factor analysis, structural\n",
      "equation modeling with latent variables, simultaneous equation modeling, as\n",
      "well as other methods require the simultaneous analysis of variables that are\n",
      "thought to be related to one another. Densities that involve more than one\n",
      "random variable are called joint densities, or more commonly, multivariate\n",
      "distributions. For the sake of concreteness, a simple, arbitrary example of\n",
      "such a distribution might be:\n",
      "\n",
      "f(x, y) =\n",
      "{\n",
      "\n",
      "c(2x + 3y + 2) if 0 < x < 2 , 0 < y < 2\n",
      "0 otherwise.\n",
      "\n",
      "(2.15)\n",
      "\n",
      "Here, the x and y are the two dimensions of the random variable, and f(x, y)\n",
      "is the height of the density, given specific values for the two variables. Thus,\n",
      "\n",
      "3 With discrete distributions, calculus is not required, only summation of the rele-\n",
      "vant discrete probabilities.\n",
      "\n",
      "\n",
      "\n",
      "20 2 Probability Theory and Classical Statistics\n",
      "\n",
      "f(x, y) gives us the relative frequency/probability of particular values of x\n",
      "and y. Once again, c is the normalizing constant that ensures the function of\n",
      "x and y is a proper density function (that it integrates to 1). In this example,\n",
      "determining c involves solving a double integral:\n",
      "\n",
      "c\n",
      "\n",
      "∫ 2\n",
      "0\n",
      "\n",
      "∫ 2\n",
      "0\n",
      "\n",
      "(2x + 3y + 2) dx dy = 1.\n",
      "\n",
      "For this distribution, c = 1/28 (find this).\n",
      "Figure 2.5 shows this density in three dimensions. The height of the density\n",
      "\n",
      "represents the relative frequencies of particular pairs of values for x and y. As\n",
      "the figure shows, the density is a partial plane (bounded at 0 and 2 in both x\n",
      "and y dimensions) that is tilted so that larger values of x and y occur more\n",
      "frequently than smaller values. Additionally, the plane inclines more steeply\n",
      "in the y dimension than the x dimension, given the larger slope in the density\n",
      "function.\n",
      "\n",
      "x\n",
      "\n",
      "−0.5\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "2.0\n",
      "\n",
      "2.5\n",
      "\n",
      "y\n",
      "\n",
      "−0.5\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "2.0\n",
      "\n",
      "2.5\n",
      "\n",
      "f(x,z)\n",
      "\n",
      "0.0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "Fig. 2.5. Sample probability density function: A bivariate plane density.\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 21\n",
      "\n",
      "What pair of variables might follow a distribution like this one (albeit\n",
      "with different parameters and domains)? Realistically, we probably would not\n",
      "use this distribution, but some variables might actually follow this sort of\n",
      "pattern. Consider two items from the 2000 GSS topic module on freedom: the\n",
      "one we previously discussed regarding the importance of the ability to express\n",
      "unpopular views in a free society, and another asking respondents to classify\n",
      "the importance of political participation to freedom. Table 2.1 is a cross-\n",
      "tabulation of these two variables. Considered separately, each variable follows\n",
      "a linear density such as discussed earlier. The proportion of individuals in\n",
      "the “Most Important” category for each variable is large, with the proportion\n",
      "diminishing across the remaining categories of the variable. Together, the\n",
      "variables appear to some extent to follow a planar density like the one above.\n",
      "Of course, there are some substantial deviations in places, with two noticeable\n",
      "‘humps’ along the diagonal of the table.\n",
      "\n",
      "Table 2.1. Cross-tabulation of importance of expressing unpopular views with im-\n",
      "portance of political participation.\n",
      "\n",
      "Express Unpopular Views\n",
      "Political\n",
      "\n",
      "Participation 1 2 3 4 5 6\n",
      "\n",
      "1 361 87 39 8 2 2 36%\n",
      "\n",
      "2 109 193 51 13 2 3 27%\n",
      "\n",
      "3 45 91 184 25 4 5 26%\n",
      "\n",
      "4 15 17 35 17 4 2 7%\n",
      "\n",
      "5 10 4 9 5 2 0 2%\n",
      "\n",
      "6 11 9 4 3 1 5 2%\n",
      "\n",
      "40% 29% 23% 5% 1% 1% 100%\n",
      "\n",
      "Note: Data are from the 2000 GSS special topic module on freedom (variables are\n",
      "expunpop and partpol). 1 = One of the most important parts of freedom . . . 6 =\n",
      "Not so important to freedom.\n",
      "\n",
      "Figure 2.6 presents a three-dimensional depiction of these data with an es-\n",
      "timated planar density superimposed. The imposed density follows the general\n",
      "pattern of the data but fits poorly in several places. First, in several places the\n",
      "planar density substantially underestimates the true frequencies (three places\n",
      "along the diagonal). Second, the density tends to substantially overestimate\n",
      "frequencies in the middle of the distribution. Based on these problems, finding\n",
      "an alternative density is warranted. For example, a density with exponential\n",
      "or quadratic components may be desirable in order to allow more rapid de-\n",
      "clines in the expected relative frequencies at higher values of the variables.\n",
      "Furthermore, we may consider using a density that contains a parameter—\n",
      "like a correlation—that captures the relationship between the two variables,\n",
      "given their apparent lack of independence (the “humps” along the diagonal).\n",
      "\n",
      "\n",
      "\n",
      "22 2 Probability Theory and Classical Statistics\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "5\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "5\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".2\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".3\n",
      "5\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Political Participation\n",
      "\n",
      "E\n",
      "xp\n",
      "\n",
      "re\n",
      "ss\n",
      "\n",
      " U\n",
      "n\n",
      "\n",
      "p\n",
      "o\n",
      "\n",
      "p\n",
      "u\n",
      "\n",
      "la\n",
      "r \n",
      "\n",
      "V\n",
      "ie\n",
      "\n",
      "w\n",
      "s\n",
      "\n",
      "f(\n",
      "x,\n",
      "\n",
      "y)\n",
      "\n",
      "●●●●●●\n",
      "\n",
      "●●●●●●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "Fig. 2.6. Three-dimensional bar chart for GSS data with “best” planar density\n",
      "superimposed.\n",
      "\n",
      "In multivariate continuous densities like this planar density, determining\n",
      "the probability that x and y fall in particular regions of the density is deter-\n",
      "mined via integration, just as in univariate densities. That is, the concept of\n",
      "cumulative distribution functions extends to multivariate densities:\n",
      "\n",
      "p(x < X , y < Y ) = F (x, y) =\n",
      "∫ X\n",
      "−∞\n",
      "\n",
      "∫ Y\n",
      "−∞\n",
      "\n",
      "f(x, y) dx dy. (2.16)\n",
      "\n",
      "Considering the planar density with parameters arbitrarily fixed at 2 and 3,\n",
      "for example, the probability that x < 1 and y < 1 is:∫ 1\n",
      "\n",
      "0\n",
      "\n",
      "∫ 1\n",
      "0\n",
      "\n",
      "(1/28)(2x + 3y + 2) dx dy =\n",
      "9\n",
      "56\n",
      "\n",
      ".\n",
      "\n",
      "This region is presented in Figure 2.7, with the shadow of the omitted portion\n",
      "of the density shown on the z = 0 plane.\n",
      "\n",
      "\n",
      "\n",
      "2.2 Probability distributions in general 23\n",
      "\n",
      "x\n",
      "\n",
      "−0.5\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "2.0\n",
      "\n",
      "2.5\n",
      "\n",
      "y\n",
      "\n",
      "−0.5\n",
      "0.0\n",
      "\n",
      "0.5\n",
      "1.0\n",
      "\n",
      "1.5\n",
      "2.0\n",
      "\n",
      "2.5\n",
      "\n",
      "f(x,z)\n",
      "\n",
      "0.0\n",
      "\n",
      "0.1\n",
      "\n",
      "0.2\n",
      "\n",
      "0.3\n",
      "\n",
      "0.4\n",
      "\n",
      "0.5\n",
      "\n",
      "Fig. 2.7. Representation of bivariate cumulative distribution function: Area under\n",
      "bivariate plane density from 0 to 1 in both dimensions.\n",
      "\n",
      "2.2.3 Marginal and conditional distributions\n",
      "\n",
      "Although determining the probabilities for particular regions of multivariate\n",
      "densities is important, we may be interested in only a subset of the dimen-\n",
      "sions of a multivariate density. Two types of “subsets” are frequently needed:\n",
      "marginal distributions and conditional distributions. The data contained in\n",
      "Table 2.1 help differentiate these two types of distributions.\n",
      "\n",
      "The marginal distribution for the “Express unpopular views” item is the\n",
      "row at the bottom of the table: It is the distribution of this variable summing\n",
      "across the categories of the other variable (or integrating, if the density were\n",
      "continuous). The conditional distribution of this item, on the other hand,\n",
      "is the row of the table corresponding to a particular value for the political\n",
      "participation variable. For example, the conditional distribution for expressing\n",
      "unpopular views, given the value of “1” for political participation, consists of\n",
      "the data in the first row of the table (361, 87, 39, 8, 2, and 2, or in renormalized\n",
      "percents: 72%, 17%, 8%, 2%, .4%, and .4%).\n",
      "\n",
      "\n",
      "\n",
      "24 2 Probability Theory and Classical Statistics\n",
      "\n",
      "Thus, we can think of marginal distributions for a variable as being the\n",
      "original distribution “flattened” in one dimension, whereas the conditional\n",
      "distribution for a variable is a “slice” through one dimension.\n",
      "\n",
      "Finding marginal and conditional distributions mathematically is concep-\n",
      "tually straightforward, although often difficult in practice. Although Equa-\n",
      "tion 2.5 was presented in terms of discrete probabilities, the rule also applies\n",
      "to density functions. From Equation 2.5, a conditional distribution can be\n",
      "computed as:\n",
      "\n",
      "f(x|y) =\n",
      "f(x, y)\n",
      "f(y)\n",
      "\n",
      "(2.17)\n",
      "\n",
      "This equation says that the conditional distribution for x given y is equal to\n",
      "the joint density of x and y divided by the marginal distribution for y, where a\n",
      "marginal distribution is the distribution of one variable, integrating/summing\n",
      "over the other variables in the joint density. Thus:\n",
      "\n",
      "f(y) =\n",
      "∫\n",
      "\n",
      "x∈S\n",
      "f(x, y)dx. (2.18)\n",
      "\n",
      "In terms of our bivariate distribution above (f(x, y) = (1/28)(2x+3y+2)),\n",
      "the marginal distributions for x and y can be found as:\n",
      "\n",
      "f(x) =\n",
      "∫ 2\n",
      "\n",
      "y=0\n",
      "\n",
      "(1/28)(2x + 3y + 2)dy = (1/28)(4x + 10)\n",
      "\n",
      "and\n",
      "\n",
      "f(y) =\n",
      "∫ 2\n",
      "\n",
      "x=0\n",
      "\n",
      "(1/28)(2x + 3y + 2)dx = (1/28)(6y + 8).\n",
      "\n",
      "The conditional distributions can then be found as:\n",
      "\n",
      "f(x|y) =\n",
      "(1/28)(2x + 3y + 2)∫ 2\n",
      "x=0\n",
      "\n",
      "(2x + 3y + 2)dx\n",
      "=\n",
      "\n",
      "(1/28)(2x + 3y + 2)\n",
      "(1/28)(6y + 8)\n",
      "\n",
      "and\n",
      "\n",
      "f(y|x) =\n",
      "(1/28)(2x + 3y + 2)∫ 2\n",
      "y=0\n",
      "\n",
      "(2x + 3y + 2)dy\n",
      "=\n",
      "\n",
      "(1/28)(2x + 3y + 2)\n",
      "(1/28)(4x + 10)\n",
      "\n",
      ".\n",
      "\n",
      "Observe how the marginal distributions for each variable exclude the other\n",
      "variable (as they should), whereas the conditional distributions do not. Once a\n",
      "specific value for x or y is chosen in the conditional distribution, however, the\n",
      "remaining function will only depend on the variable of interest. Once again,\n",
      "in other words, the conditional distribution is akin to taking a slice through\n",
      "one dimension of the bivariate distribution.\n",
      "\n",
      "As a final example, take the conditional distribution f(x|y), where y = 0,\n",
      "so that we are looking at the slice of the bivariate distribution that lies on the\n",
      "x axis. The conditional distribution for that slice is:\n",
      "\n",
      "\n",
      "\n",
      "2.3 Some important distributions in social science 25\n",
      "\n",
      "f(x|y = 0) =\n",
      "2x + 3(y = 0) + 2\n",
      "\n",
      "6(y = 0) + 8\n",
      "= (1/8)(2x + 2).\n",
      "\n",
      "With very little effort, it is easy to see that this result gives us the formula\n",
      "for the line that we observe in the x, z plane when we set y = 0 in the original\n",
      "unnormalized function and we exclude the constant 1/8. In other words:\n",
      "\n",
      "(1/8)(2x + 2) ∝ (1/28)(2x + 3y + 2)\n",
      "\n",
      "when y = 0. Thus, an important finding is that the conditional distribution\n",
      "f(x|y) is proportional to the joint distribution for f(x, y) evaluated at a par-\n",
      "ticular value for y [expressed f(x|y) ∝ f(x, y)], differing only by a normalizing\n",
      "constant. This fact will be useful when we discuss Gibbs sampling in Chap-\n",
      "ter 4.\n",
      "\n",
      "2.3 Some important distributions in social science\n",
      "\n",
      "Unlike the relatively simple distributions we developed in the previous sec-\n",
      "tion, the distributions that have been found to be most useful in social science\n",
      "research appear more complicated. However, it should be remembered that,\n",
      "despite their sometimes more complicated appearance, they are simply alge-\n",
      "braic functions that describe the relative frequencies of occurence for partic-\n",
      "ular values of a random variable. In this section, I discuss several of the most\n",
      "important distributions used in social science research. I limit the discussion\n",
      "at this point to distributions that are commonly applied to random variables\n",
      "as social scientists view them. In the next chapter, I discuss some additional\n",
      "distributions that are commonly used in Bayesian statistics as “prior distri-\n",
      "butions” for parameters (which, as we will see, are also treated as random\n",
      "variables by Bayesians). I recommend Evans, Hastings, and Peacock (2000)\n",
      "for learning more about these and other common probability distributions.\n",
      "\n",
      "2.3.1 The binomial distribution\n",
      "\n",
      "The binomial distribution is a common discrete distribution used in social\n",
      "science statistics. This distribution represents the probability for x successes\n",
      "in n trials, given a success probability p for each trial. If x ∼ Bin(n, p), then:\n",
      "\n",
      "pr(x|n, p) =\n",
      "(\n",
      "\n",
      "n\n",
      "x\n",
      "\n",
      ")\n",
      "px(1− p)n−x. (2.19)\n",
      "\n",
      "Here, I change the notation on the left side of the mass function to “pr”\n",
      "to avoid confusion with the parameter p in the function. The combinatorial,(\n",
      "\n",
      "n\n",
      "x\n",
      "\n",
      ")\n",
      ", at the front of the function, compensates for the fact that the x suc-\n",
      "\n",
      "cesses can come in any order in the n trials. For example, if we are interested\n",
      "\n",
      "\n",
      "\n",
      "26 2 Probability Theory and Classical Statistics\n",
      "\n",
      "in the probability of obtaining exactly 10 heads in 50 flips of a fair coin [thus,\n",
      "pr(x = 10 | n = 50, p = .5)], the 10 heads could occur back-to-back, or several\n",
      "may appear in a row, followed by several tails, followed by more heads, etc.\n",
      "This constant is computed as n!/(x!(n − x)!) and acts as a normalizing con-\n",
      "stant to ensure the mass under the curve sums to 1. The latter two terms in\n",
      "the function multiply the independent success and failure probabilities, based\n",
      "on the observed number of successes and failures. Once the parameters n and\n",
      "p are chosen, the probability of observing any number x of successes can be\n",
      "computed/deduced. For example, if we wanted to know the probability of ex-\n",
      "actly x = 10 heads out of n = 50 flips, then we would simply substitute those\n",
      "numbers into the right side of the equation, and the result would tell us the\n",
      "probability. If we wanted to determine the probability of obtaining at least 10\n",
      "heads in 50 flips, we would need to sum the probabilities from 10 successes up\n",
      "to 50 successes. Obviously, in this example, the probability of obtaining more\n",
      "heads than 50 or fewer heads than 0 is 0. Hence, this sample space is bounded\n",
      "to counting integers between 0 and 50, and computing the probability of at\n",
      "least 10 heads would require summing 41 applications of the function (for\n",
      "x = 10, x = 11, ..., x = 50).\n",
      "\n",
      "The mean of the binomial distribution is np, and the variance of the bi-\n",
      "nomial distribution is np(1 − p). When p = .5, the distribution is symmetric\n",
      "around the mean. When p > .5, the distribution is skewed to the left; when\n",
      "p < .5, the distribution is skewed to the right. See Figure 2.8 for an example of\n",
      "the effect of p on the shape of the distribution (n = 10). Note that, although\n",
      "the figure is presented in a histogram format for the purpose of appearance\n",
      "(the densities are presented as lines), the distribution is discrete, and so 0\n",
      "probability is associated with non-integer values of x.\n",
      "\n",
      "A normal approximation to the binomial may be used when p is close to\n",
      ".5 and n is large, by setting µx = np and σx =\n",
      "\n",
      "√\n",
      "np(1− p). For example,\n",
      "\n",
      "in the case mentioned above in which we were interested in computing the\n",
      "probability of obtaining 10 or more heads in a series of 50 coin flips, computing\n",
      "41 probabilities with the function would be tedious. Instead, we could set\n",
      "µx = 25, and σx =\n",
      "\n",
      "√\n",
      "50(.5)(1− .5) = 3.54, and compute a z-score as z =\n",
      "\n",
      "(10−25)/(3.54) = −4.24. Recalling from basic statistics that there is virtually\n",
      "0 probability in the tail of the z distribution to the left of −4.24, we would\n",
      "conclude that the probability of obtaining at least 10 heads is practically 1,\n",
      "using this approximation. In fact, the actual probability of obtaining at least\n",
      "10 heads is .999988.\n",
      "\n",
      "When n = 1, the binomial distribution reduces to another important dis-\n",
      "tribution called the Bernoulli distribution. The binomial distribution is often\n",
      "used in social science statistics as a building block for models for dichotomous\n",
      "outcome variables like whether a Republican or Democrat will win an upcom-\n",
      "ing election, whether an individual will die within a specified period of time,\n",
      "etc.\n",
      "\n",
      "\n",
      "\n",
      "2.3 Some important distributions in social science 27\n",
      "\n",
      "0 2 4 6 8 10\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "x\n",
      "\n",
      "p\n",
      "r(\n",
      "\n",
      "x)\n",
      "\n",
      "Bin(p=.2) Bin(p=.5) Bin(p=.8)\n",
      "\n",
      "Fig. 2.8. Some binomial distributions (with parameter n = 10).\n",
      "\n",
      "2.3.2 The multinomial distribution\n",
      "\n",
      "The multinomial distribution is a generalization of the binomial distribution\n",
      "in which there are more than two outcome categories, and thus, there are\n",
      "more than two “success” probabilities (one for each outcome category). If\n",
      "x ∼ Multinomial(n, p1, p2, . . . , pk), then:\n",
      "\n",
      "pr(x1 . . . xk | n, p1 . . . pk) =\n",
      "n!\n",
      "\n",
      "x1! x2! . . . xk!\n",
      "px11 p\n",
      "\n",
      "x2\n",
      "2 . . . p\n",
      "\n",
      "xk\n",
      "k , (2.20)\n",
      "\n",
      "where the leading combinatorial expression is a normalizing constant,\n",
      "∑k\n",
      "\n",
      "i=1 pi =\n",
      "1, and\n",
      "\n",
      "∑k\n",
      "i=1 xi = n. Whereas the binomial distribution allows us to compute\n",
      "\n",
      "the probability of obtaining a given number of successes (x) out of n trials,\n",
      "given a particular success probability (p), the multinomial distribution allows\n",
      "us to compute the probability of obtaining particular sets of successes, given\n",
      "n trials and given different success probabilities for each member of the set.\n",
      "To make this idea concrete, consider rolling a pair of dice. The sample space\n",
      "\n",
      "\n",
      "\n",
      "28 2 Probability Theory and Classical Statistics\n",
      "\n",
      "for possible outcomes of a single roll is S = {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, and\n",
      "we can consider the number of occurrences in multiple rolls of each of these\n",
      "outcomes to be represented by a particular x (so, x1 represents the number\n",
      "of times a 2 is rolled, x2 represents the number of times a 3 is rolled, etc.).\n",
      "The success probabilities for these possible outcomes vary, given the fact that\n",
      "there are more ways to obtain some sums than others. The vector of prob-\n",
      "abilities p1 . . . p11 is { 136 ,\n",
      "\n",
      "2\n",
      "36\n",
      "\n",
      ", 3\n",
      "36\n",
      "\n",
      ", 4\n",
      "36\n",
      "\n",
      ", 5\n",
      "36\n",
      "\n",
      ", 6\n",
      "36\n",
      "\n",
      ", 5\n",
      "36\n",
      "\n",
      ", 4\n",
      "36\n",
      "\n",
      ", 3\n",
      "36\n",
      "\n",
      ", 2\n",
      "36\n",
      "\n",
      ", 1\n",
      "36\n",
      "}. Suppose we roll\n",
      "\n",
      "the pair of dice 36 times. Then, if we want to know the probability of obtain-\n",
      "ing one “2”, two “3s”, three “4s”, etc., we would simply substitute n = 36,\n",
      "p1 = 136 , p2 =\n",
      "\n",
      "2\n",
      "36\n",
      "\n",
      ", . . . , p11 = 136 , and x1 = 1, x2 = 2, x3 = 3, . . . into the\n",
      "function and compute the probability.\n",
      "\n",
      "The multinomial distribution is often used in social science statistics to\n",
      "model variables with qualitatively different outcomes categories, like religious\n",
      "affiliation, political party affiliation, race, etc, and I will discuss this distribu-\n",
      "tion in more depth in later chapters as a building block of some generalized\n",
      "linear models and some multivariate models.\n",
      "\n",
      "2.3.3 The Poisson distribution\n",
      "\n",
      "The Poisson distribution is another discrete distribution, like the binomial,\n",
      "but instead of providing the probabilities for a particular number of successes\n",
      "out of a given number of trials, it essentially provides the probabilities for a\n",
      "given number of successes in an infinite number of trials. Put another way,\n",
      "the Poisson distribution is a distribution for count variables. If x ∼ Poi(λ),\n",
      "then:\n",
      "\n",
      "p(x|λ) =\n",
      "e−λλx\n",
      "\n",
      "x!\n",
      ". (2.21)\n",
      "\n",
      "Figure 2.9 shows three Poisson distributions, with different values for the λ\n",
      "parameter. When λ is small, the distribution is skewed to the right, with most\n",
      "of the mass concentrated close to 0. As λ increases, the distribution becomes\n",
      "more symmetric and shifts to the right. As with the figure for the binomial\n",
      "distribution above, I have plotted the densities as if they were continuous for\n",
      "the sake of appearance, but because the distribution is discrete, 0 probability\n",
      "is associated with non-integer values of x\n",
      "\n",
      "The Poisson distribution is often used to model count outcome variables,\n",
      "(e.g., numbers of arrests, number of children, etc.), especially those with low\n",
      "expected counts, because the distributions of such variables are often skewed\n",
      "to the right with most values clustered close to 0. The mean and variance of the\n",
      "Poisson distribution are both λ, which is often found to be unrealistic for many\n",
      "count variables, however. Also problematic with the Poisson distribution is\n",
      "the fact that many count variables, such as the number of times an individual\n",
      "is arrested, have a greater frequency of 0 counts than the Poisson density\n",
      "predicts. In such cases, the negative binomial distribution (not discussed here)\n",
      "and mixture distributions (also not discussed) are often used (see Degroot 1986\n",
      "\n",
      "\n",
      "\n",
      "2.3 Some important distributions in social science 29\n",
      "\n",
      "5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "x\n",
      "\n",
      "p\n",
      "r(\n",
      "\n",
      "x \n",
      "| \n",
      "\n",
      "λ)\n",
      "λ=1\n",
      "\n",
      "λ=3\n",
      "\n",
      "λ=8\n",
      "\n",
      "Fig. 2.9. Some Poisson distributions.\n",
      "\n",
      "for the development of the negative binomial distribution; see Long 1997 for\n",
      "a discussion of negative binomial regression modeling; see Land, McCall, and\n",
      "Nagin 1996 for a discussion of the use of Poisson mixture models).\n",
      "\n",
      "2.3.4 The normal distribution\n",
      "\n",
      "The most commonly used distribution in social science statistics and statistics\n",
      "in general is the normal distribution. Many, if not most, variables of interest\n",
      "follow a bell-shaped distribution, and the normal distribution, with both a\n",
      "mean and variance parameter, fits such variables quite well. If x ∼ N(µ, σ2),\n",
      "then:\n",
      "\n",
      "f(x|µ, σ) =\n",
      "1\n",
      "\n",
      "√\n",
      "2πσ2\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "(x− µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ". (2.22)\n",
      "\n",
      "In this density, the preceding (\n",
      "√\n",
      "\n",
      "2πσ2)−1 is included as a normalizing\n",
      "constant so that the area under the curve from −∞ to +∞ integrates to 1.\n",
      "The latter half of the pdf is the “kernel” of the density and gives the curve its\n",
      "\n",
      "\n",
      "\n",
      "30 2 Probability Theory and Classical Statistics\n",
      "\n",
      "location and shape. Given a value for the parameters of the distribution, µ and\n",
      "σ2, the curve shows the relative probabilities for every value of x. In this case,\n",
      "x can range over the entire real line, from −∞ to +∞. Technically, because an\n",
      "infinite number of values exist between any two other values of x (ironically\n",
      "making p(x = X) = 0,∀X), the value returned by the function f(x) does not\n",
      "reveal the probability of x, unlike with the binomial and Poisson distribution\n",
      "above (as well as other discrete distributions). Rather, when using continuous\n",
      "pdfs, one must consider the probability for regions under the curve. Just as\n",
      "above in the discussion of the binomial distribution, where we needed to sum\n",
      "all the probabilities between x = 10 and x = 50 to obtain the probability\n",
      "that x ≥ 10, here we would need to integrate the continuous function from\n",
      "x = a to x = b to obtain the probability that a < x < b. Note that we did\n",
      "not say a ≤ x ≤ b; we did not for the same reason mentioned just above: The\n",
      "probability that x equals any number q is 0 (the area of a line is 0). Hence\n",
      "a < x < b is equivalent to a ≤ x ≤ b.\n",
      "\n",
      "The case in which µ = 0 and σ2 = 1 is called the “standard normal\n",
      "distribution,” and often, the z distribution. In that case, the kernel of the\n",
      "density reduces to exp\n",
      "\n",
      "{\n",
      "−x2/2\n",
      "\n",
      "}\n",
      ", and the bell shape of the distribution can\n",
      "\n",
      "be easily seen. That is, where x = 0, the function value is 1, and as x moves\n",
      "away from 0 in either direction, the function value rapidly declines.\n",
      "\n",
      "Figure 2.10 depicts three different normal distributions: The first has a\n",
      "mean of 0 and a standard deviation of 1; the second has the same mean but\n",
      "a standard deviation of 2; and the third has a standard deviation of 1 but a\n",
      "mean of 3.\n",
      "\n",
      "The normal distribution is used as the foundation for ordinary least squares\n",
      "(OLS) regression, for some generalized linear models, and for many other mod-\n",
      "els in social science statistics. Furthermore, it is an important distribution in\n",
      "statistical theory: The Central Limit Theorem used to justify most of classical\n",
      "statistical testing states that sampling distributions for statistics are, in the\n",
      "limit, normal. Thus, the z distribution is commonly used to assess statistical\n",
      "“significance” within a classical statistics framework. For these reasons, we\n",
      "will consider the normal distribution repeatedly throughout the remainder of\n",
      "the book.\n",
      "\n",
      "2.3.5 The multivariate normal distribution\n",
      "\n",
      "The normal distribution easily extends to more than one dimension. If X ∼\n",
      "MV N(µ,Σ), then:\n",
      "\n",
      "f(X|µ,Σ) = (2π)−\n",
      "k\n",
      "2 |Σ|−\n",
      "\n",
      "1\n",
      "2 exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2\n",
      "(X − µ)T Σ−1(X − µ)\n",
      "\n",
      "}\n",
      ", (2.23)\n",
      "\n",
      "where X is a vector of random variables, k is the dimensionality of the vector,\n",
      "µ is the vector of means of X, and Σ is the covariance matrix of X. The\n",
      "multivariate normal distribution is an extension of the univariate normal in\n",
      "\n",
      "\n",
      "\n",
      "2.3 Some important distributions in social science 31\n",
      "\n",
      "−10 −5 0 5 10\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "N(0,2)\n",
      "\n",
      "N(0,1) N(3,1)\n",
      "\n",
      "Fig. 2.10. Some normal distributions.\n",
      "\n",
      "which x is expanded from a scalar to a k-dimensional vector of variables,\n",
      "x1, x2, . . . , xk, that are related to one another via the covariance matrix Σ.\n",
      "If X is multivariate normal, then each variable in the vector X is normal. If\n",
      "Σ is diagonal (all off-diagonal elements are 0), then the multivariate normal\n",
      "distribution is equivalent to k univariate normal densities.\n",
      "\n",
      "When the dimensionality of the MVN distribution is equal to two, the\n",
      "distribution is called the “bivariate normal distribution.” Its density function,\n",
      "although equivalent to the one presented above, is often expressed in scalar\n",
      "form as:\n",
      "\n",
      "f(x1, x2) =\n",
      "1\n",
      "\n",
      "2πσ1σ2\n",
      "√\n",
      "\n",
      "1− ρ2\n",
      "exp\n",
      "\n",
      "[\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "(Q−R + S)\n",
      "]\n",
      "\n",
      ", (2.24)\n",
      "\n",
      "where\n",
      "\n",
      "Q =\n",
      "(x1 − µ1)2\n",
      "\n",
      "σ21\n",
      ", (2.25)\n",
      "\n",
      "R =\n",
      "2ρ(x1 − µ1)(x2 − µ2)\n",
      "\n",
      "σ1σ2\n",
      ", (2.26)\n",
      "\n",
      "\n",
      "\n",
      "32 2 Probability Theory and Classical Statistics\n",
      "\n",
      "and\n",
      "\n",
      "S =\n",
      "(x2 − µ2)2\n",
      "\n",
      "σ22\n",
      ". (2.27)\n",
      "\n",
      "The bivariate normal distribution, when the correlation parameter ρ is 0,\n",
      "looks like a three-dimensional bell. As ρ becomes larger (in either positive or\n",
      "negative directions), the bell flattens, as shown in Figure 2.11. The upper part\n",
      "of the figure shows a three-dimensional view and a (top-down) contour plot of\n",
      "the bivariate normal density when ρ = 0. The lower part of the figure shows\n",
      "the density when ρ = .8.\n",
      "\n",
      "x1\n",
      "x2\n",
      "\n",
      "f(X\n",
      " | r=\n",
      "\n",
      "0\n",
      ")\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "x1\n",
      "x2\n",
      "\n",
      "f(X\n",
      " | r=\n",
      "\n",
      ".8\n",
      ")\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "Fig. 2.11. Two bivariate normal distributions.\n",
      "\n",
      "\n",
      "\n",
      "2.4 Classical statistics in social science 33\n",
      "\n",
      "The multivariate normal distribution is used fairly frequently in social\n",
      "science statistics. Specifically, the bivariate normal distribution is used to\n",
      "model simultaneous equations for two outcome variables that are known to\n",
      "be related, and structural equation models rely on the full multivariate normal\n",
      "distribution. I will discuss this distribution in more depth in later chapters\n",
      "describing multivariate models.\n",
      "\n",
      "2.3.6 t and multivariate t distributions\n",
      "\n",
      "The t (Student’s t) and multivariate t distributions are quite commonly used\n",
      "in modern social science statistics. For example, when the variance is un-\n",
      "known in a model that assumes a normal distribution for the data, with the\n",
      "variance following an inverse gamma distribution (see subsequent chapters),\n",
      "the marginal distribution for the mean follows a t distribution (consider tests\n",
      "of coefficients in a regression model). Also, when the sample size is small, the\n",
      "t is used as a robust alternative to the normal distribution in order to com-\n",
      "pensate for heavier tails in the distribution of the data. As the sample size\n",
      "increases, uncertainty about σ decreases, and the t distribution converges on\n",
      "a normal distribution (see Figure 2.12). The density functions for the t dis-\n",
      "tribution appears much more complicated than the normal. If x ∼ t(µ, σ, v),\n",
      "then:\n",
      "\n",
      "f(x) =\n",
      "Γ ((v + 1)/2)\n",
      "Γ (v/2)σ\n",
      "\n",
      "√\n",
      "vπ\n",
      "\n",
      "(\n",
      "1 + v−1\n",
      "\n",
      "(\n",
      "x− µ\n",
      "\n",
      "σ\n",
      "\n",
      ")2)−(v+1)/2\n",
      ", (2.28)\n",
      "\n",
      "where µ is the mean, σ is the standard deviation, and v is the “degrees of\n",
      "freedom.” If X is a k-dimensional vector of variables (x1 . . . xk), and X ∼\n",
      "mvt(µ, Σ, v), then:\n",
      "\n",
      "f(X) =\n",
      "Γ ((v + d)/2)\n",
      "\n",
      "Γ (v/2)vk/2πk/2\n",
      "| Σ |−1/2\n",
      "\n",
      "(\n",
      "1 + v−1(X − µ)T Σ−1(X − µ)\n",
      "\n",
      ")−(v+k)/2\n",
      ",\n",
      "\n",
      "(2.29)\n",
      "where µ is a vector of means, and Σ is the variance-covariance matrix of X.\n",
      "\n",
      "We will not explicitly use the t and multivariate t distributions in this\n",
      "book, although a number of marginal distributions we will be working with\n",
      "will be implicitly t distributions.\n",
      "\n",
      "2.4 Classical statistics in social science\n",
      "\n",
      "Throughout the fall of 2004, CNN/USAToday/Gallup conducted a number\n",
      "of polls attempting to predict whether George W. Bush or John F. Kerry\n",
      "would win the U.S. presidential election. One of the key battleground states\n",
      "was Ohio, which ultimately George Bush won, but all the polls leading up\n",
      "\n",
      "\n",
      "\n",
      "34 2 Probability Theory and Classical Statistics\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "t(1 df)\n",
      "t(2 df)\n",
      "t(10 df)\n",
      "t(120 df)\n",
      "\n",
      "N(0,1) [solid line]\n",
      "\n",
      "Fig. 2.12. The t(0, 1, 1), t(0, 1, 10), and t(0, 1, 120) distributions (with an N(0, 1)\n",
      "distribution superimposed).\n",
      "\n",
      "to the election showed the two candidates claiming proportions of the votes\n",
      "that were statistically indistinguishable in the state. The last poll in Ohio\n",
      "consisted of 1,111 likely voters, 46% of whom stated that they would vote for\n",
      "Bush, and 50% of whom stated that they would vote for Kerry, but the poll\n",
      "had a margin of error of ±3%.4\n",
      "\n",
      "In the previous sections, we discussed probability theory, and I stated\n",
      "that statistics is essentially the inverse of probability. In probability, once we\n",
      "are given a distribution and its parameters, we can deduce the probabilities\n",
      "for events. In statistics, we have a collection of events and are interested in\n",
      "\n",
      "4 see http://www.cnn.com/ELECTION/2004/special/president/showdown/OH/\n",
      "polls.html for the data reported in this and the next chapter. Additional polls are\n",
      "displayed on the website, but I use only the CNN/USAToday/Gallup polls, given\n",
      "that they are most likely similar in sample design. Unfortunately, the proportions\n",
      "are rounded, and so my results from here on are approximate. For example, in\n",
      "the last poll, 50% planned to vote for Kerry, and 50% of 1,111 is 556. However,\n",
      "the actual number could range from 550 to 561 given the rounding.\n",
      "\n",
      "\n",
      "\n",
      "2.5 Maximum likelihood estimation 35\n",
      "\n",
      "determining the values of the parameters that produced them. Returning to\n",
      "the polling data, determining who would win the election is tantamount to\n",
      "determining the population parameter (the proportion of actual voters who\n",
      "will vote for a certain candidate) given a collection of events (a sample of\n",
      "potential votes) thought to arise from this parameter and the probability\n",
      "distribution to which it belongs.\n",
      "\n",
      "Classical statistics provides one recipe for estimating this population pa-\n",
      "rameter; in the remainder of this chapter, I demonstrate how. In the next\n",
      "chapter, I tackle the problem from a Bayesian perspective. Throughout this\n",
      "section, by “classical statistics” I mean the approach that is most commonly\n",
      "used among academic researchers in the social sciences. To be sure, the clas-\n",
      "sical approach to statistics in use is a combination of several approaches,\n",
      "involving the use of theorems and perspectives of a number of statisticians.\n",
      "For example, the most common approach to model estimation is maximum\n",
      "likelihood estimation, which has its roots in the works of Fisher, whereas the\n",
      "common approach to hypothesis testing using p-values has its roots in the\n",
      "works of both Neyman and Pearson and Fisher—each of whom in fact devel-\n",
      "oped somewhat differing views of hypothesis testing using p-values (again, see\n",
      "Hubbard and Bayarri 2003 or see Gill 2002 for an even more detailed history).\n",
      "\n",
      "2.5 Maximum likelihood estimation\n",
      "\n",
      "The classical approach to statistics taught in social science statistics courses\n",
      "involves two basic steps: (1) model estimation and (2) inference. The first step\n",
      "involves first determining an appropriate probability distribution/model for\n",
      "the data at hand and then estimating its parameters. Maximum likelihood\n",
      "(ML) is the most commonly used method of estimating parameters and de-\n",
      "termining the extent of error in the estimation (steps 1 and 2, respectively) in\n",
      "social science statistics (see Edwards 1992 for a detailed, theoretical discus-\n",
      "sion of likelihood analysis; see Eliason 1993 for a more detailed discussion of\n",
      "the mechanics of ML estimation).\n",
      "\n",
      "The fundamental idea behind maximum likelihood estimation is that a\n",
      "good choice for the estimate of a parameter of interest is the value of the\n",
      "parameter that makes the observed data most likely to have occurred. To do\n",
      "this, we need to establish some sort of function that gives us the probability\n",
      "for the data, and we need to find the value of the parameter that maximizes\n",
      "this probability. This function is called the “likelihood function” in classical\n",
      "statistics, and it is essentially the product of sampling densities—probability\n",
      "distributions—for each observation in the sample. The process of estimation\n",
      "thus involves the following steps:\n",
      "\n",
      "1. Construct a likelihood function for the parameter(s) of interest.\n",
      "2. Simplify the likelihood function and take its logarithm.\n",
      "3. Take the partial derivative of the log-likelihood function with respect to\n",
      "\n",
      "each parameter, and set the resulting equation(s) equal to 0.\n",
      "\n",
      "\n",
      "\n",
      "36 2 Probability Theory and Classical Statistics\n",
      "\n",
      "4. Solve the system of equations to find the parameters.\n",
      "\n",
      "This process seems complicated, and indeed it can be. Step 4 can be quite\n",
      "difficult when there are lots of parameters. Generally, some sort of iterative\n",
      "method is required to find the maximum. Below I detail the process of ML\n",
      "estimation.\n",
      "\n",
      "2.5.1 Constructing a likelihood function\n",
      "\n",
      "If x1, x2 . . . xn are independent observations of a random variable, x, in a data\n",
      "set of size n, then we know from the multiplication rule in probability theory\n",
      "that the joint probability for the vector X is:\n",
      "\n",
      "f(X|θ) ≡ L(θ | x) =\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "f(xi | θ). (2.30)\n",
      "\n",
      "This equation is the likelihood function for the model. Notice how the\n",
      "parameter and the data switch places in the L(.) notation versus the f(.)\n",
      "notation. We denote this as L(.), because from a classical standpoint, the\n",
      "parameter is assumed to be fixed. However, we are interested in estimating\n",
      "the parameter θ, given the data we have observed, so we use this notation.\n",
      "The primary point of constructing a likelihood function is that, given the data\n",
      "at hand, we would like to solve for the value of the parameter that makes\n",
      "the occurence of the data most probable, or most “likely” to have actually\n",
      "occurred.\n",
      "\n",
      "As the right-hand side of the equation shows, the construction of the like-\n",
      "lihood function first relies on determining an appropriate probability distri-\n",
      "bution f(.) thought to generate the observed data. In our election polling\n",
      "example, the data consist of 1,111 potential votes, the vast majority of which\n",
      "were either for Bush or for Kerry. If we assume that candidates other than\n",
      "these two are unimportant—that is, the election will come down to whom\n",
      "among these two receives more votes—then the data ultimately reduce to 556\n",
      "potential votes for Kerry and 511 potential votes for Bush. An appropriate\n",
      "distribution for such data is the binomial distribution. If we are interested\n",
      "in whether Kerry will win the election, we can consider a vote for Kerry a\n",
      "“success,” and its opposite, a vote for Bush, a “failure,” and we can set up\n",
      "our likelihood function with the goal of determining the success probability\n",
      "p. The likelihood function in this case looks like:\n",
      "\n",
      "L(p|X) =\n",
      "(\n",
      "\n",
      "1067\n",
      "556\n",
      "\n",
      ")\n",
      "p556(1− p)511.\n",
      "\n",
      "As an alternative view that ultimately produces the same results, we can con-\n",
      "sider that, at the individual level, each of our votes arises from a Bernoulli dis-\n",
      "tribution, and so our likelihood function is the product of n = 1, 067 Bernoulli\n",
      "distributions. In that case:\n",
      "\n",
      "\n",
      "\n",
      "2.5 Maximum likelihood estimation 37\n",
      "\n",
      "L(p|X) =\n",
      "n=1067∏\n",
      "\n",
      "i=1\n",
      "\n",
      "pxi(1− p)1−xi . (2.31)\n",
      "\n",
      "Given that we know nothing about our potential voters beyond for whom they\n",
      "plan to vote, we can consider the individuals “exchangeable,” and after car-\n",
      "rying out the multiplication across individuals, this version of the likelihood\n",
      "function is proportional to the first one based on the binomial distribution,\n",
      "only differing by a combinatorial expression. This expression simply scales\n",
      "the curve, and so it is ultimately unimportant in affecting our estimate. Fig-\n",
      "ure 2.13 shows this result: The upper figure shows the likelihood function\n",
      "based on the binomial distribution; the lower figure shows the likelihood func-\n",
      "tion based on the Bernoulli distribution. The only difference between the two\n",
      "functions can be found in the scale of the y axis.\n",
      "\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "p (Binomial Likelihood)\n",
      "\n",
      "L\n",
      "(p\n",
      "\n",
      " |\n",
      " x\n",
      "\n",
      "=\n",
      "5\n",
      "\n",
      "5\n",
      "6\n",
      "\n",
      ")\n",
      "\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8\n",
      "\n",
      "0\n",
      "  \n",
      " e\n",
      "\n",
      "+\n",
      "0\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "p (Bernoulli Likelihood)\n",
      "\n",
      "L\n",
      "(p\n",
      "\n",
      " |\n",
      " s\n",
      "\n",
      "u\n",
      "m\n",
      "\n",
      "(x\n",
      ")=\n",
      "\n",
      "5\n",
      "5\n",
      "\n",
      "6\n",
      ")\n",
      "\n",
      "Fig. 2.13. Binomial (top) and Bernoulli (bottom) likelihood functions for the OH\n",
      "presidential poll data.\n",
      "\n",
      "\n",
      "\n",
      "38 2 Probability Theory and Classical Statistics\n",
      "\n",
      "2.5.2 Maximizing a likelihood function\n",
      "\n",
      "How do we obtain the estimates for the parameters after we set up the like-\n",
      "lihood function? Just as many pdfs are unimodal and slope away from the\n",
      "mode of the distribution, we expect the likelihood function to look about the\n",
      "same. So, what we need to find is the peak of this curve. From calculus we\n",
      "know that the slope of the curve should be 0 at its peak. Thus, we should\n",
      "take the derivative of the likelihood function with respect to the parameter,\n",
      "set it equal to 0, and find the x coordinate (the parameter value) for which\n",
      "the curve reaches a maximum.\n",
      "\n",
      "We generally take the logarithm of the likelihood function before we dif-\n",
      "ferentiate, because the log function converts the repeated multiplication to\n",
      "repeated addition, and repeated addition is much easier to work with. The\n",
      "log-likelihood reaches a maximum at the same point as the original function.\n",
      "Generically:\n",
      "\n",
      "Log-Likelihood ≡ LL(θ | X) =\n",
      "n∑\n",
      "\n",
      "i=1\n",
      "\n",
      "log(f(xi | θ)). (2.32)\n",
      "\n",
      "For our specific problem:\n",
      "\n",
      "LL(p|x) ∝ 556 ln p + 511 ln(1− p).\n",
      "\n",
      "To find the value of p where this log-likelihood function reaches a maximum,\n",
      "we need to take the derivative of the function with respect to p, set it equal\n",
      "to 0, and solve for p. Generically, the derivative of a binomial log-likelihood\n",
      "function is:\n",
      "\n",
      "dLL\n",
      "\n",
      "dp\n",
      "=\n",
      "∑\n",
      "\n",
      "xi\n",
      "p\n",
      "\n",
      "−\n",
      "n−\n",
      "\n",
      "∑\n",
      "xi\n",
      "\n",
      "1− p\n",
      ". (2.33)\n",
      "\n",
      "If we set this derivative equal to 0 and solve for p, we obtain:\n",
      "\n",
      "n−\n",
      "∑\n",
      "\n",
      "xi\n",
      "1− p\n",
      "\n",
      "=\n",
      "∑\n",
      "\n",
      "xi\n",
      "p\n",
      "\n",
      ".\n",
      "\n",
      "Simplifying yields:\n",
      "\n",
      "p̂ =\n",
      "∑\n",
      "\n",
      "xi\n",
      "n\n",
      "\n",
      ". (2.34)\n",
      "\n",
      "This result shows that the maximum likelihood estimate for p is simply the\n",
      "observed proportion of successes. In our example, this is the proportion of\n",
      "potential votes for Kerry, out of those who opted for either Kerry or Bush\n",
      "(here, 556/1067 = .521). Given that this value for p is an estimate, we typically\n",
      "denote it p̂, rather than p.\n",
      "\n",
      "Figure 2.14 displays this process of estimation graphically. The figure\n",
      "shows that both the likelihood function and the log-likelihood functions peak\n",
      "at the same point. The horizontal lines are the tangent lines to the curve\n",
      "\n",
      "\n",
      "\n",
      "2.5 Maximum likelihood estimation 39\n",
      "\n",
      "where the slopes of these lines are 0; they are at the maximum of the func-\n",
      "tions. The corresponding x coordinate where the curves reach their maximum\n",
      "is the maximum likelihood estimate (MLE).\n",
      "\n",
      "0.2 0.3 0.4 0.5 0.6 0.7 0.8\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      ".0\n",
      "3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "4\n",
      "\n",
      "p\n",
      "\n",
      "L\n",
      "(p\n",
      "\n",
      "|x\n",
      ")\n",
      "\n",
      "dL(p)/dp=0\n",
      "\n",
      "p where\n",
      "dL(p)/dp=dLnL(p)/dp=0\n",
      "\n",
      "p\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "dLnL(p)/dp=0\n",
      "\n",
      "L(p)\n",
      "LnL(p)\n",
      "\n",
      "Fig. 2.14. Finding the MLE: Likelihood and log-likelihood functions for the OH\n",
      "presidential poll data.\n",
      "\n",
      "2.5.3 Obtaining standard errors\n",
      "\n",
      "p̂ is an estimate and is not guaranteed to equal the population parameter p in\n",
      "any particular sample. Thus, we need some way to quantify our uncertainty\n",
      "in estimating p with p̂ from our sample. A nice additional feature of the\n",
      "log-likelihood is that a function of the second derivative of the log-likelihood\n",
      "function can be used to estimate the variance of the sampling distribution\n",
      "(the square root of which is called the “standard error”).5 Specifically, we\n",
      "\n",
      "5 See Appendix B for a discussion of the Central Limit Theorem and the basis for\n",
      "classical inference.\n",
      "\n",
      "\n",
      "\n",
      "40 2 Probability Theory and Classical Statistics\n",
      "\n",
      "must take the inverse of the negative expected value of the second derivative\n",
      "of the log-likelihood function. Mathematically:\n",
      "\n",
      "I(θ)−1 =\n",
      "(\n",
      "−E\n",
      "\n",
      "(\n",
      "∂2LL\n",
      "\n",
      "∂θ∂θT\n",
      "\n",
      "))−1\n",
      ", (2.35)\n",
      "\n",
      "where θ is our parameter or vector of parameters and I(θ) is called the “infor-\n",
      "mation matrix.” The square root of the diagonal elements of this matrix are\n",
      "the parameter standard errors. I(θ)−1 can be computed using the following\n",
      "steps:\n",
      "\n",
      "1. Take the second partial derivatives of the log-likelihood. In multiparameter\n",
      "models, this produces a matrix of partial derivatives (called the Hessian\n",
      "matrix).\n",
      "\n",
      "2. Take the negative of the expectation of this matrix to obtain the “infor-\n",
      "mation matrix” I(θ).\n",
      "\n",
      "3. Invert this matrix to obtain estimates of the variances and covariances of\n",
      "parameters (get standard errors by square-rooting the diagonal elements\n",
      "of the matrix).\n",
      "\n",
      "The fact that I(θ)−1 contains the standard errors is not intuitive. But,\n",
      "if you recall that the first derivative is a measure of the slope of a function\n",
      "at a point (the rate of change in the function at that point), and the second\n",
      "derivative is a measure of the rate of change in the slope, we can think of\n",
      "the second derivative as indicating the rate of curvature of the curve. A very\n",
      "steep curve, then, has a very high rate of curvature, which makes its second\n",
      "derivative large. Thus, when we invert it, it makes the standard deviation\n",
      "small. On the other hand, a very shallow curve has a very low rate of curvature,\n",
      "which makes its second derivative small. When we invert a small number, it\n",
      "makes the standard deviation large. Note that, when we evaluate the second\n",
      "derivative, we substitute the MLE estimate for the parameter into the result\n",
      "to obtain the standard error at the estimate.\n",
      "\n",
      "Returning to our data at hand, the second partial derivative of the generic\n",
      "binomial log-likelihood function with respect to p is:\n",
      "\n",
      "∂2LL\n",
      "\n",
      "∂p2\n",
      "=\n",
      "∑\n",
      "\n",
      "x\n",
      "\n",
      "p2\n",
      "−\n",
      "\n",
      "n−\n",
      "∑\n",
      "\n",
      "x\n",
      "\n",
      "(1− p)2\n",
      ". (2.36)\n",
      "\n",
      "Taking expectations yields:\n",
      "\n",
      "E\n",
      "\n",
      "(\n",
      "∂2LL\n",
      "\n",
      "∂p2\n",
      "\n",
      ")\n",
      "= E\n",
      "\n",
      "[\n",
      "−\n",
      "∑\n",
      "\n",
      "x\n",
      "\n",
      "p2\n",
      "−\n",
      "\n",
      "n−\n",
      "∑\n",
      "\n",
      "x\n",
      "\n",
      "(1− p)2\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "The expectation of these expressions can be computed by realizing that the\n",
      "expectation of\n",
      "\n",
      "∑\n",
      "x/n is p (put another way: E(p̂) = p). Thus:\n",
      "\n",
      "E\n",
      "\n",
      "(\n",
      "∂2LL\n",
      "\n",
      "∂p2\n",
      "\n",
      ")\n",
      "= −\n",
      "\n",
      "np\n",
      "\n",
      "p2\n",
      "−\n",
      "\n",
      "n− np\n",
      "(1− p)2\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "2.5 Maximum likelihood estimation 41\n",
      "\n",
      "Some simplification yields:\n",
      "\n",
      "E\n",
      "\n",
      "(\n",
      "∂2LL\n",
      "\n",
      "∂p2\n",
      "\n",
      ")\n",
      "= −\n",
      "\n",
      "n\n",
      "\n",
      "p(1− p)\n",
      ".\n",
      "\n",
      "At this point, we can negate the expectation, invert it, and evaluate it at the\n",
      "MLE (p̂) to obtain:\n",
      "\n",
      "I(p)−1 =\n",
      "p̂(1− p̂)\n",
      "\n",
      "n\n",
      ". (2.37)\n",
      "\n",
      "Taking the square root of this yields the estimated standard error. In our\n",
      "polling data case, the standard error is\n",
      "\n",
      "√\n",
      "(.521× .479)/1067 = .015.\n",
      "\n",
      "Recall that our question is whether Kerry would win the vote in Ohio. Our\n",
      "estimate for the Ohio population proportion to vote for Kerry (versus Bush)\n",
      "was .521, which suggests he would win the popular vote in Ohio (discounting\n",
      "third party candidates). However, the standard error of this estimate was\n",
      ".015. We can construct our usual confidence interval around the maximum\n",
      "likelihood estimate to obtain a 95% interval for the MLE. If we do this, we\n",
      "obtain an interval of [.492, .550] (CI = p̂ ± 1.96 × s.e.(p̂)). Given that the\n",
      "lower bound on this interval is below .5, we can conclude that we cannot rule\n",
      "out the possibility that Kerry would not win the popular vote in Ohio.\n",
      "\n",
      "An alternative to the confidence interval approach to answering this ques-\n",
      "tion is to construct a t test, with a null hypothesis H0 : p < .5. Following that\n",
      "approach:\n",
      "\n",
      "t =\n",
      "(.521− .5)\n",
      "\n",
      ".015\n",
      "= 1.4.\n",
      "\n",
      "This t value is not large enough to reject the null hypothesis (that Kerry’s\n",
      "proportion of the vote is less than .5), and thus, the conclusion we would reach\n",
      "is the same: We do not have enough evidence to conclude that Kerry will win\n",
      "(see Appendix B for more discussion of null hypotheses, confidence intervals,\n",
      "and t tests).\n",
      "\n",
      "Note that this result is consistent with the result I stated at the beginning\n",
      "of this section: The results of the original poll suggested that the vote was\n",
      "too close to call, given a ±3% margin of error. Here, I have shown essentially\n",
      "from where that margin of error arose. We ended up with a margin of error\n",
      "of .0294, which is approximately equal to the margin of error in the original\n",
      "poll.\n",
      "\n",
      "2.5.4 A normal likelihood example\n",
      "\n",
      "Because the normal distribution is used repeatedly throughout this book and\n",
      "throughout the social sciences, I conclude this chapter by deriving parameter\n",
      "estimates and standard errors for a normal distribution problem. I keep this\n",
      "example at a general level; in subsequent chapters, we will return to this\n",
      "likelihood function with specific problems and data.\n",
      "\n",
      "\n",
      "\n",
      "42 2 Probability Theory and Classical Statistics\n",
      "\n",
      "Suppose you have n observations x1, x2, . . . , xn that you assume are nor-\n",
      "mally distributed. Once again, if the observations are assumed to be indepen-\n",
      "dent, a likelihood function can be constructed as the multiple of independent\n",
      "normal density functions:\n",
      "\n",
      "L(µ, σ | X) =\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "2πσ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(xi − µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ". (2.38)\n",
      "\n",
      "We can simplify the likelihood as:\n",
      "\n",
      "L(µ, σ | X) = (2πσ2)−\n",
      "n\n",
      "2 exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2\n",
      "\n",
      "n∑\n",
      "i=1\n",
      "\n",
      "(xi − µ)2\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "The log of the likelihood is:\n",
      "\n",
      "LL(µ, σ | X) ∝ −n ln(σ)−\n",
      "1\n",
      "\n",
      "2σ2\n",
      "\n",
      "n∑\n",
      "i=1\n",
      "\n",
      "(xi − µ)2. (2.39)\n",
      "\n",
      "In the above equation, I have eliminated the −n\n",
      "2\n",
      "log(2π), an irrelevant con-\n",
      "\n",
      "stant. It is irrelevant, because it does not depend on either parameter and\n",
      "will therefore drop once the partial derivatives are taken. In this example, we\n",
      "have two parameters, µ and σ, and hence the first partial derivative must be\n",
      "taken with respect to each parameter. This will leave us with two equations\n",
      "(one for each parameter). After taking the partial derivatives with respect to\n",
      "each parameter, we obtain the following:\n",
      "\n",
      "∂LL\n",
      "\n",
      "∂µ\n",
      "=\n",
      "\n",
      "n(x̄− µ)\n",
      "σ2\n",
      "\n",
      "and\n",
      "∂LL\n",
      "\n",
      "∂σ\n",
      "= −\n",
      "\n",
      "n\n",
      "\n",
      "σ\n",
      "+\n",
      "\n",
      "1\n",
      "σ3\n",
      "\n",
      "n∑\n",
      "i=1\n",
      "\n",
      "(xi − µ)2.\n",
      "\n",
      "Setting these partial derivatives each equal to 0 and doing a little algebra\n",
      "yields:\n",
      "\n",
      "µ̂ = x̄ (2.40)\n",
      "\n",
      "and\n",
      "\n",
      "σ̂2 =\n",
      "∑n\n",
      "\n",
      "i=1(xi − µ)\n",
      "2\n",
      "\n",
      "n\n",
      ". (2.41)\n",
      "\n",
      "These estimators should look familiar: The MLE for the population mean is\n",
      "the sample mean; the MLE for the population variance is the sample variance.6\n",
      "\n",
      "Estimates of the variability in the estimates for the mean and standard\n",
      "deviation can be obtained as we did in the binomial example. However, as\n",
      "\n",
      "6 The MLE is known to be biased, and hence, a correction is added, so that the\n",
      "denominator is n− 1 rather than n.\n",
      "\n",
      "\n",
      "\n",
      "2.5 Maximum likelihood estimation 43\n",
      "\n",
      "noted above, given that we have two parameters, our second derivate matrix\n",
      "will, in fact, be a matrix. For the purposes of avoiding taking square roots\n",
      "until the end, let τ = σ2, and we’ll construct the Hessian matrix in terms of\n",
      "τ . Also, let θ be a vector containing both µ and τ . Thus, we must compute:\n",
      "\n",
      "∂2LL\n",
      "\n",
      "∂θ∂θT\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "∂2LL\n",
      "∂µ2\n",
      "\n",
      "∂2LL\n",
      "∂µ∂τ\n",
      "\n",
      "∂2LL\n",
      "∂τ∂µ\n",
      "\n",
      "∂2LL\n",
      "∂τ2\n",
      "\n",
      "\n",
      " . (2.42)\n",
      "\n",
      "Without showing all the derivatives (see Exercises), the elements of the Hes-\n",
      "sian matrix are then:\n",
      "\n",
      "∂2LL\n",
      "\n",
      "∂θ∂θT\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "−n\n",
      "τ\n",
      "\n",
      "−n(x̄−µ)\n",
      "τ2\n",
      "\n",
      "−n(x̄−µ)\n",
      "τ2\n",
      "\n",
      "n\n",
      "2τ2\n",
      "\n",
      "−\n",
      "Pn\n",
      "\n",
      "i=1(xi−µ)\n",
      "2\n",
      "\n",
      "τ3\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "In order to obtain the information matrix, which can be used to compute the\n",
      "standard errors, we must take the expectation of this Hessian matrix and take\n",
      "its negative. Let’s take the expectation of the off-diagonal elements first. The\n",
      "expectation of x̄− µ is 0 (given that the MLE is unbiased), which makes the\n",
      "off-diagonal elements of the information matrix equal to 0. This result should\n",
      "be somewhat intuitive: There need be no relationship between the mean and\n",
      "variance in a normal distribution.\n",
      "\n",
      "The first element, (−n/τ), is unchanged under expectation. Thus, after\n",
      "substituting σ2 back in for τ and negating the result, we obtain n/σ2 for this\n",
      "element of the information matrix.\n",
      "\n",
      "The last element, (n/2τ2) − (\n",
      "∑n\n",
      "\n",
      "i=1(xi − µ)\n",
      "2)/τ3, requires a little con-\n",
      "\n",
      "sideration. The only part of this expression that changes under expecta-\n",
      "tion is\n",
      "\n",
      "∑n\n",
      "i=1(xi − µ)\n",
      "\n",
      "2. The expectation of this expression is nτ . That is,\n",
      "E(xi − µ)2 = τ , and we are taking this value n times (notice the summa-\n",
      "tion). Thus, this element, after a little algebraic manipulation, negation, and\n",
      "substitution of σ2 for τ , becomes: n/2σ4. So, our information matrix appears\n",
      "as:\n",
      "\n",
      "I(θ) =\n",
      "[\n",
      "\n",
      "n\n",
      "σ2\n",
      "\n",
      "0\n",
      "0 n\n",
      "\n",
      "2σ4\n",
      "\n",
      "]\n",
      ". (2.43)\n",
      "\n",
      "To obtain standard errors, we need to (1) invert this matrix, and (2) take\n",
      "the square root of the diagonal elements (variances) to obtain the standard\n",
      "errors. Matrix inversion in this case is quite simple, given that the off-diagonal\n",
      "elements are equal to 0. In this case, the inverse of the matrix is simply the\n",
      "inverse of the diagonal elements.\n",
      "\n",
      "Once we invert and square root the elements of the information matrix,\n",
      "we find that the estimate for the standard error for our estimate µ̂ is σ̂/\n",
      "\n",
      "√\n",
      "n,\n",
      "\n",
      "and our estimate for the standard error for σ̂2 is σ̂2\n",
      "√\n",
      "\n",
      "2/n. The estimate for\n",
      "the standard error for µ̂ should look familiar: It is the standard deviation of\n",
      "\n",
      "\n",
      "\n",
      "44 2 Probability Theory and Classical Statistics\n",
      "\n",
      "the sampling distribution for a mean based on the Central Limit Theorem\n",
      "(see Appendix B).\n",
      "\n",
      "2.6 Conclusions\n",
      "\n",
      "In this chapter, we have reviewed the basics of probability theory. Importantly,\n",
      "we have developed the concept of probability distributions in general, and we\n",
      "have discussed a number of actual probability distributions. In addition, we\n",
      "have discussed how important quantities like the mean and variance can be\n",
      "derived analytically from probability distributions. Finally, we reviewed the\n",
      "most common approach to estimating such quantities in a classical setting—\n",
      "maximum likelihood estimation—given a collection of data thought to arise\n",
      "from a particular distribution. As stated earlier, I recommend reading De-\n",
      "Groot (1986) for a more thorough introduction to probability theory, and I\n",
      "recommend Billingsley (1995) and Chung and AitSahlia (2003) for more ad-\n",
      "vanced and detailed expositions. For a condensed exposition, I suggest Rudas\n",
      "2004. Finally, I recommend Edwards (1992) and Gill (2002) for detailed dis-\n",
      "cussions of the history and practice of maximum likelihood (ML) estimation,\n",
      "and I suggest Eliason (1993) for a highly applied perspective on ML estima-\n",
      "tion. In the next chapter, we will discuss the Bayesian approach to statistics\n",
      "as an alternative to this classical approach to model building and estimation.\n",
      "\n",
      "2.7 Exercises\n",
      "\n",
      "2.7.1 Probability exercises\n",
      "\n",
      "1. Find the normalizing constant for the linear density in Equation 2.8.\n",
      "2. Using the binomial mass function, find the probability of obtaining 3 heads\n",
      "\n",
      "in a row with a fair coin.\n",
      "3. Find the probability of obtaining 3 heads in a row with a coin weighted\n",
      "\n",
      "so that the probability of obtaining a head is .7.\n",
      "4. What is the probability of obtaining 3 heads OR 3 tails in a row with a\n",
      "\n",
      "fair coin?\n",
      "5. What is the probability of obtaining 3 heads and 1 tail (order irrelevant)\n",
      "\n",
      "on four flips of a fair coin?\n",
      "6. Using a normal approximation to the binomial distribution, find the prob-\n",
      "\n",
      "ability of obtaining 130 or more heads in 200 flips of a fair coin.\n",
      "7. Plot a normal distribution with parameters µ = 5 and σ = 2.\n",
      "8. Plot a normal distribution with parameters µ = 2 and σ = 5.\n",
      "9. Plot the t(0, 1, df = 1) and t(0, 1, df = 10) distributions. Note: Γ is a\n",
      "\n",
      "function. The function is: Γ (n) =\n",
      "∫∞\n",
      "0\n",
      "\n",
      "e−uun−1du. For integers, Γ (n) =\n",
      "(n − 1)! Thus, Γ (4) = (4 − 1)! = 6. However, when the argument to the\n",
      "function is not an integer, this formula will not work. Instead, it is easier\n",
      "to use a software package to compute the function value for you.\n",
      "\n",
      "\n",
      "\n",
      "2.7 Exercises 45\n",
      "\n",
      "10. Show that the multivariate normal density function reduces to the univari-\n",
      "ate normal density function when the dimensionality of the distribution\n",
      "is 1.\n",
      "\n",
      "2.7.2 Classical inference exercises\n",
      "\n",
      "1. Find the MLE for p in a binomial distribution representing a sample in\n",
      "which 20 successes were obtained out of 30 trials.\n",
      "\n",
      "2. Based on the binomial sample in the previous question, if the trials in-\n",
      "volved coin flips, would having 20 heads be sufficient to question the fair-\n",
      "ness of the coin? Why or why not?\n",
      "\n",
      "3. Suppose a sample of students at a major university were given an IQ test,\n",
      "which resulted in a mean of 120 and a standard deviation of 10. If we\n",
      "know that IQs are normally distributed in the population with a mean of\n",
      "100 and a standard deviation of 16, is there sufficient evidence to suggest\n",
      "that the college students are more intelligent than average?\n",
      "\n",
      "4. Suppose a single college student were given an IQ test and scored 120. Is\n",
      "there sufficient evidence to indicate that college students are more intelli-\n",
      "gent than average based on this sample?\n",
      "\n",
      "5. What is the difference (if any) between the responses to the previous two\n",
      "questions?\n",
      "\n",
      "6. Derive the Hessian matrix for the normal distribution example at the end\n",
      "of the chapter.\n",
      "\n",
      "7. Derive the MLE for λ from a sample of n observations from a Poisson\n",
      "distribution.\n",
      "\n",
      "8. Derive the standard error estimate for λ from the previous question.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Basics of Bayesian Statistics\n",
      "\n",
      "Suppose a woman believes she may be pregnant after a single sexual encounter,\n",
      "but she is unsure. So, she takes a pregnancy test that is known to be 90%\n",
      "accurate—meaning it gives positive results to positive cases 90% of the time—\n",
      "and the test produces a positive result.1 Ultimately, she would like to know the\n",
      "probability she is pregnant, given a positive test (p(preg | test +)); however,\n",
      "what she knows is the probability of obtaining a positive test result if she is\n",
      "pregnant (p(test + |preg)), and she knows the result of the test.\n",
      "\n",
      "In a similar type of problem, suppose a 30-year-old man has a positive\n",
      "blood test for a prostate cancer marker (PSA). Assume this test is also ap-\n",
      "proximately 90% accurate. Once again, in this situation, the individual would\n",
      "like to know the probability that he has prostate cancer, given the positive\n",
      "test, but the information at hand is simply the probability of testing positive\n",
      "if he has prostate cancer, coupled with the knowledge that he tested positive.\n",
      "\n",
      "Bayes’ Theorem offers a way to reverse conditional probabilities and,\n",
      "hence, provides a way to answer these questions. In this chapter, I first show\n",
      "how Bayes’ Theorem can be applied to answer these questions, but then I\n",
      "expand the discussion to show how the theorem can be applied to probability\n",
      "distributions to answer the type of questions that social scientists commonly\n",
      "ask. For that, I return to the polling data described in the previous chapter.\n",
      "\n",
      "3.1 Bayes’ Theorem for point probabilities\n",
      "\n",
      "Bayes’ original theorem applied to point probabilities. The basic theorem\n",
      "states simply:\n",
      "\n",
      "p(B|A) =\n",
      "p(A|B)p(B)\n",
      "\n",
      "p(A)\n",
      ". (3.1)\n",
      "\n",
      "1 In fact, most pregnancy tests today have a higher accuracy rate, but the accuracy\n",
      "rate depends on the proper use of the test as well as other factors.\n",
      "\n",
      "\n",
      "\n",
      "48 3 Basics of Bayesian Statistics\n",
      "\n",
      "In English, the theorem says that a conditional probability for event B\n",
      "given event A is equal to the conditional probability of event A given event\n",
      "B, multiplied by the marginal probability for event B and divided by the\n",
      "marginal probability for event A.\n",
      "\n",
      "Proof: From the probability rules introduced in Chapter 2, we know that\n",
      "p(A,B) = p(A|B)p(B). Similarly, we can state that p(B,A) = p(B|A)p(A).\n",
      "Obviously, p(A,B) = p(B,A), so we can set the right sides of each of these\n",
      "equations equal to each other to obtain:\n",
      "\n",
      "p(B|A)p(A) = p(A|B)p(B).\n",
      "\n",
      "Dividing both sides by p(A) leaves us with Equation 3.1.\n",
      "The left side of Equation 3.1 is the conditional probability in which we\n",
      "\n",
      "are interested, whereas the right side consists of three components. p(A|B)\n",
      "is the conditional probability we are interested in reversing. p(B) is the un-\n",
      "conditional (marginal) probability of the event of interest. Finally, p(A) is the\n",
      "marginal probability of event A. This quantity is computed as the sum of\n",
      "the conditional probability of A under all possible events Bi in the sample\n",
      "space: Either the woman is pregnant or she is not. Stated mathematically for\n",
      "a discrete sample space:\n",
      "\n",
      "p(A) =\n",
      "∑\n",
      "\n",
      "Bi∈SB\n",
      "\n",
      "p(A | Bi)p(Bi).\n",
      "\n",
      "Returning to the pregnancy example to make the theorem more concrete,\n",
      "suppose that, in addition to the 90% accuracy rate, we also know that the\n",
      "test gives false-positive results 50% of the time. In other words, in cases in\n",
      "which a woman is not pregnant, she will test positive 50% of the time. Thus,\n",
      "there are two possible events Bi: B1 = preg and B2 = not preg. Additionally,\n",
      "given the accuracy and false-positive rates, we know the conditional probabil-\n",
      "ities of obtaining a positive test under these events: p(test +|preg) = .9 and\n",
      "p(test +|not preg) = .5. With this information, combined with some “prior”\n",
      "information concerning the probability of becoming pregnant from a single\n",
      "sexual encounter, Bayes’ theorem provides a prescription for determining the\n",
      "probability of interest.\n",
      "\n",
      "The “prior” information we need, p(B) ≡ p(preg), is the marginal probabil-\n",
      "ity of being pregnant, not knowing anything beyond the fact that the woman\n",
      "has had a single sexual encounter. This information is considered prior infor-\n",
      "mation, because it is relevant information that exists prior to the test. We may\n",
      "know from previous research that, without any additional information (e.g.,\n",
      "concerning date of last menstrual cycle), the probability of conception for any\n",
      "single sexual encounter is approximately 15%. (In a similar fashion, concerning\n",
      "the prostate cancer scenario, we may know that the prostate cancer incidence\n",
      "rate for 30-year-olds is .00001—see Exercises). With this information, we can\n",
      "determine p(B | A) ≡ p(preg|test +) as:\n",
      "\n",
      "\n",
      "\n",
      "3.1 Bayes’ Theorem for point probabilities 49\n",
      "\n",
      "p(preg | test +) =\n",
      "p(test + | preg)p(preg)\n",
      "\n",
      "p(test + | preg)p(preg) + p(test + | not preg)p(not preg)\n",
      ".\n",
      "\n",
      "Filling in the known information yields:\n",
      "\n",
      "p(preg | test +) =\n",
      "(.90)(.15)\n",
      "\n",
      "(.90)(.15) + (.50)(.85)\n",
      "=\n",
      "\n",
      ".135\n",
      ".135 + .425\n",
      "\n",
      "= .241.\n",
      "\n",
      "Thus, the probability the woman is pregnant, given the positive test, is only\n",
      ".241. Using Bayesian terminology, this probability is called a “posterior prob-\n",
      "ability,” because it is the estimated probability of being pregnant obtained\n",
      "after observing the data (the positive test). The posterior probability is quite\n",
      "small, which is surprising, given a test with so-called 90% “accuracy.” How-\n",
      "ever, a few things affect this probability. First is the relatively low probability\n",
      "of becoming pregnant from a single sexual encounter (.15). Second is the ex-\n",
      "tremely high probability of a false-positive test (.50), especially given the high\n",
      "probability of not becoming pregnant from a single sexual encounter (p = .85)\n",
      "(see Exercises).\n",
      "\n",
      "If the woman is aware of the test’s limitations, she may choose to repeat the\n",
      "test. Now, she can use the “updated” probability of being pregnant (p = .241)\n",
      "as the new p(B); that is, the prior probability for being pregnant has now been\n",
      "updated to reflect the results of the first test. If she repeats the test and again\n",
      "observes a positive result, her new “posterior probability” of being pregnant\n",
      "is:\n",
      "\n",
      "p(preg | test +) =\n",
      "(.90)(.241)\n",
      "\n",
      "(.90)(.241) + (.50)(.759)\n",
      "=\n",
      "\n",
      ".135\n",
      ".135 + .425\n",
      "\n",
      "= .364.\n",
      "\n",
      "This result is still not very convincing evidence that she is pregnant, but if she\n",
      "repeats the test again and finds a positive result, her probability increases to\n",
      ".507 (for general interest, subsequent positive tests yield the following prob-\n",
      "abilities: test 4 = .649, test 5 = .769, test 6 = .857, test 7 = .915, test 8 =\n",
      ".951, test 9 = .972, test 10 = .984).\n",
      "\n",
      "This process of repeating the test and recomputing the probability of in-\n",
      "terest is the basic process of concern in Bayesian statistics. From a Bayesian\n",
      "perspective, we begin with some prior probability for some event, and we up-\n",
      "date this prior probability with new information to obtain a posterior prob-\n",
      "ability. The posterior probability can then be used as a prior probability in\n",
      "a subsequent analysis. From a Bayesian point of view, this is an appropriate\n",
      "strategy for conducting scientific research: We continue to gather data to eval-\n",
      "uate a particular scientific hypothesis; we do not begin anew (ignorant) each\n",
      "time we attempt to answer a hypothesis, because previous research provides\n",
      "us with a priori information concerning the merit of the hypothesis.\n",
      "\n",
      "\n",
      "\n",
      "50 3 Basics of Bayesian Statistics\n",
      "\n",
      "3.2 Bayes’ Theorem applied to probability distributions\n",
      "\n",
      "Bayes’ theorem, and indeed, its repeated application in cases such as the ex-\n",
      "ample above, is beyond mathematical dispute. However, Bayesian statistics\n",
      "typically involves using probability distributions rather than point probabili-\n",
      "ties for the quantities in the theorem. In the pregnancy example, we assumed\n",
      "the prior probability for pregnancy was a known quantity of exactly .15. How-\n",
      "ever, it is unreasonable to believe that this probability of .15 is in fact this\n",
      "precise. A cursory glance at various websites, for example, reveals a wide range\n",
      "for this probability, depending on a woman’s age, the date of her last men-\n",
      "strual cycle, her use of contraception, etc. Perhaps even more importantly,\n",
      "even if these factors were not relevant in determining the prior probability\n",
      "for being pregnant, our knowledge of this prior probability is not likely to be\n",
      "perfect because it is simply derived from previous samples and is not a known\n",
      "and fixed population quantity (which is precisely why different sources may\n",
      "give different estimates of this prior probability!). From a Bayesian perspec-\n",
      "tive, then, we may replace this value of .15 with a distribution for the prior\n",
      "pregnancy probability that captures our prior uncertainty about its true value.\n",
      "The inclusion of a prior probability distribution ultimately produces a poste-\n",
      "rior probability that is also no longer a single quantity; instead, the posterior\n",
      "becomes a probability distribution as well. This distribution combines the\n",
      "information from the positive test with the prior probability distribution to\n",
      "provide an updated distribution concerning our knowledge of the probability\n",
      "the woman is pregnant.\n",
      "\n",
      "Put generally, the goal of Bayesian statistics is to represent prior uncer-\n",
      "tainty about model parameters with a probability distribution and to update\n",
      "this prior uncertainty with current data to produce a posterior probability dis-\n",
      "tribution for the parameter that contains less uncertainty. This perspective\n",
      "implies a subjective view of probability—probability represents uncertainty—\n",
      "and it contrasts with the classical perspective. From the Bayesian perspective,\n",
      "any quantity for which the true value is uncertain, including model param-\n",
      "eters, can be represented with probability distributions. From the classical\n",
      "perspective, however, it is unacceptable to place probability distributions on\n",
      "parameters, because parameters are assumed to be fixed quantities: Only the\n",
      "data are random, and thus, probability distributions can only be used to rep-\n",
      "resent the data.\n",
      "\n",
      "Bayes’ Theorem, expressed in terms of probability distributions, appears\n",
      "as:\n",
      "\n",
      "f(θ|data) =\n",
      "f(data|θ)f(θ)\n",
      "\n",
      "f(data)\n",
      ", (3.2)\n",
      "\n",
      "where f(θ|data) is the posterior distribution for the parameter θ, f(data|θ)\n",
      "is the sampling density for the data—which is proportional to the Likeli-\n",
      "hood function, only differing by a constant that makes it a proper density\n",
      "function—f(θ) is the prior distribution for the parameter, and f(data) is the\n",
      "\n",
      "\n",
      "\n",
      "3.2 Bayes’ Theorem applied to probability distributions 51\n",
      "\n",
      "marginal probability of the data. For a continuous sample space, this marginal\n",
      "probability is computed as:\n",
      "\n",
      "f(data) =\n",
      "∫\n",
      "\n",
      "f(data|θ)f(θ)dθ,\n",
      "\n",
      "the integral of the sampling density multiplied by the prior over the sample\n",
      "space for θ. This quantity is sometimes called the “marginal likelihood” for the\n",
      "data and acts as a normalizing constant to make the posterior density proper\n",
      "(but see Raftery 1995 for an important use of this marginal likelihood). Be-\n",
      "cause this denominator simply scales the posterior density to make it a proper\n",
      "density, and because the sampling density is proportional to the likelihood\n",
      "function, Bayes’ Theorem for probability distributions is often stated as:\n",
      "\n",
      "Posterior ∝ Likelihood× Prior, (3.3)\n",
      "\n",
      "where the symbol “∝” means “is proportional to.”\n",
      "\n",
      "3.2.1 Proportionality\n",
      "\n",
      "As Equation 3.3 shows, the posterior density is proportional to the likelihood\n",
      "function for the data (given the model parameters) multiplied by the prior for\n",
      "the parameters. The prior distribution is often—but not always—normalized\n",
      "so that it is a true density function for the parameter. The likelihood function,\n",
      "however, as we saw in the previous chapter, is not itself a density; instead, it is\n",
      "a product of densities and thus lacks a normalizing constant to make it a true\n",
      "density function. Consider, for example, the Bernoulli versus binomial speci-\n",
      "fications of the likelihood function for the dichotomous voting data. First, the\n",
      "Bernoulli specification lacked the combinatorial expression to make the like-\n",
      "lihood function a true density function for either the data or the parameter.\n",
      "Second, although the binomial representation for the likelihood function con-\n",
      "stituted a true density function, it only constituted a true density for the data\n",
      "and not for the parameter p. Thus, when the prior distribution for a parameter\n",
      "is multiplied by the likelihood function, the result is also not a proper density\n",
      "function. Indeed, Equation 3.3 will be “off” by the denominator on the right\n",
      "side of Equation 3.2, in addition to whatever normalizing constant is needed\n",
      "to equalize the likelihood function and the sampling density p(data | θ).\n",
      "\n",
      "Fortunately, the fact that the posterior density is only proportional to the\n",
      "product of the likelihood function and prior is not generally a problem in\n",
      "Bayesian analysis, as the remainder of the book will demonstrate. However,\n",
      "a note is in order regarding what proportionality actually means. In brief, if\n",
      "a is proportional to b, then a and b only differ by a multiplicative constant.\n",
      "How does this translate to probability distributions? First, we need to keep in\n",
      "mind that, in a Bayesian analysis, model parameters are considered random\n",
      "quantities, whereas the data, having been already observed, are considered\n",
      "fixed quantities. This view is completely opposite that assumed under the\n",
      "\n",
      "\n",
      "\n",
      "52 3 Basics of Bayesian Statistics\n",
      "\n",
      "classical approach. Second, we need to recall from Chapter 2 that potential\n",
      "density functions often need to have a normalizing constant included to make\n",
      "them proper density functions, but we also need to recall that this normalzing\n",
      "constant only has the effect of scaling the density—it does not fundamentally\n",
      "change the relative frequencies of different values of the random variable.\n",
      "As we saw in Chapter 2, the normalizing constant is sometimes simply a\n",
      "true constant—a number—but sometimes the constant involves the random\n",
      "variable(s) themselves.\n",
      "\n",
      "As a general rule, when considering a univariate density, any term, say\n",
      "Q (no matter how complicated), that can be factored away from the random\n",
      "variable in the density—so that all the term(s) involving the random variable\n",
      "are simply multiples of Q—can be considered an irrelevant proportionality\n",
      "constant and can be eliminated from the density without affecting the results.\n",
      "\n",
      "In theory, this rule is fairly straightforward, but it is often difficult to apply\n",
      "for two key reasons. First, it is sometimes difficult to see whether a term can\n",
      "be factored out. For example, consider the following function for θ:\n",
      "\n",
      "f(θ) = e−θ+Q.\n",
      "\n",
      "It may not be immediately clear that Q here is an arbitrary constant with\n",
      "respect to θ, but it is. This function can be rewritten as:\n",
      "\n",
      "f(θ) = e−θ × eQ,\n",
      "\n",
      "using the algebraic rule that ea+b = eaeb. Thus, if we are considering f(θ)\n",
      "as a density function for θ, eQ would be an arbitrary constant and could be\n",
      "removed without affecting inference about θ. Thus, we could state without\n",
      "loss of information that:\n",
      "\n",
      "f(θ) ∝ e−θ.\n",
      "\n",
      "In fact, this particular function, without Q, is an exponential density for θ\n",
      "with parameter β = 1 (see the end of this chapter). With Q, it is proportional\n",
      "to an exponential density; it simply needs a normalizing constant of e−Q so\n",
      "that the function integrates to 1 over the sample space S = {θ : θ > 0}:∫ ∞\n",
      "\n",
      "0\n",
      "\n",
      "e−θ+Q dθ = −\n",
      "1\n",
      "\n",
      "e∞−Q\n",
      "+ eQ = eQ.\n",
      "\n",
      "Thus, given that this function integrates to eQ, e−Q renormalizes the integral\n",
      "to 1.\n",
      "\n",
      "A second difficulty with this rule is that multivariate densities sometimes\n",
      "make it difficult to determine what is an irrelevant constant and what is not.\n",
      "With Gibbs sampling, as we will discuss in the next chapter and throughout\n",
      "the remainder of the book, we generally break down multivariate densities into\n",
      "univariate conditional densities. When we do this, we can consider all terms\n",
      "not involving the random variable to which the conditional density applies to\n",
      "\n",
      "\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting example 53\n",
      "\n",
      "be proportionality constants. I will show this shortly in the last example in\n",
      "this chapter.\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting\n",
      "example\n",
      "\n",
      "To make the notion of Bayes’ Theorem applied to probability distributions\n",
      "concrete, consider the polling data from the previous chapter. In the previous\n",
      "chapter, we attempted to determine whether John F. Kerry would win the\n",
      "popular vote in Ohio, using the most recent CNN/USAToday/Gallup polling\n",
      "data. When we have a sample of data, such as potential votes for and against a\n",
      "candidate, and we assume they arise from a particular probability distribution,\n",
      "the construction of a likelihood function gives us the joint probability of the\n",
      "events, conditional on the parameter of interest: p(data|parameter). In the\n",
      "election polling example, we maximized this likelihood function to obtain a\n",
      "value for the parameter of interest—the proportion of Kerry voters in Ohio—\n",
      "that maximized the probability of obtaining the polling data we did. That\n",
      "estimated proportion (let’s call it K to minimize confusion) was .521. We\n",
      "then determined how uncertain we were about our finding that K = .521.\n",
      "To be more precise, we determined under some assumptions how far K may\n",
      "reasonably be from .521 and still produce the polling data we observed.\n",
      "\n",
      "This process of maximizing the likelihood function ultimately simply tells\n",
      "us how probable the data are under different values for K—indeed, that is\n",
      "precisely what a likelihood function is —but our ultimate question is really\n",
      "whether Kerry will win, given the polling data. Thus, our question of interest\n",
      "is “what is p(K > .5),” but the likelihood function gives us p(poll data |K)—\n",
      "that is, the probability of the data given different values of K.\n",
      "\n",
      "In order to answer the question of interest, we need to apply Bayes’ The-\n",
      "orem in order to obtain a posterior distribution for K and then evaluate\n",
      "p(K > .5) using this distribution. Bayes’ Theorem says:\n",
      "\n",
      "f(K|poll data) ∝ f(poll data|K)f(K),\n",
      "or verbally: The posterior distribution for K, given the sample data, is propor-\n",
      "tional to the probability of the sample data, given K, multiplied by the prior\n",
      "probability for K. f(poll data|K) is the likelihood function (or sampling den-\n",
      "sity for the data). As we discussed in the previous chapter, it can be viewed\n",
      "as a binomial distribution with x = 556 “successes” (votes for Kerry) and\n",
      "n − x = 511 “failures” (votes for Bush), with n = 1, 067 total votes between\n",
      "the two candidates. Thus,\n",
      "\n",
      "f(poll data|K) ∝ K556(1−K)511.\n",
      "What remains to be specified to complete the Bayesian development of the\n",
      "model is a prior probability distribution for K. The important question is:\n",
      "How do we do construct a prior?\n",
      "\n",
      "\n",
      "\n",
      "54 3 Basics of Bayesian Statistics\n",
      "\n",
      "3.3.1 Specification of a prior: The beta distribution\n",
      "\n",
      "Specification of an appropriate prior distribution for a parameter is the most\n",
      "substantial aspect of a Bayesian analysis that differentiates it from a classi-\n",
      "cal analysis. In the pregnancy example, the prior probability for pregnancy\n",
      "was said to be a point estimate of .15. However, as we discussed earlier, that\n",
      "specification did not consider that that prior probability is not known with\n",
      "complete certainty. Thus, if we wanted to be more realistic in our estimate of\n",
      "the posterior probability of pregnancy, we could compute the posterior prob-\n",
      "ability under different values for the prior probability to obtain a collection\n",
      "of possible posterior probabilities that we could then consider and compare\n",
      "to determine which estimated posterior probability we thought was more rea-\n",
      "sonable. More efficiently, we could replace the point estimate of .15 with a\n",
      "probability distribution that represented (1) the plausible values of the prior\n",
      "probability of pregnancy and (2) their relative merit. For example, we may\n",
      "give considerable prior weight to the value .15 with diminishing weight to\n",
      "values of the prior probability that are far from .15.\n",
      "\n",
      "Similarly, in the polling data example, we can use a distribution to repre-\n",
      "sent our prior knowledge and uncertainty regarding K. An appropriate prior\n",
      "distribution for an unknown proportion such as K is a beta distribution. The\n",
      "pdf of the beta distribution is:\n",
      "\n",
      "f(K | α, β) =\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      "Kα−1(1−K)β−1,\n",
      "\n",
      "where Γ (a) is the gamma function applied to a and 0 < K < 1.2 The param-\n",
      "eters α and β can be thought of as prior “successes” and “failures,” respec-\n",
      "tively. The mean and variance of a beta distribution are determined by these\n",
      "parameters:\n",
      "\n",
      "E(K | α, β) =\n",
      "α\n",
      "\n",
      "α + β\n",
      "\n",
      "and\n",
      "Var(K | α, β) =\n",
      "\n",
      "αβ\n",
      "\n",
      "(α + β)2(α + β + 1)\n",
      ".\n",
      "\n",
      "This distribution looks similar to the binomial distribution we have already\n",
      "discussed. The key difference is that, whereas the random variable is x and the\n",
      "key parameter is K in the binomial distribution, the random variable is K and\n",
      "the parameters are α and β in the beta distribution. Keep in mind, however,\n",
      "from a Bayesian perspective, all unknown quantities can be considered random\n",
      "variables.\n",
      "\n",
      "2 The gamma function is the generalization of the factorial to nonintegers. For\n",
      "integers, Γ (a) = (a − 1)!. For nonintegers, Γ (a) =\n",
      "\n",
      "R∞\n",
      "0\n",
      "\n",
      "xa−1 e−x dx. Most soft-\n",
      "ware packages will compute this function, but it is often unnecessary in practice,\n",
      "because it tends to be part of the normalizing constant in most problems.\n",
      "\n",
      "\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting example 55\n",
      "\n",
      "How do we choose α and β for our prior distribution? The answer to this\n",
      "question depends on at least two factors. First, how much information prior\n",
      "to this poll do we have about the parameter K? Second, how much stock\n",
      "do we want to put into this prior information? These are questions that all\n",
      "Bayesian analyses must face, but contrary to the view that this is a limitation\n",
      "of Bayesian statistics, the incorporation of prior information can actually be\n",
      "an advantage and provides us considerable flexibility. If we have little or no\n",
      "prior information, or we want to put very little stock in the information we\n",
      "have, we can choose values for α and β that reduce the distribution to a\n",
      "uniform distribution. For example, if we let α = 1 and β = 1, we get\n",
      "\n",
      "f(p|α = 1, β = 1) ∝ K1−1=0(1−K)1−1=0 = 1,\n",
      "\n",
      "which is proportional to a uniform distribution on the allowable interval for\n",
      "K ([0,1]). That is, the prior distribution is flat, not producing greater a priori\n",
      "weight for any value of K over another. Thus, the prior distribution will have\n",
      "little effect on the posterior distribution. For this reason, this type of prior is\n",
      "called “noninformative.”3\n",
      "\n",
      "At the opposite extreme, if we have considerable prior information and we\n",
      "want it to weigh heavily relative to the current data, we can use large values of\n",
      "α and β. A little algebraic manipulation of the formula for the variance reveals\n",
      "that, as α and β increase, the variance decreases, which makes sense, because\n",
      "adding additional prior information ought to reduce our uncertainty about the\n",
      "parameter. Thus, adding more prior successes and failures (increasing both\n",
      "parameters) reduces prior uncertainty about the parameter of interest (K).\n",
      "Finally, if we have considerable prior information but we do not wish for it to\n",
      "weigh heavily in the posterior distribution, we can choose moderate values of\n",
      "the parameters that yield a mean that is consistent with the previous research\n",
      "but that also produce a variance around that mean that is broad.\n",
      "\n",
      "Figure 3.1 displays some beta distributions with different values of α and\n",
      "β in order to clarify these ideas. All three displayed beta distributions have\n",
      "a mean of .5, but they each have different variances as a result of having α\n",
      "and β parameters of different magnitude. The most-peaked beta distribution\n",
      "has parameters α = β = 50. The least-peaked distribution is actually flat—\n",
      "uniform—with parameters α = β = 1. As with the binomial distribution, the\n",
      "beta distribution becomes skewed if α and β are unequal, but the basic idea\n",
      "is the same: the larger the parameters, the more prior information and the\n",
      "narrower the density.\n",
      "\n",
      "Returning to the voting example, CNN/USAToday/Gallup had conducted\n",
      "three previous polls, the results of which could be treated as prior information.\n",
      "\n",
      "3 Virtually all priors, despite sometimes being called “noninformative,” impart\n",
      "some information to the posterior distribution. Another way to say this is that\n",
      "claiming ignorance is, in fact, providing some information! However, flat priors\n",
      "generally have little weight in affecting posterior inference, and so they are called\n",
      "noninformative. See Box and Tiao 1973; Gelman et al. 1995; and Lee 1989.\n",
      "\n",
      "\n",
      "\n",
      "56 3 Basics of Bayesian Statistics\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "K\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Beta(1,1)\n",
      "\n",
      "Beta(5,5)\n",
      "\n",
      "Beta(50,50)\n",
      "\n",
      "Fig. 3.1. Three beta distributions with mean α/(α + β) = .5.\n",
      "\n",
      "These additional polling data are shown in Table 3.1.4 If we consider these\n",
      "previous polls to provide us prior knowledge about the election, then our prior\n",
      "information consists of 1,008 (339 + 325 + 344) votes for Bush and 942 votes\n",
      "for Kerry (346 + 312 + 284) out of a total of 1,950 votes.\n",
      "\n",
      "This prior information can be included by using a beta distribution with\n",
      "parameters α = 942 and β = 1008:\n",
      "\n",
      "f(K | α, β) ∝ K942−1(1−K)1008−1.\n",
      "\n",
      "4 The data appear to show some trending, in the sense that the proportion stating\n",
      "that they would vote for Bush declined across time, whereas the proportion stating\n",
      "that they would vote for Kerry increased. This fact may suggest consideration\n",
      "of a more complex model than discussed here. Nonetheless, given a margin of\n",
      "error of ±4% for each of these additional polls, it is unclear whether the trend\n",
      "is meaningful. In other words, we could simply consider these polls as repeated\n",
      "samples from the same, unchanging population. Indeed, the website shows the\n",
      "results of 22 polls taken by various organizations, and no trending is apparent in\n",
      "the proportions from late September on.\n",
      "\n",
      "\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting example 57\n",
      "\n",
      "Table 3.1. CNN/USAToday/Gallup 2004 presidential election polls.\n",
      "\n",
      "Date n % for Bush ≈ n % for Kerry ≈ n\n",
      "Oct 17-20 706 48% 339 49% 346\n",
      "Sep 25-28 664 49% 325 47% 312\n",
      "Sep 4-7 661 52% 344 43% 284\n",
      "TOTAL 2,031 1,008 942\n",
      "\n",
      "Note: Proportions and candidate-specific sample sizes may not add to 100% of total\n",
      "sample n, because proportions opting for third-party candidates have been excluded.\n",
      "\n",
      "After combining this prior with the binomial likelihood for the current sample,\n",
      "we obtain the following posterior density for K:\n",
      "\n",
      "p(K | α, β, x) ∝ K556(1−K)511K941(1−K)1007 = K1497(1−K)1518.\n",
      "\n",
      "This posterior density is also a beta density, with parameters α = 1498 and\n",
      "β = 1519, and highlights the important concept of “conjugacy” in Bayesian\n",
      "statistics. When the prior and likelihood are of such a form that the poste-\n",
      "rior distribution follows the same form as the prior, the prior and likelihood\n",
      "are said to be conjugate. Historically, conjugacy has been very important to\n",
      "Bayesians, because, prior to the development of the methods discussed in this\n",
      "book, using conjugate priors/likelihoods with known forms ensured that the\n",
      "posterior would be a known distribution that could be easily evaluated to\n",
      "answer the scientific question of interest.\n",
      "\n",
      "Figure 3.2 shows the prior, likelihood, and posterior densities. The likeli-\n",
      "hood function has been normalized as a proper density for K, rather than x.\n",
      "The figure shows that the posterior density is a compromise between the prior\n",
      "distribution and the likelihood (current data). The prior is on the left side of\n",
      "the figure; the likelihood is on the right side; and the posterior is between,\n",
      "but closer to the prior. The reason the posterior is closer to the prior is that\n",
      "the prior contained more information than the likelihood: There were 1,950\n",
      "previously sampled persons and only 1,067 in the current sample.5\n",
      "\n",
      "With the posterior density determined, we now can summarize our up-\n",
      "dated knowledge about K, the proportion of voters in Ohio who will vote for\n",
      "Kerry, and answer our question of interest: What is the probability that Kerry\n",
      "would win Ohio? A number of summaries are possible, given that we have a\n",
      "posterior distribution with a known form (a beta density). First, the mean\n",
      "of K is 1498/(1498 + 1519) = .497, and the median is also .497 (found using\n",
      "the qbeta function in R). The variance of this beta distribution is .00008283\n",
      "(standard deviation=.0091). If we are willing to assume that this beta distri-\n",
      "bution is approximately normal, then we could construct a 95% interval based\n",
      "on a normal approximation and conclude that the proportion of Ohio voters\n",
      "5 This movement of the posterior distribution away from the prior and toward the\n",
      "\n",
      "likelihood is sometimes called “Bayesian shrinkage” (see Gelman et al. 1995).\n",
      "\n",
      "\n",
      "\n",
      "58 3 Basics of Bayesian Statistics\n",
      "\n",
      "0.40 0.45 0.50 0.55 0.60\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "6\n",
      "\n",
      "0\n",
      "\n",
      "K\n",
      "\n",
      "f(\n",
      "K\n",
      "\n",
      ")\n",
      "\n",
      "Prior (Normalized)\n",
      "Likelihood\n",
      "\n",
      "Posterior\n",
      "\n",
      "Fig. 3.2. Prior, likelihood, and posterior for polling data example: The likelihood\n",
      "function has been normalized as a density for the parameter K.\n",
      "\n",
      "who would vote for Kerry falls between .479 and .515 (.497±1.96×.0091). This\n",
      "interval is called a “credible interval,” a “posterior probability interval,” or a\n",
      "“probability interval,” and it has a simpler interpretation than the classical\n",
      "confidence interval. Using this interval, we can say simply that the proportion\n",
      "K falls in this interval with probability .95.\n",
      "\n",
      "If, on the other hand, we are not willing to assume that this posterior\n",
      "density is approximately normal, we can directly compute a 95% probability\n",
      "interval by selecting the lower and upper values of this beta density that\n",
      "produce the desired interval. That is, we can determine the values of this beta\n",
      "density below which 2.5% of the distribution falls and above which 2.5% of\n",
      "the distribution falls. These values are .479 and .514, which are quite close to\n",
      "those under the normal approximation.\n",
      "\n",
      "These results suggest that, even with the prior information, the election\n",
      "may have been too close to call, given that the interval estimate for K captures\n",
      ".5. However, the substantive question—what is the probability that Kerry\n",
      "would win—can also be answered within the Bayesian framework. This prob-\n",
      "ability is the probability that Kerry will get more than half of the votes, which\n",
      "\n",
      "\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting example 59\n",
      "\n",
      "is simply the probability that K > .5. This probability can be directly com-\n",
      "puted from the beta distribution as the integral of this density from .5 to 1\n",
      "(the mass of the curve to the right of .5; see Figure 3.3). The result is .351,\n",
      "which means that Kerry did not have a favorable chance to win Ohio, given\n",
      "the complete polling data.\n",
      "\n",
      "0.40 0.45 0.50 0.55 0.60\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "K\n",
      "\n",
      "f(\n",
      "K\n",
      "\n",
      ")\n",
      "\n",
      "p(K>.5)=.351p(K<.5)=.649\n",
      "\n",
      "Fig. 3.3. Posterior for polling data example: A vertical line at K = .5 is included to\n",
      "show the area needed to be computed to estimate the probability that Kerry would\n",
      "win Ohio.\n",
      "\n",
      "In fact, Kerry did not win Ohio; he obtained 48.9% of the votes cast for\n",
      "either Kerry or Bush. The classical analysis did not yield this conclusion: It\n",
      "simply suggested that the results were too close to call. The Bayesian anal-\n",
      "ysis, on the other hand, while recognizing that the election would be close,\n",
      "suggested that there was not a very high probability that Kerry would win.\n",
      "The price that had to be paid for reaching this conclusion, however, was (1)\n",
      "we had to be willing to specify a prior probability for K, and (2) we had to\n",
      "be willing to treat the parameter of interest as a random, and not a fixed,\n",
      "quantity.\n",
      "\n",
      "\n",
      "\n",
      "60 3 Basics of Bayesian Statistics\n",
      "\n",
      "3.3.2 An alternative model for the polling data: A gamma prior/\n",
      "Poisson likelihood approach\n",
      "\n",
      "In this section, I repeat the analysis from the previous section. However, in-\n",
      "stead of considering the problem as a binomial problem with the proportion\n",
      "parameter p, I consider the problem as a Poisson distribution problem with\n",
      "rate parameter λ. As we discussed in the previous chapter, the Poisson dis-\n",
      "tribution is a distribution for count variables; we can consider an individual’s\n",
      "potential vote for Kerry as a discrete count that takes values of either 0 or 1.\n",
      "From that perspective, the likelihood function for the 1,067 sample members\n",
      "in the most recent survey prior to the election is:\n",
      "\n",
      "L(λ|Y ) =\n",
      "1067∏\n",
      "i=1\n",
      "\n",
      "λyie−λ\n",
      "\n",
      "yi!\n",
      "=\n",
      "\n",
      "λ\n",
      "P1067\n",
      "\n",
      "i=1 yie−1067λ∏1067\n",
      "i=1 yi!\n",
      "\n",
      ",\n",
      "\n",
      "where yi is the 0 (Bush) or 1 (Kerry) vote of the ith individual.\n",
      "As in the binomial example, we would probably like to include the previ-\n",
      "\n",
      "ous survey data in our prior distribution. A conjugate prior for the Poisson\n",
      "distribution is a gamma distribution. The pdf of the gamma distribution is as\n",
      "follows. If x ∼ gamma(α, β), then:\n",
      "\n",
      "f(x) =\n",
      "βα\n",
      "\n",
      "Γ (α)\n",
      "xα−1e−βx.\n",
      "\n",
      "The parameters α and β in the gamma distribution are shape and inverse-\n",
      "scale parameters, respectively. The mean of a gamma distribution is α/β, and\n",
      "the variance is α/β2. Figure 3.4 shows four different gamma distributions. As\n",
      "the plot shows, the distribution is very flexible: Slight changes in the α and\n",
      "β parameters—which can take any non-negative value—yield highly variable\n",
      "shapes and scales for the density.\n",
      "\n",
      "For the moment, we will leave α and β unspecified in our voting model so\n",
      "that we can see how they enter into the posterior distribution. If we combine\n",
      "this gamma prior with the likelihood function, we obtain:\n",
      "\n",
      "p(λ | Y ) ∝\n",
      "(\n",
      "\n",
      "βα\n",
      "\n",
      "Γ (α)\n",
      "\n",
      ")\n",
      "λα−1e−βλ\n",
      "\n",
      "(\n",
      "1∏1067\n",
      "\n",
      "i=1 yi!\n",
      "\n",
      ")\n",
      "λ\n",
      "\n",
      "P1067\n",
      "i=1 yie−1067λ.\n",
      "\n",
      "This expression can be simplified by combining like terms and excluding the\n",
      "arbitrary proportionality constants (the terms in parentheses, which do not\n",
      "include λ) to obtain:\n",
      "\n",
      "p(λ | y) ∝ λ\n",
      "P1067\n",
      "\n",
      "i=1 yi+α−1e−(1067+β)λ.\n",
      "\n",
      "Given that each yi is either a 0 (vote for Bush) or 1 (vote for Kerry),\n",
      "∑1067\n",
      "\n",
      "i=1 yi\n",
      "is simply the count of votes for Kerry in the current sample (=556). Thus,\n",
      "just as in the binomial example, the parameters α and β—at least in this\n",
      "\n",
      "\n",
      "\n",
      "3.3 Bayes’ Theorem with distributions: A voting example 61\n",
      "\n",
      "0 5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "G(1,1)\n",
      "G(10,1)\n",
      "G(1,.1)\n",
      "G(20,2\n",
      "\n",
      "Fig. 3.4. Some examples of the gamma distribution.\n",
      "\n",
      "particular model—appear to capture prior “successes” and “failures.” Specif-\n",
      "ically, α is the count of prior “successes,” and β is the total number of prior\n",
      "observations. The mean of the gamma distribution (α/β) also supports this\n",
      "conclusion. Thus, as in the beta prior/binomial likelihood example, if we want\n",
      "to incorporate the data from previous survey into the prior distribution, we\n",
      "can set α = 942 and β = 942 + 1008 = 1950 to obtain the following posterior:\n",
      "\n",
      "p(λ | Y ) ∝ λ556+942−1e−(1067+1950)λ = λ1497e−3017λ.\n",
      "\n",
      "Thus, the posterior density is also a gamma density with parameters α =\n",
      "1498 and β = 3017. Because the gamma density is a known density, we can\n",
      "immediately compute the posterior mean and standard deviation for λ: λ̄ =\n",
      ".497; σ̂λ = .0128. If we wish to construct a 95% probability/credible interval\n",
      "for λ, and we are willing to make a normal approximation given the large\n",
      "sample size, we can construct the interval as .497± 1.96× .0128. This result\n",
      "gives us an interval estimate of [.472, .522] for λ. On the other hand, if we\n",
      "wish to compute the interval directly using integration of the gamma density\n",
      "(i.e., the cdf for the gamma distribution), we obtain an interval of [.472, .522].\n",
      "\n",
      "\n",
      "\n",
      "62 3 Basics of Bayesian Statistics\n",
      "\n",
      "In this case, the normal-theory interval and the analytically derived interval\n",
      "are the same when rounded to three decimal places.\n",
      "\n",
      "How does this posterior inference compare with that obtained using the\n",
      "beta prior/binomial likelihood approach? The means for K in the beta/binomial\n",
      "approach and for λ in the gamma/Poisson approach are identical. The inter-\n",
      "vals are also quite comparable, but the interval in this latter approach is\n",
      "wider—about 42% wider. If we wish to determine the probability that Kerry\n",
      "would win Ohio, we simply need to compute p(λ > .5), which equals .390.\n",
      "Thus, under this model, Kerry had a probability of winning of .390, which is\n",
      "still an unfavorable result, although it is a slightly greater probability than\n",
      "the beta/binomial result of .351.\n",
      "\n",
      "Which model is to be preferred? In this case, the substantive conclusion\n",
      "we reached was comparable for the two models: Kerry was unlikely to win\n",
      "Ohio. So, it does not matter which model we choose. The fact that the two\n",
      "models produced comparable results is reassuring, because the conclusion does\n",
      "not appear to be very sensitive to choice of model. Ultimately, however, we\n",
      "should probably place greater emphasis on the beta/binomial model, because\n",
      "the Poisson distribution is a distribution for counts, and our data, which\n",
      "consisted of dichotomous outcomes, really does not fit the bill. Consider the\n",
      "parameter λ: There is no guarantee with the gamma/Poisson setup that λ will\n",
      "be less than 1. This lack of limit could certainly be problematic if we had less\n",
      "data, or if the underlying proportion favoring Kerry were closer to 1. In such\n",
      "a case, the upper bound on the interval for λ may have exceeded 1, and our\n",
      "results would therefore be suspect. In this particular case, however, we had\n",
      "enough data and prior information that ultimately made the interval width\n",
      "very narrow, and so the bounding problem was not an issue. Nonetheless, the\n",
      "beta/binomial setup is a more natural model for the voting data.\n",
      "\n",
      "3.4 A normal prior–normal likelihood example with σ2\n",
      "\n",
      "known\n",
      "\n",
      "The normal distribution is one of the most common distributions used in\n",
      "statistics by social scientists, in part because many social phenomena in fact\n",
      "follow a normal distribution. Thus, it is not uncommon for a social scientist\n",
      "to use a normal distribution as the basis for a likelihood function for a set of\n",
      "data. Here, I develop a normal distribution problem, but for the sake of keeping\n",
      "this example general for use in later chapters, I used a contrived scenario and\n",
      "keep the mathematics fairly general. The purpose at this point is simply to\n",
      "illustrate a Bayesian approach with a multivariate posterior distribution.6\n",
      "\n",
      "6 The normal distribution involves two parameters: the mean (µ) and variance (σ2).\n",
      "When considered as a density for x, it is univariate, but when a normal likelihood\n",
      "and some prior for the parameters are combined, the result is a joint posterior\n",
      "distribution for µ and σ2, which makes the posterior a multivariate density.\n",
      "\n",
      "\n",
      "\n",
      "3.4 A normal prior–normal likelihood example with σ2 known 63\n",
      "\n",
      "Suppose that we have a class of 30 students who have recently taken a\n",
      "midterm exam, and the mean grade was x̄ = 75 with a standard deviation of\n",
      "σ = 10. Note that for now we have assumed that the variance is known, hence,\n",
      "the use of σ rather than s. We have taught the course repeatedly, semester\n",
      "after semester, and past test means have given us an overall mean µ of 70, but\n",
      "the class means have varied from class to class, giving us a standard deviation\n",
      "for the class means of τ = 5. That is, τ reflects how much our class means have\n",
      "varied and does not directly reflect the variability of individual test scores.\n",
      "We will discuss this more in depth momentarily.\n",
      "\n",
      "Our goal is ultimately to update our knowledge of µ, the unobservable\n",
      "population mean test score with the new test grade data. In other words, we\n",
      "wish to find f(µ|x). Bayes’ Theorem tells us that:\n",
      "\n",
      "f(µ|X) ∝ f(X|µ)f(µ),\n",
      "\n",
      "where f(X|µ) is the likelihood function for the current data, and f(µ) is the\n",
      "prior for the test mean. (At the moment, I am omitting σ2 from the notation).\n",
      "If we assume the current test scores are normally distributed with a mean\n",
      "equal to µ and variance σ2, then our likelihood function for X is:\n",
      "\n",
      "f(X|µ) ∝ L(µ|X) =\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "2πσ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(xi − µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Furthermore, our previous test results have provided us with an overall mean\n",
      "of 70, but we are uncertain about µ’s actual value, given that class means\n",
      "vary semester by semester (giving us τ = 5). So our prior distribution for µ\n",
      "is:\n",
      "\n",
      "f(µ) =\n",
      "1\n",
      "\n",
      "√\n",
      "2πτ2\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "(µ−M)2\n",
      "\n",
      "2τ2\n",
      "\n",
      "}\n",
      ",\n",
      "\n",
      "where in this expression, µ is the random variable, with M as the prior mean\n",
      "(=70), and τ2 (=25) reflects the variation of µ around M .\n",
      "\n",
      "Our posterior is the product of the likelihood and prior, which gives us:\n",
      "\n",
      "f(µ|X) ∝\n",
      "1\n",
      "\n",
      "√\n",
      "τ2σ2\n",
      "\n",
      "exp\n",
      "{\n",
      "−(µ−M)2\n",
      "\n",
      "2τ2\n",
      "+\n",
      "−\n",
      "∑n\n",
      "\n",
      "i=1(xi − µ)\n",
      "2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "This posterior can be reexpressed as a normal distribution for µ, but it takes\n",
      "some algebra in order to see this. First, since the terms outside the exponential\n",
      "are simply normalizing constants with respect to µ, we can drop them and\n",
      "work with the terms inside the exponential function. Second, let’s expand\n",
      "the quadratic components and the summations. For the sake of simplicty, I\n",
      "temporarily drop the exponential function in this expression:\n",
      "\n",
      "(−1/2)\n",
      "[\n",
      "µ2 − 2µM + M2\n",
      "\n",
      "τ2\n",
      "+\n",
      "∑\n",
      "\n",
      "x2 − 2nx̄µ + nµ2\n",
      "\n",
      "σ2\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "64 3 Basics of Bayesian Statistics\n",
      "\n",
      "Using this expression, any term that does not include µ can be viewed as\n",
      "a proportionality constant, can be factored out of the exponent, and can be\n",
      "dropped (recall that ea+b = eaeb). After obtaining common denominators for\n",
      "the remaining terms by cross-multiplying by each of the individual denomi-\n",
      "nators and dropping proportionality constants, we are left with:\n",
      "\n",
      "(−1/2)\n",
      "[\n",
      "σ2µ2 − 2σ2µM − 2τ2nx̄µ + τ2nµ2\n",
      "\n",
      "σ2τ2\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "From here, we need to combine terms involving µ2 and those involving µ:\n",
      "\n",
      "(−1/2)\n",
      "[\n",
      "(nτ2 + σ2)µ2 − 2(σ2M + τ2nx̄)µ\n",
      "\n",
      "σ2τ2\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "Dividing the numerator and denominator of this fraction by the (nτ2 + σ2)\n",
      "in front of µ2 yields:\n",
      "\n",
      "(−1/2)\n",
      "\n",
      "\n",
      "µ2 − 2µ (σ2M+nτ2x̄)(nτ2+σ2)\n",
      "\n",
      "σ2τ2\n",
      "\n",
      "(nτ2+σ2)\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "Finally, all we need to do is to complete the square in µ and discard any\n",
      "remaining constants to obtain:\n",
      "\n",
      "(−1/2)\n",
      "\n",
      "\n",
      "\n",
      "(\n",
      "µ− (σ\n",
      "\n",
      "2M+nτ2x̄)\n",
      "(nτ2+σ2)\n",
      "\n",
      ")2\n",
      "σ2τ2\n",
      "\n",
      "(nτ2+σ2)\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "This result shows that our updated µ is normally distributed with mean\n",
      "(σ2M + τ2nx̄)/(nτ2 + σ2) and variance (σ2τ2)/(nτ2 + σ2). Notice how the\n",
      "posterior mean is a weighted combination of the prior mean and the sample\n",
      "mean. The prior mean is multiplied by the known variance of test scores in the\n",
      "sample, σ2, whereas the sample mean x̄ is multiplied by n and by the prior\n",
      "variance τ2. This shows first that the sample mean will tend to have more\n",
      "weight than the prior mean (because of the n multiple), but also that the\n",
      "prior and sample variances affect the weighting of the means. If the sample\n",
      "variance is large, then the prior mean has considerable weight in the poste-\n",
      "rior; if the prior variance is large, the sample mean has considerable weight in\n",
      "the posterior. If the two quantities are equal (σ2 = τ2), then the calculation\n",
      "reduces to (M +nx̄)/(n+1), which means that the prior mean will only have\n",
      "a weight of 1/(n + 1) in the posterior.\n",
      "\n",
      "In this particular example, our posterior mean would be:\n",
      "\n",
      "(100× 70) + (25× 30× 75)/(30× 25 + 100) = 74.4.\n",
      "\n",
      "Thus, our result is clearly more heavily influenced by the sample data than\n",
      "by the prior. One thing that must be kept in mind but is easily forgotten is\n",
      "that our updated variance parameter (which is 20—the standard deviation is\n",
      "\n",
      "\n",
      "\n",
      "3.4 A normal prior–normal likelihood example with σ2 known 65\n",
      "\n",
      "therefore 4.47) reflects our uncertainty about µ. This estimate is smaller than\n",
      "both the prior variance and the sample variance, and it is much closer to τ2\n",
      "\n",
      "than to σ2. Why? Again, this quantity reflects how much µ varies (or, put\n",
      "another way, how much uncertainty we have in knowing M , the true value\n",
      "of µ) and not how much we know about any particular sample. Thus, the\n",
      "fact that our sample standard deviation was 10 does not play a large role in\n",
      "changing our minds about uncertainty in µ, especially given that the sample\n",
      "mean was not that different from the prior mean. In other words, our sample\n",
      "mean is sufficiently close to our prior mean µ so that we are unconvinced that\n",
      "the variance of µ around M should be larger than it was. Indeed, the data\n",
      "convince us that our prior variance should actually be smaller, because the\n",
      "current sample mean is well within the range around M implied by our prior\n",
      "value for τ .\n",
      "\n",
      "3.4.1 Extending the normal distribution example\n",
      "\n",
      "The natural extension of the previous example in which the variance σ2 was\n",
      "considered known is to consider the more realistic case in which the variance is\n",
      "not known. Recall that, ultimately in the previous example, we were interested\n",
      "in the quantity µ—the overall mean test score. Previous data had given us an\n",
      "estimate of µ, but we were still uncertain about its value, and thus, we used\n",
      "τ to represent our uncertainty in µ. We considered σ2 to be a known quantity\n",
      "(10). In reality, we typically do not know σ2 any more than we know µ, and\n",
      "thus we have two quantities of interest that we should be updating with new\n",
      "information. A full probability model for µ and σ2 would look like:\n",
      "\n",
      "f(µ, σ2|x) ∝ f(x|µ, σ2)f(µ, σ2).\n",
      "\n",
      "This model is similar to the one in the example above, but we have now\n",
      "explicitly noted that σ2 is also an unknown quantity, by including it in the\n",
      "prior distribution. Therefore, we now need to specify a joint prior for both µ\n",
      "and σ2, and not just a prior for µ. If we assume µ and σ2 are independent—\n",
      "and this is a reasonable assumption as we mentioned in the previous chapter;\n",
      "there’s no reason the two parameters need be related—then we can consider\n",
      "p(µ, σ2) = p(µ)p(σ2) and establish separate priors for each.\n",
      "\n",
      "In the example above, we established the prior for µ to be µ ∼ N(M, τ2),\n",
      "where M was the prior mean (70) and τ2 was the measure of uncertainty\n",
      "we had in µ. We did not, however, specify a prior for σ2, but we used σ2 to\n",
      "update our knowledge of τ .7\n",
      "\n",
      "How do we specify a prior distribution for µ and σ2 in a more general case?\n",
      "Unlike in the previous example, we often do not have prior information about\n",
      "these parameters, and so we often wish to develop noninformative priors for\n",
      "7 Recall from the CLT that x̄ ∼ N(µ, σ2/n); thus σ2 and τ2 are related: σ2/n\n",
      "\n",
      "should be an estimate for τ2, and so treating σ2 as fixed yields an updated τ2\n",
      "\n",
      "that depends heavily on the new sample data.\n",
      "\n",
      "\n",
      "\n",
      "66 3 Basics of Bayesian Statistics\n",
      "\n",
      "them. There are several ways to do this in the normal distribution problem,\n",
      "but two of the most common approaches lead to the same prior. One approach\n",
      "is to assign a uniform prior over the real line for µ and the same uniform prior\n",
      "for log(σ2). We assign a uniform prior on log(σ2) because σ2 is a nonega-\n",
      "tive quantity, and the transformation to log(σ2) stretches this new parameter\n",
      "across the real line. If we transform the uniform prior on log(σ2) into a density\n",
      "for σ2, we obtain p(σ2) ∝ 1/σ2.8 Thus, our joint prior is: p(µ, σ2) ∝ 1/σ2.\n",
      "\n",
      "A second way to obtain this prior is to give µ and σ2 proper prior distribu-\n",
      "tions (not uniform over the real line, which is improper). If we continue with\n",
      "the assumption that µ ∼ N(M, τ2), we can choose values of M and τ2 that\n",
      "yield a flat distribution. For example, if we let µ ∼ N(0, 10000), we have a\n",
      "very flat prior for µ. We can also choose a relatively noninformative prior for\n",
      "σ2 by first noting that variance parameters follow an inverse gamma distri-\n",
      "bution (see the next section) and then choosing values for the inverse gamma\n",
      "distribution that produce a noninformative prior. If σ2 ∼ IG(a, b), the pdf\n",
      "appears as:\n",
      "\n",
      "f(σ2|a, b) ∝ (σ2)−(a+1)e−β/(σ\n",
      "2).\n",
      "\n",
      "In the limit, if we let the parameters a and b approach 0, a noninformative\n",
      "prior is obtained as 1/σ2. Strictly speaking, however, if a and b are 0, the\n",
      "distribution is improper, but we can let both parameters approach 0. We can\n",
      "then use this as our prior for σ2 (that is, σ2 ∼ IG(0, 0); p(σ2) ∝ 1/σ2). There\n",
      "are other ways to arrive at this choice for the prior distribution for µ and σ,\n",
      "but I will not address them here (see Gelman et al. 1995).\n",
      "\n",
      "The resulting posterior for µ and σ2, if we assume a joint prior of 1/σ2 for\n",
      "these parameters, is:\n",
      "\n",
      "f(µ, σ2|X) ∝\n",
      "1\n",
      "σ2\n",
      "\n",
      "n∏\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "2πσ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(xi − µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ". (3.4)\n",
      "\n",
      "Unlike in the previous example, however, this is a joint posterior density\n",
      "for two parameters rather than one. Yet we can determine the conditional\n",
      "posterior distributions for both parameters, using the rule discussed in the\n",
      "previous chapter that, generally, f(x|y) ∝ f(x, y).\n",
      "\n",
      "Determining the form for the posterior density for µ follows the same logic\n",
      "as in the previous section. First, we carry out the product over all observations.\n",
      "Next, we expand the quadratic, eliminate terms that are constant with respect\n",
      "to µ and rearrange the terms with the µ2 term first. Doing so yields:\n",
      "\n",
      "8 This transformation of variables involves a Jacobian, as discussed in the previous\n",
      "chapter. Let m = log(σ2), and let p(m) ∝ constant. Then p(σ2) ∝ constant× J ,\n",
      "where J is the Jacobian of the transformation from m to σ2. The Jacobian is then\n",
      "dm/dσ2 = 1/σ2. See DeGroot (1986) for a fuller exposition of this process, and\n",
      "see any introductory calculus book for a general discussion of transformations of\n",
      "variables. See Gelman et al. 1995 for further discussion of this prior.\n",
      "\n",
      "\n",
      "\n",
      "3.4 A normal prior–normal likelihood example with σ2 known 67\n",
      "\n",
      "f(µ|X, σ2) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "nµ2 − 2nx̄µ\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Next, to isolate µ2, we can divide the numerator and denominator by n.\n",
      "Finally, we can complete the square in µ to find:\n",
      "\n",
      "f(µ|X, σ2) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "(µ− x̄)2\n",
      "\n",
      "2σ2/n\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "This result shows us that the conditional distribution for µ|X, σ2 ∼ N(x̄, σ\n",
      "2\n",
      "\n",
      "n\n",
      "),\n",
      "\n",
      "which should look familiar. That is, this is a similar result to what the Central\n",
      "Limit Theorem in classical statistics claims regarding the sampling distribu-\n",
      "tion for x̄.\n",
      "\n",
      "What about the posterior distribution for σ2? There are at least two ways\n",
      "to approach this derivation. First, we could consider the conditional distribu-\n",
      "tion for σ2|µ,X. If we take this approach, then we again begin with the full\n",
      "posterior density, but we now must consider all terms that involve σ2. If we\n",
      "carry out the multiplication in the posterior density and combine like terms,\n",
      "we obtain:\n",
      "\n",
      "f(µ, σ2) ∝\n",
      "1\n",
      "\n",
      "(σ2)n/2+1\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "∑\n",
      "\n",
      "(xi − µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Referring back to the above description of the inverse gamma distribution, it\n",
      "is clear that, if µ is considered fixed, the conditional posterior density for σ2\n",
      "\n",
      "is inverse gamma with parameters a = n/2 and b =\n",
      "∑\n",
      "\n",
      "(xi − µ)2/2.\n",
      "A second way to approach this problem is to consider that the joint pos-\n",
      "\n",
      "terior density for µ and σ2 can be factored using the conditional probability\n",
      "rule as:\n",
      "\n",
      "f(µ, σ2|X) = f(µ|σ2, X)f(σ2|X).\n",
      "\n",
      "The first term on the right-hand side we have already considered in the pre-\n",
      "vious example with σ2 considered to be a known, fixed quantity. The latter\n",
      "term, however, is the marginal posterior density for σ2. Technically, an exact\n",
      "expression for it can be found by integrating the joint posterior density over\n",
      "µ (i.e.,\n",
      "\n",
      "∫\n",
      "f(µ, σ2)dµ.) (see Gelman et al. 1995). Alternatively, we can find an\n",
      "\n",
      "expression proportional to it by factoring Equation 3.4. We know that the\n",
      "distribution for µ|σ2, X is proportional to a normal density with mean x̄ and\n",
      "variance σ2/n. Thus, if we factor this term out of the posterior, what is left\n",
      "is proportional to the marginal density for σ2.\n",
      "\n",
      "In order to factor the posterior, first, expand the quadratic again to obtain:\n",
      "\n",
      "1\n",
      "(σ2)n/2+1\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "∑\n",
      "\n",
      "x2i − 2nx̄µ + nµ\n",
      "2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Next, rearrange terms to put µ2 first, and divide the numerator and denomi-\n",
      "nator by n. Once again, complete the square to obtain:\n",
      "\n",
      "\n",
      "\n",
      "68 3 Basics of Bayesian Statistics\n",
      "\n",
      "1\n",
      "(σ2)n/2+1\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "(µ− x̄)2 +\n",
      "∑\n",
      "\n",
      "x2i /n− x̄\n",
      "2\n",
      "\n",
      "2σ2/n\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "We can now separate the two parts of the exponential to obtain:\n",
      "\n",
      "1\n",
      "σ\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "(µ− x̄)2\n",
      "\n",
      "2σ2/n\n",
      "\n",
      "}\n",
      "×\n",
      "\n",
      "1\n",
      "(σ2)n/2\n",
      "\n",
      "exp\n",
      "{∑\n",
      "\n",
      "x2i − nx̄\n",
      "2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "The first term is the conditional posterior for µ. The latter term is proportional\n",
      "to the marginal posterior density for σ2. The numerator in the exponential is\n",
      "the numerator for the computational version of the sample variance,\n",
      "\n",
      "∑\n",
      "(xi −\n",
      "\n",
      "x̄)2, and so, the result is recognizable as an inverse gamma distribution with\n",
      "parameters a = (n− 1)/2 and b = (n− 1)var(x)/2.\n",
      "\n",
      "3.5 Some useful prior distributions\n",
      "\n",
      "Thus far, we have discussed the use of a beta prior for proportion parameter\n",
      "p combined with a binomial likelihood function, a gamma prior for a Poisson\n",
      "rate parameter λ, a normal prior for a mean parameter combined with a\n",
      "normal likelihood function for the case in which the variance parameter σ2\n",
      "\n",
      "was assumed to be known, and a reference prior of 1/σ2—a special case of an\n",
      "inverse gamma distribution—for a normal likelihood function for the case in\n",
      "which neither µ nor σ2 were assumed to be known. In this section, I discuss a\n",
      "few additional distributions that are commonly used as priors for parameters\n",
      "in social science models. These distributions are commonly used as priors,\n",
      "because they are conjugate for certain sampling densities/likelihood functions.\n",
      "Specifically, I discuss the Dirichlet, the inverse gamma (in some more depth),\n",
      "and the Wishart and inverse Wishart distributions.\n",
      "\n",
      "One thing that must be kept in mind when considering distributions as\n",
      "priors and/or sampling densities is what symbols in the density are parameters\n",
      "versus what symbols are the random variables. For example, take the binomial\n",
      "distribution discussed in Chapter 2. In the binomial mass function, the ran-\n",
      "dom variable is represented by x, whereas the parameter is represented by\n",
      "p. However, in the beta distribution, the random variable is represented by\n",
      "p and the parameters are α and β. From a Bayesian perspective, parameters\n",
      "are random variables or at least can be treated as such. Thus, what is im-\n",
      "portant to realize is that we may need to change notation in the pdf so that\n",
      "we maintain the appropriate notation for representing the prior distribution\n",
      "for the parameter(s). For example, if we used θ to represent the parameter p\n",
      "in the binomial likelihood function, while p is used as the random variable in\n",
      "the beta distribution, the two distributions, when multiplied together, would\n",
      "contain p, θ, and x, and it would be unclear how θ and p were related. In fact,\n",
      "in the beta-binomial setup, θ = p, but we need to make sure our notation is\n",
      "clear so that that can be immediately seen.\n",
      "\n",
      "\n",
      "\n",
      "3.5 Some useful prior distributions 69\n",
      "\n",
      "3.5.1 The Dirichlet distribution\n",
      "\n",
      "Just as the multinomial distribution is a multivariate extension of the bi-\n",
      "nomial distribution, the Dirichlet distribution is a multivariate generaliza-\n",
      "tion of the beta distribution. If X is a k-dimensional vector and X ∼\n",
      "Dirichlet(α1, α2, . . . , αk), then:\n",
      "\n",
      "f(X) =\n",
      "Γ (α1 + . . . + αk)\n",
      "Γ (α1) . . . Γ (αk)\n",
      "\n",
      "xα1−11 . . . x\n",
      "αk−1\n",
      "k .\n",
      "\n",
      "Just as the beta distribution is a conjugate prior for the binomial distribution,\n",
      "the Dirichlet is a conjugate prior for the multinomial distribution. We can see\n",
      "this result clearly, if we combine a Dirichlet distribution as a prior with a\n",
      "multinomial distribution likelihood:\n",
      "\n",
      "f(p1 . . . pk|X) ∝ f(X|p1 . . . pk)f(p1 . . . pk)\n",
      "∝ Multinomial(X|p1 . . . pk)Dirichlet(p1 . . . pk|α1 . . . αk)\n",
      "∝ Dirichlet(p1 . . . pk|α1 + x1, α2 + x2, . . . , αk + xk)\n",
      "∝ pα1+x1−11 p\n",
      "\n",
      "α2+x2−1\n",
      "2 . . . p\n",
      "\n",
      "αk+xk−1\n",
      "k .\n",
      "\n",
      "Notice how here, as we discussed at the beginning of the section, the vector X\n",
      "in the original specification of the Dirichlet pdf has been changed to a vector\n",
      "p. In this specification, p is the random variable in the Dirichlet distribution,\n",
      "whereas α1 . . . αk are the parameters representing prior counts of outcomes in\n",
      "each of the k possible outcome categories.\n",
      "\n",
      "Also observe how the resulting Dirichlet posterior distribution looks just\n",
      "like the resulting beta posterior distribution, only with more possible out-\n",
      "comes.\n",
      "\n",
      "3.5.2 The inverse gamma distribution\n",
      "\n",
      "We have already discussed the gamma distribution in the Poisson/gamma\n",
      "example, and we have briefly discussed the inverse gamma distribution. If\n",
      "1/x ∼ gamma(α, β), then x ∼ IG(α, β). The density function for the inverse\n",
      "gamma distribution is:\n",
      "\n",
      "f(x) =\n",
      "βα\n",
      "\n",
      "Γ (α)\n",
      "x−(α+1)e−β/x,\n",
      "\n",
      "with x > 0. Just as in the gamma distribution, the parameters α and β affect\n",
      "the shape and scale of the curve (respectively), and both must be greater than\n",
      "0 to make the density proper.\n",
      "\n",
      "As discussed earlier, the inverse gamma distribution is used as a conju-\n",
      "gate prior for the variance in a normal model. If the normal distribution is\n",
      "parameterized with a precision parameter rather than with a variance param-\n",
      "eter, where the precision parameter is simply the inverse of the variance, the\n",
      "\n",
      "\n",
      "\n",
      "70 3 Basics of Bayesian Statistics\n",
      "\n",
      "gamma distribution is appropriate as a conjugate prior distribution for the\n",
      "precision parameter. In a normal model, if an inverse gamma distribution is\n",
      "used as the prior for the variance, the marginal distribution for the mean is a\n",
      "t distribution.\n",
      "\n",
      "The gamma and inverse gamma distributions are general distributions;\n",
      "other distributions arise by fixing the parameters to specific values. For ex-\n",
      "ample, if α is set to 1, the exponential distribution results:\n",
      "\n",
      "f(x) = (1/β)e−x/β ,\n",
      "\n",
      "or, more commonly f(x) = βe−βx, where β is an inverse scale parameter.\n",
      "Under this parameterization, βinverse scale = 1/βscale.\n",
      "\n",
      "If α is set to v/2, where v is the degrees of freedom, and β is set to 1/2, the\n",
      "chi-square distribution results. Setting the parameters equal to the same value\n",
      "in the inverse-gamma distribution yields an inverse-chi-square distribution.\n",
      "\n",
      "3.5.3 Wishart and inverse Wishart distributions\n",
      "\n",
      "The Wishart and inverse Wishart distributions are complex in appearance;\n",
      "they are multivariate generalizations of the gamma and inverse gamma dis-\n",
      "tributions, respectively. Thus, just as the inverse gamma is a conjugate prior\n",
      "density for the variance in a univariate normal model, the inverse Wishart\n",
      "is a conjugate prior density for the variance-covariance matrix in a multi-\n",
      "variate normal model. With an inverse Wishart distribution for the variance-\n",
      "covariance matrix in a multivariate normal model, the marginal distribution\n",
      "for the mean vector is multivariate t.\n",
      "\n",
      "If X ∼ Wishart(S), where S is a scale matrix of dimension d, then\n",
      "\n",
      "f(X) ∝| X |(v−d−1)/2 exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2\n",
      "tr(S−1X)\n",
      "\n",
      "}\n",
      ",\n",
      "\n",
      "where v is the degrees of freedom.\n",
      "If X ∼ inverse Wishart(S−1), then:\n",
      "\n",
      "f(X) ∝| X |−(v+d+1)/2 exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2\n",
      "tr(SX−1)\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "The assumption for both the Wishart and inverse Wishart distributions is\n",
      "that X and S are both positive definite; that is, zT Xz > 0 and zT Sz > 0 for\n",
      "any non-zero vector z of length d.\n",
      "\n",
      "3.6 Criticism against Bayesian statistics\n",
      "\n",
      "As we have seen in the examples, the development of a Bayesian model re-\n",
      "quires the inclusion of a prior distribution for the parameters in the model.\n",
      "The notion of using prior research or other information to inform a current\n",
      "\n",
      "\n",
      "\n",
      "3.6 Criticism against Bayesian statistics 71\n",
      "\n",
      "analysis and to produce an updated prior for subsequent use seems quite rea-\n",
      "sonable, if not very appropriate, for the advancement of research toward a\n",
      "more refined knowledge of the parameters that govern social processes. How-\n",
      "ever, the Bayesian approach to updating knowledge of parameters has been\n",
      "criticized on philosophical grounds for more than a century, providing one\n",
      "reason its adoption has been relatively limited in mainstream social science\n",
      "research.\n",
      "\n",
      "What is in philosophical dispute between Bayesians and classical statisti-\n",
      "cians includes: (1) whether data and hypotheses (which are simply statements\n",
      "about parameters of distributions9) can hold the same status as random vari-\n",
      "ables, and (2) whether the use of a prior probability injects too much subjec-\n",
      "tivity into the modeling process.\n",
      "\n",
      "The first standard argument presented against the Bayesian approach is\n",
      "that, because parameters are fixed, it is unreasonable to place a probability\n",
      "distribution on them (they simply are what they are). More formally, pa-\n",
      "rameters and data cannot share the same sample space. However, recall that\n",
      "the Bayesian perspective on probability is that probability is a subjective ap-\n",
      "proach to uncertainty. Whether a parameter is indeed fixed, to a Bayesian, is\n",
      "irrelevant, because we are still uncertain about its true value. Thus, impos-\n",
      "ing a probability distribution over a parameter space is reasonable, because\n",
      "it provides a method to reflect our uncertainty about the parameter’s true\n",
      "value.\n",
      "\n",
      "Bayesians argue that doing so has some significant advantages. First, as\n",
      "we have seen, Bayesian interval estimates have a clearer and more direct inter-\n",
      "pretation than classical confidence intervals. That is, we can directly conclude\n",
      "that a parameter falls in some interval with some probability. This is a com-\n",
      "mon but incorrect interpretation of classical confidence intervals, which simply\n",
      "reflect the probability of obtaining an interval estimate that contains the pa-\n",
      "rameter of interest under repeated sampling. Second, the Bayesian approach\n",
      "can naturally incorporate the findings of previous research with the prior,\n",
      "whereas the classical approach to statistics really has no coherent means of\n",
      "using previous results in current analyses beyond assisting with the specifica-\n",
      "tion of a hypothesis. That is, the Bayesian approach formalizes the process of\n",
      "hypothesis construction by incorporating it as part of the model. Third, the\n",
      "Bayesian approach more easily allows more detailed summaries concerning\n",
      "parameters. Instead of simply obtaining a maximum likelihood estimate and\n",
      "standard error, we have an entire distribution that can be summarized using\n",
      "various measures (e.g., mean, median, mode, and interquartile range).\n",
      "\n",
      "9 An alternative representation of Bayes’ Theorem is p(Hypothesis | data) ∝\n",
      "p(data | Hypothesis) × p(Hypothesis), which shows that, from a Bayesian per-\n",
      "spective, we can place a probability (distribution) on a scientific hypothesis. See\n",
      "Jeffreys 1961 for a detailed discussion of the theory of “inverse probability,” which\n",
      "describes the Bayesian approach in these terms.\n",
      "\n",
      "\n",
      "\n",
      "72 3 Basics of Bayesian Statistics\n",
      "\n",
      "The second general argument that has been advanced against Bayesian\n",
      "analysis is that incorporating a prior injects too much subjectivity into statis-\n",
      "tical modeling. The Bayesian response to this argument is multifaceted. First,\n",
      "all statistics is subjective. The choice of sampling density (likelihood) to use\n",
      "in a specific project is a subjective determination. For example, when faced\n",
      "with an ordinal outcome, some choose to use a normal likelihood function,\n",
      "leading to the ordinary least squares (OLS) regression model. Others choose a\n",
      "binomial likelihood with a link function, leading to an ordinal logit or probit\n",
      "regression model. These are subjective choices.\n",
      "\n",
      "Second, the choice of cut-point (α) at which to declare a result “statisti-\n",
      "cally significant” in a classical sense is a purely subjective determination. Also,\n",
      "similarly, the decision to declare a statistically significant result substantively\n",
      "meaningful is a subjective decision.\n",
      "\n",
      "A third response to the subjectivity criticism is that priors tend to be\n",
      "overwhelmed by data, especially in social science research. The prior distribu-\n",
      "tion generally contributes to the posterior once, whereas data enter into the\n",
      "likelihood function multiple times. As n → ∞, the prior’s influence on the\n",
      "posterior often becomes negligible.\n",
      "\n",
      "Fourth, priors can be quite noninformative, obviating the need for large\n",
      "quantities of data to “outweigh” them. In other words, a prior can be made\n",
      "to contribute little information to the posterior. That is, given that the pos-\n",
      "terior density is simply a weighted likelihood function, where the weighting\n",
      "is imposed by the prior, we can simply choose a prior distribution for the\n",
      "parameters that assigns approximately equal weight to all possible values of\n",
      "the parameters. The simplest noninformative prior that is often used is thus a\n",
      "uniform prior. Use of this prior yields a posterior density that is proportional\n",
      "to the likelihood function. In that case, the mode of the likelihood function\n",
      "(the maximum likelihood estimate) is the same as the Bayesian maximum a\n",
      "posteriori (MAP) estimate, and the substantive conclusions reached by both\n",
      "approaches may be similar, only differing in interpretation.\n",
      "\n",
      "In defense of the classical criticism, although uniform densities for param-\n",
      "eters are often used as priors, transformation from one parameterization of\n",
      "a parameter to another may yield an informative prior. However, alternative\n",
      "approaches have been developed for generating noninformative priors, includ-\n",
      "ing the development of Jeffreys priors and other priors. These noninformative\n",
      "priors tend to be based on the information matrix and are invariant under pa-\n",
      "rameter transformation. An in-depth discussion of such priors is beyond the\n",
      "scope of this book, given the goal of a general introduction to estimation. For\n",
      "more details, see Gelman et al. (1995) or see Gill (2002) for a more in-depth\n",
      "discussion of the history of the use and construction of noninformative priors.\n",
      "\n",
      "A fourth response is that the influence of priors can be evaluated after\n",
      "modeling the data to determine whether posterior inference is reasonable. Ul-\n",
      "timately, the results of any statistical analysis, whether Bayesian or classical,\n",
      "must be subjectively evaluated to determine whether they are reasonable, and\n",
      "so, the use of informative priors cannot introduce any more subjectivity than\n",
      "\n",
      "\n",
      "\n",
      "3.7 Conclusions 73\n",
      "\n",
      "could be included via other means in any analysis. Another response along\n",
      "these lines is that we can use priors to our advantage to examine how pow-\n",
      "erful the data are at invalidating the prior. For example, we may establish\n",
      "a conservative prior for a regression coefficient that claims that the a priori\n",
      "probability for a regression coefficient is heavily concentrated around 0 (i.e.,\n",
      "the covariate has no effect on the outcome). We can then examine the strength\n",
      "of the data in rejecting this prior, providing a conservative test of a covariate’s\n",
      "effect.\n",
      "\n",
      "In general, the historical criticisms of Bayesian statistics are philosophical\n",
      "in nature and cannot be conclusively adjudicated. Instead, the rise in the\n",
      "use of Bayesian statistics over the last few decades has largely occurred for\n",
      "pragmatic reasons, including (1) that many contemporary research questions\n",
      "readily lend themselves to a Bayesian approach, and (2) that the development\n",
      "of sampling methods used to estimate model parameters has increased their\n",
      "ease of use. The remaining chapters attempt to demonstrate these points.\n",
      "\n",
      "3.7 Conclusions\n",
      "\n",
      "In this chapter, we have developed the basics of the Bayesian approach to\n",
      "statistical inference. First, we derived Bayes’ Theorem from the probability\n",
      "rules developed in the previous chapter, and we applied Bayes’ Theorem to\n",
      "problems requiring point estimates for probabilities. We then extended the\n",
      "Bayesian approach to handle prior distributions for parameters rather than\n",
      "simply point estimates for prior probabilties. The result was that our posterior\n",
      "probability became a distribution, rather than a point estimate. Next, we dis-\n",
      "cussed how to summarize posterior probability distributions, and we demon-\n",
      "strated how to do so using several common examples. Finally, we discussed\n",
      "some common criticisms of the Bayesian approach that have been advanced\n",
      "over the last century, and we reviewed some common Bayesian responses to\n",
      "them. Although the material presented in this chapter is sufficient for gaining\n",
      "a basic understanding of the Bayesian approach to statistics, I recommend\n",
      "several additional sources for more in-depth coverage. I recommend Lee 1989\n",
      "for an extremely thorough but accessible exposition of the Bayesian paradigm,\n",
      "and I recommend Box and Tiao (1973) for a more advanced exposition.\n",
      "\n",
      "In the next chapter, we will continue exploring the Bayesian approach\n",
      "to posterior summarization and inference, but we will ultimately focus on\n",
      "multivariate posterior distributions—the most common type of posterior dis-\n",
      "tribution found in social science research—where the multivariate posterior\n",
      "distribution may not be as easy to summarize directly as the univariate pos-\n",
      "terior densities shown in this chapter.\n",
      "\n",
      "\n",
      "\n",
      "74 3 Basics of Bayesian Statistics\n",
      "\n",
      "3.8 Exercises\n",
      "\n",
      "1. In your own words, state what Bayes’ Theorem for point probabilities ac-\n",
      "tually does. For example, refer to Chapter 2 where I defined conditional\n",
      "probability, and use the same sort of discussion to describe how the the-\n",
      "orem works.\n",
      "\n",
      "2. The pregnancy example was completely contrived. In fact, most pregnancy\n",
      "tests today do not have such high rates of false positives. The “accuracy\n",
      "rate” is usually determined by computing the percent of correct answers\n",
      "the test gives; that is, the combined percent of positive results for positive\n",
      "cases and negative results for negative cases (versus false positives and\n",
      "false negatives). Recompute the posterior probability for being pregnant\n",
      "based on an accuracy rate of 90% defined in this manner. Assume that\n",
      "false positives and false negatives occur equally frequently under this 90%\n",
      "rate. What changes in the calculation?\n",
      "\n",
      "3. Determine the posterior probability that a 30-year-old male has prostate\n",
      "cancer, given (1) a positive PSA test result; (2) a 90% accuracy rate (as\n",
      "defined in the pregnancy example), coupled with a 90% false positive rate;\n",
      "and (3) a prior probability of .00001 for a 30-year-old male having prostate\n",
      "cancer. Based on the result, why might a physician consider not testing a\n",
      "30-year-old male using the PSA test?\n",
      "\n",
      "4. Find and plot the posterior distribution for a binomial likelihood with\n",
      "x = 5 successes out of n = 10 trials using at least three different beta prior\n",
      "distributions. Does the prior make a large difference in the outcome—\n",
      "when?\n",
      "\n",
      "5. Find and plot the posterior distribution for a normal distribution likeli-\n",
      "hood with a sample mean x̄ = 100 and variance var(x) = 144 (assume\n",
      "n = 169) using at least three different normal priors for the mean. When\n",
      "does the prior make the largest difference in the outcome—when the prior\n",
      "mean varies substantially from the sample mean, or when the prior vari-\n",
      "ance is small or large?\n",
      "\n",
      "6. Reconsider the pregnancy example from the beginning of the chapter. I\n",
      "showed the posterior probabilities for the second through the tenth sub-\n",
      "sequent tests. Reproduce these results, using the posterior obtained from\n",
      "the kth test as the prior for the (k + 1)st test. Next, assume the original\n",
      "prior (p = .15) and assume the 10 tests were taken simultaneously and\n",
      "all yielded a positive result. What is the posterior probability for preg-\n",
      "nancy? Finally, reconduct the pregnancy example with the 10 positive\n",
      "tests treated simultaneously as the current data, and use a beta prior\n",
      "distribution. Interpret the results.\n",
      "\n",
      "7. In the 2004 U.S. presidential election, surveys throughout the fall con-\n",
      "stantly reversed the projected victor. As each survey was conducted, would\n",
      "it have been appropriate to incorporate the results of previous surveys as\n",
      "priors and treat the current survey as new data to update the prior in\n",
      "a Bayesian fashion? If so, do you think a more consistent picture of the\n",
      "\n",
      "\n",
      "\n",
      "3.8 Exercises 75\n",
      "\n",
      "winner would have emerged before the election? If a Bayesian approach\n",
      "would not have been appropriate, why not?\n",
      "\n",
      "8. Give two simple examples showing a case in which a prior distribution\n",
      "would not be overwhelmed by data, regardless of the sample size.\n",
      "\n",
      "9. Show how the multinomial likelihood and Dirichlet prior are simply a\n",
      "multivariate generalization of the binomial likelihood and beta prior.\n",
      "\n",
      "10. Show how the Wishart distribution reduces to the gamma distribution\n",
      "when the number of dimensions of the random variable is 1.\n",
      "\n",
      "11. I said throughout the chapter that the inverse gamma distribution was the\n",
      "appropriate distribution for a variance parameter. It could be said that\n",
      "variance parameter could be considered to be distributed as an inverse\n",
      "chi-square random variable. Both of these statements are true. How?\n",
      "\n",
      "12. Why can a prior distribution that equals a constant be considered pro-\n",
      "portional to a uniform distribution?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "\n",
      "Modern Model Estimation Part 1: Gibbs\n",
      "Sampling\n",
      "\n",
      "The estimation of a Bayesian model is the most difficult part of undertaking\n",
      "a Bayesian analysis. Given that researchers may use different priors for any\n",
      "particular model, estimation must be tailored to the specific model under\n",
      "consideration. Classical analyses, on the other hand, often involve the use\n",
      "of standard likelihood functions, and hence, once an estimation routine is\n",
      "developed, it can be used again and again.\n",
      "\n",
      "The trade-off for the additional work required for a Bayesian analysis is\n",
      "that (1) a more appropriate model for the data can be constructed than extant\n",
      "software may allow, (2) more measures of model fit and outlier/influential case\n",
      "diagnostics can be produced, and (3) more information is generally available\n",
      "to summarize knowledge about model parameters than a classical analysis\n",
      "based on maximum likelihood (ML) estimation provides. Along these same\n",
      "lines, additional measures may be constructed to test hypotheses concerning\n",
      "parameters not directly estimated in the model.\n",
      "\n",
      "In this chapter, I first discuss the goal of model estimation in the Bayesian\n",
      "paradigm and contrast it with that of maximum likelihood estimation. Then,\n",
      "I discuss modern simulation/sampling methods used by Bayesian statisticians\n",
      "to perform analyses, including Gibbs sampling. In the next chapter, I discuss\n",
      "the Metropolis-Hastings algorithm as an alternative to Gibbs sampling.\n",
      "\n",
      "4.1 What Bayesians want and why\n",
      "\n",
      "As the discussion of ML estimation in Chapter 2 showed, the ML approach\n",
      "finds the parameter values that maximize the likelihood function for the ob-\n",
      "served data and then produces point estimates of the standard errors of these\n",
      "estimates. A typical classical statistical test is then conducted by subtracting\n",
      "a hypothesized value for the parameter from the ML estimate and dividing\n",
      "the result by the estimated standard error. This process yields a standardized\n",
      "estimate (under the hypothesized value). The Central Limit Theorem states\n",
      "that the sampling distribution for a sample statistic/parameter estimate is\n",
      "\n",
      "\n",
      "\n",
      "78 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "asymptotically normal, and so we can use the z (or t) distribution to evalu-\n",
      "ate the probability of observing the sample statistic we observed under the\n",
      "assumption that the hypothesized value for it were true. If observing the sam-\n",
      "ple statistic we did would be an extremely rare event under the hypothesized\n",
      "value, we reject the hypothesized value.\n",
      "\n",
      "In contrast to the use of a single point estimate for a parameter and its\n",
      "standard error and reliance on the Central Limit Theorem, a Bayesian analysis\n",
      "derives the posterior distribution for a parameter and then seeks to summarize\n",
      "the entire distribution. As we discussed in Chapter 2, many of the quantities\n",
      "that may be of interest in summarizing knowledge about a distribution are\n",
      "integrals of it, like the mean, median, variance, and various quantiles. Obtain-\n",
      "ing such integrals, therefore, is a key focus of Bayesian summarization and\n",
      "inference.\n",
      "\n",
      "The benefits of using the entire posterior distribution, rather than point es-\n",
      "timates of the mode of the likelihood function and standard errors, are several.\n",
      "First, if we can summarize the entire posterior distribution for a parameter,\n",
      "there is no need to rely on asymptotic arguments about the normality of the\n",
      "distribution: It can be directly assessed. Second, as stated above, having the\n",
      "entire posterior distribution for a parameter available allows for a considerable\n",
      "number of additional tests and summaries that cannot be performed under a\n",
      "classical likelihood-based approach. Third, as discussed in subsequent chap-\n",
      "ters, distributions for the parameters in the model can be easily transformed\n",
      "into distributions of quantities that may be of interest but may not be di-\n",
      "rectly estimated as part of the original model. For example, in Chapter 10,\n",
      "I show how distributions for hazard model parameters estimated via Markov\n",
      "chain Monte Carlo (MCMC) methods can be transformed into distributions\n",
      "of life table quantities like healthy life expectancy. Distributions of this quan-\n",
      "tity cannot be directly estimated from data but instead can be computed as a\n",
      "function of parameters from a hazard model. A likelihood approach that pro-\n",
      "duces only point estimates of the parameters and their associated standard\n",
      "errors cannot accomplish this.\n",
      "\n",
      "Given the benefits of a Bayesian approach to inference, the key question\n",
      "then is: How difficult is it to integrate a posterior distribution to produce\n",
      "summaries of parameters?\n",
      "\n",
      "4.2 The logic of sampling from posterior densities\n",
      "\n",
      "For some distributions, integrals for summarizing posterior distributions have\n",
      "closed-form solutions and are known, or they can be easily computed using\n",
      "numerical methods. For example, in the previous chapter, we determined the\n",
      "expected proportion of—and a plausible range for—votes for Kerry in the\n",
      "2004 presidential election in Ohio, as well as the probability that Kerry would\n",
      "win Ohio, using known information about integrals of the beta distribution.\n",
      "We also computed several summaries using a normal approximation to the\n",
      "\n",
      "\n",
      "\n",
      "4.2 The logic of sampling from posterior densities 79\n",
      "\n",
      "posterior density, and of course, integrals of the normal distribution are well-\n",
      "known.\n",
      "\n",
      "For many distributions, especially multivariate ones, however, integrals\n",
      "may not be easy to compute. For example, if we had a beta prior distribu-\n",
      "tion on the variance of a normal distribution, the posterior distribution for\n",
      "the variance would not have a known form. In order to remedy this problem,\n",
      "Bayesians often work with conjugate priors, as we discussed in the previous\n",
      "chapter. However, sometimes conjugate priors are unrealistic, or a model may\n",
      "involve distributions that simply are not amenable to simple computation of\n",
      "quantiles and other quantities. In those cases, there are essentially two ba-\n",
      "sic approaches to computing integrals: approximation methods and sampling\n",
      "methods.\n",
      "\n",
      "Before modern sampling methods (e.g., MCMC) were available or com-\n",
      "putationally feasible, Bayesians used a variety of approximation methods to\n",
      "perform integrations necessary to summarize posterior densities. Using these\n",
      "methods often required extensive knowledge of advanced numerical methods\n",
      "that social scientists generally do not possess, limiting the usefulness of a\n",
      "Bayesian approach. For example, quadrature methods—which involve eval-\n",
      "uating weighted points on a multidimensional grid—were often used. As an-\n",
      "other example, Bayesians often generated Taylor series expansions around the\n",
      "mode of the log-posterior distribution, and then used normal approximations\n",
      "to the posterior for which integrals are known. For multimodal distributions,\n",
      "Bayesians would often use approximations based on mixtures of normals. All\n",
      "of these approaches were methods of approximation and, hence, formed a foun-\n",
      "dation for criticizing Bayesian analysis. Of course, it is true that a Bayesian\n",
      "Central Limit Theorem shows that asymptotically most posterior distributions\n",
      "are normal (see Gelman et al. 1995 for an in-depth discussion of asymptotic\n",
      "normal theory in a Bayesian setting), but reliance on this theorem undermines\n",
      "a key benefit of having a complete posterior distribution: the lack of need to—\n",
      "and, in small samples, the inability to—rely on asymptotic arguments. I do\n",
      "not focus on these methods in this book.\n",
      "\n",
      "Sampling methods constitute an alternative to approximation methods.\n",
      "The logic of sampling is that we can generate (simulate) a sample of size\n",
      "n from the distribution of interest and then use discrete formulas applied\n",
      "to these samples to approximate the integrals of interest. Under a sampling\n",
      "approach, we can estimate a mean by:∫\n",
      "\n",
      "xf(x)dx ≈\n",
      "1\n",
      "n\n",
      "\n",
      "∑\n",
      "x\n",
      "\n",
      "and the variance by: ∫\n",
      "(x− µ)2f(x)dx ≈\n",
      "\n",
      "1\n",
      "n\n",
      "\n",
      "∑\n",
      "(x− µ)2.\n",
      "\n",
      "Various quantiles can be computed empirically by noting the value of x for\n",
      "which Q% of the sampled values fall below it.\n",
      "\n",
      "\n",
      "\n",
      "80 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "Thus, modern Bayesian inference typically involves (1) establishing a\n",
      "model and obtaining a posterior distribution for the parameter(s) of interest,\n",
      "(2) generating samples from the posterior distribution, and (3) using discrete\n",
      "formulas applied to the samples from the posterior distribution to summarize\n",
      "our knowledge of the parameters. These summaries are not limited to a single\n",
      "quantity but instead are virtually limitless. Any summary statistic that we\n",
      "commonly compute to describe a sample of data can also be computed for a\n",
      "sample from a posterior distribution and can then be used to describe it!\n",
      "\n",
      "Consider, for example, the voting example from the previous chapter in\n",
      "which we specified a beta prior distribution for K, coupled with a binomial\n",
      "likelihood for the most recent polling data. In that example, the posterior\n",
      "density for K was a beta density with parameters α = 1498 and β = 1519.\n",
      "Given that the beta density is a known density, we computed the posterior\n",
      "mean as 1498/(1498 + 1519) = .497, and the probability that K > .5 as\n",
      ".351. However, assume these integrals could not be computed analytically. In\n",
      "that case, we could simulate several thousand draws from this particular beta\n",
      "density (using x=rbeta(5000,1498,1519)in R, with the first argument being\n",
      "the desired number of samples), and we could then compute the mean, median,\n",
      "and other desired quantities from this sample. I performed this simulation and\n",
      "obtained a mean of .496 for the 5,000 samples (obtained by typing mean(x)\n",
      "in R) and a probability of .351 that Kerry would win (obtained by typing\n",
      "sum(x>.5)/5000).\n",
      "\n",
      "Notice that the mean obtained analytically (via integration of the posterior\n",
      "density) and the mean obtained via sampling are identical to almost three\n",
      "decimal places, as are the estimated probabilities that Kerry would win. The\n",
      "reason that these estimates are close is that sampling methods, in the limit,\n",
      "are not approximations; instead, they provide exact summaries equivalent\n",
      "to those obtained via integration. A sample of 5,000 draws from this beta\n",
      "distribution is more than sufficient to accurately summarize the density. As\n",
      "a demonstration, Figure 4.1 shows the convergence of the sample-estimated\n",
      "mean for this particular beta distribution as the sample size increases from\n",
      "1 to 100,000. At samples of size n = 5, 000, the confidence band around the\n",
      "mean is only approximately .0005 units wide. In other words, our error in\n",
      "using simulation rather than analytic integration is extremely small. As the\n",
      "sample size increases, we can see that the simulation error diminishes even\n",
      "further.\n",
      "\n",
      "4.3 Two basic sampling methods\n",
      "\n",
      "In the example shown above, it was easy to obtain samples from the desired\n",
      "beta density using a simple command in R. For many distributions, there\n",
      "are effective routines in existence for simulating from them (some of which\n",
      "ultimately rely on the inversion method discussed below). For other distribu-\n",
      "tions, there may not be an extant routine, and hence, a statistician may need\n",
      "\n",
      "\n",
      "\n",
      "4.3 Two basic sampling methods 81\n",
      "\n",
      "0   e+00 2   e+04 4   e+04 6   e+04 8   e+04 1   e+05\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "9\n",
      "5\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".4\n",
      "9\n",
      "\n",
      "6\n",
      "5\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "9\n",
      "7\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".4\n",
      "9\n",
      "\n",
      "8\n",
      "5\n",
      "\n",
      "Size of Sample from Beta(1498,1519) Density\n",
      "\n",
      "E\n",
      "st\n",
      "\n",
      "im\n",
      "a\n",
      "\n",
      "te\n",
      "d\n",
      "\n",
      " M\n",
      "e\n",
      "\n",
      "a\n",
      "n\n",
      "\n",
      " o\n",
      "f \n",
      "S\n",
      "\n",
      "a\n",
      "m\n",
      "\n",
      "p\n",
      "le\n",
      "\n",
      "n=5000\n",
      "\n",
      "Confidence Band for Mean at n=5,000\n",
      "\n",
      "Fig. 4.1. Convergence of sample means on the true beta distribution mean across\n",
      "samples sizes: Vertical line shows sample size of 5,000; dashed horizontal lines show\n",
      "approximate confidence band of sample estimates for samples of size n = 5, 000; and\n",
      "solid horizontal line shows the true mean.\n",
      "\n",
      "to create one. Indeed, this is the entire reason for MCMC methods, as we will\n",
      "discuss: Integration of posterior densities is often impossible, and there may\n",
      "not be extant routines for sampling from them either, especially when they\n",
      "are high-dimensional. I first discuss two sampling methods, each of which is\n",
      "important for a basic understanding of MCMC methods. These methods, as\n",
      "well as several others, are described in greater depth in Gilks (1996). For a\n",
      "more detailed exposition on simulation methods, see Ripley (1987).\n",
      "\n",
      "4.3.1 The inversion method of sampling\n",
      "\n",
      "For drawing a sample from a univariate distribution f(x), we can often use\n",
      "the inversion method. The inversion method is quite simple and follows two\n",
      "steps:\n",
      "\n",
      "1. Draw a uniform random number u between 0 and 1 (a U(0, 1) random\n",
      "variable).\n",
      "\n",
      "\n",
      "\n",
      "82 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "2. Then z = F−1(u) is a draw from f(x).\n",
      "\n",
      "In step 1, we draw a U(0, 1) random variable. This draw represents the\n",
      "area under the curve up to the value of our desired random draw from the\n",
      "distribution of interest. Thus, we simply need to find z such that:\n",
      "\n",
      "u =\n",
      "∫ z\n",
      "\n",
      "L\n",
      "\n",
      "f(x)dx,\n",
      "\n",
      "where L is the lower limit of the density f . Put another way, u = F (z). So,\n",
      "phrased in terms of z:\n",
      "\n",
      "z = F−1(u).\n",
      "\n",
      "To provide a concrete example, take the linear density function from Chap-\n",
      "ter 2: f(x) = (1/40)(2x+3) (with 0 < x < 5). As far as I know, no routines are\n",
      "readily available that allow sampling from this density, and so, if one needed\n",
      "draws from this density, one would need to develop one. In order to generate a\n",
      "draw from this distribution using the inversion method, we first need to draw\n",
      "u ∼ U(0, 1) and then compute z that satisfies\n",
      "\n",
      "u =\n",
      "∫ z\n",
      "\n",
      "0\n",
      "\n",
      "1\n",
      "40\n",
      "\n",
      "(2x + 3)dx.\n",
      "\n",
      "We can solve this equation for z as follows. First, evaluate the integral:\n",
      "\n",
      "40u = x2 + 3x\n",
      "∣∣z\n",
      "0\n",
      "\n",
      "= z2 + 3z.\n",
      "\n",
      "Second, complete the square in z:\n",
      "\n",
      "40u +\n",
      "9\n",
      "4\n",
      "\n",
      "= z2 + 3z +\n",
      "9\n",
      "4\n",
      "\n",
      "=\n",
      "(\n",
      "\n",
      "z +\n",
      "3\n",
      "2\n",
      "\n",
      ")2\n",
      ".\n",
      "\n",
      "Third, take the square root of both sides and rearrange to find z:\n",
      "\n",
      "z =\n",
      "−3±\n",
      "\n",
      "√\n",
      "160u + 9\n",
      "2\n",
      "\n",
      ".\n",
      "\n",
      "This result reveals two solutions for z; however, given that z must be between\n",
      "0 and 5, only the positive root is relevant. If we substitute 0 and 1—the\n",
      "minimum and maximum values for u—we find that the range of z is [0, 5] as\n",
      "it should be.\n",
      "\n",
      "Figure 4.2 displays the results of an algorithm simulating 1,000 random\n",
      "draws from this density using the inversion method. The figures on the left-\n",
      "hand side show the sequence of draws from the U(0, 1) density, which are\n",
      "then inverted to produce the sequence of draws from the density of interest.\n",
      "The right-hand side of the figure shows the simulated and theoretical density\n",
      "functions. Notice how the samples from both densities closely follow, but do\n",
      "not exactly match, the theoretical densities. This error is sampling error, which\n",
      "diminishes as the simulation sample size increases.\n",
      "\n",
      "\n",
      "\n",
      "4.3 Two basic sampling methods 83\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Random Sample Item\n",
      "\n",
      "U\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "U\n",
      "\n",
      "f(\n",
      "U\n",
      "\n",
      ")\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Random Sample Item\n",
      "\n",
      "Z\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "Z\n",
      "\n",
      "f(\n",
      "Z\n",
      "\n",
      ")\n",
      "\n",
      "Fig. 4.2. Example of the inversion method: Left-hand figures show the sequence\n",
      "of draws from the U(0, 1) density (upper left) and the sequence of draws from the\n",
      "density f(x) = (1/40)(2x + 3) density (lower left); and the right-hand figures show\n",
      "these draws in histogram format, with true density functions superimposed.\n",
      "\n",
      "The following R program was used to generate these draws. The first line\n",
      "simulates 1,000 random draws from the U(0, 1) distribution; the second line\n",
      "generates the vector z as the inverse of u :\n",
      "\n",
      "#R program for inversion method of sampling\n",
      "\n",
      "u=runif(1000,min=0,max=1)\n",
      "\n",
      "z=(1/2) * (-3 + sqrt(160*u +9))\n",
      "\n",
      "Although the inversion method is very efficient and easy to implement, two\n",
      "key limitations reduce its usability as a general method for drawing samples\n",
      "from posterior densities. First, if the inverse function is impossible to derive\n",
      "analytically, obviously the method cannot be used. For example, the normal\n",
      "integral cannot be directly solved, and hence, the inversion method cannot be\n",
      "used to simulate from the normal distribution.1 To some extent, this problem\n",
      "1 Of course, we do have efficient algorithms for computing this integral, but the\n",
      "\n",
      "integral cannot be solved analytically.\n",
      "\n",
      "\n",
      "\n",
      "84 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "begs the question: If we can integrate the density as required by the inversion\n",
      "method, then why bother with simulation? This question will be addressed\n",
      "shortly, but the short answer is that we may not be able to perform integration\n",
      "on a multivariate density, but we can often break a multivariate density into\n",
      "univariate ones for which inversion may work.\n",
      "\n",
      "The second problem with the inversion method is that the method will\n",
      "not work with multivariate distributions, because the inverse is generally not\n",
      "unique beyond one dimension. For example, consider the bivariate planar den-\n",
      "sity function discussed in Chapter 2:\n",
      "\n",
      "f(x, y) =\n",
      "1\n",
      "28\n",
      "\n",
      "(2x + 3y + 2),\n",
      "\n",
      "with 0 < x, y < 2. If we draw u ∼ U(0, 1) and attempt to solve the double\n",
      "integral for x and y, we get:\n",
      "\n",
      "28u = yx2 +\n",
      "3xy2\n",
      "\n",
      "2\n",
      "+ 2xy,\n",
      "\n",
      "which, of course, has infinitely many solutions (one equation with two un-\n",
      "knowns). Thinking ahead, we could select a value for one variable and then\n",
      "use the inversion method to draw from the conditional distribution of the\n",
      "other variable. This process would reduce the problem to one of sampling\n",
      "from univariate conditional distributions, which is the basic idea of Gibbs\n",
      "sampling, as I discuss shortly.\n",
      "\n",
      "4.3.2 The rejection method of sampling\n",
      "\n",
      "When F−1(u) cannot be computed, other methods of sampling exist. A very\n",
      "important one is rejection sampling. In rejection sampling, sampling from a\n",
      "distribution f(x) for x involves three basic steps:\n",
      "\n",
      "1. Sample a value z from a distribution g(x) from which sampling is easy\n",
      "and for which values of m× g(x) are greater than f(x) at all points (m is\n",
      "a constant).\n",
      "\n",
      "2. Compute the ratio R = f(z)\n",
      "m×g(z) .\n",
      "\n",
      "3. Sample u ∼ U(0, 1). If R > u, then accept z as a draw from f(x). Other-\n",
      "wise, return to step 1.\n",
      "\n",
      "In this algorithm, m×g(x) is called an “envelope function,” because of the\n",
      "requirement that the density function g(x) multiplied by some constant m be\n",
      "greater than the density function value for the distribution of interest [f(x)]\n",
      "at the same point for all points. In other words, m × g(x) envelops f(x). In\n",
      "step 1, we sample a point z from the pdf g(x).\n",
      "\n",
      "In step 2, we compute the ratio of the envelope function [m × g(x)] eval-\n",
      "uated at z to the density function of interest [f(x)] evaluated at the same\n",
      "point.\n",
      "\n",
      "\n",
      "\n",
      "4.3 Two basic sampling methods 85\n",
      "\n",
      "Finally, in step 3, we draw a U(0, 1) random variable u and compare it\n",
      "with R. If R > u, then we treat the draw as a draw from f(x). If not, we\n",
      "reject z as coming from f(x), and we repeat the process until we obtain a\n",
      "satisfactory draw.\n",
      "\n",
      "This routine is easy to implement, but it is not immediately apparent why\n",
      "it works. Let’s again examine the density discussed in the previous section and\n",
      "consider an envelope function that is a uniform density on the [0, 5] interval\n",
      "multiplied by a constant of 2. I choose this constant because the height of\n",
      "the U(0, 5) density is .2, whereas the maximum height of the density f(x) =\n",
      "(1/40)(2x + 3) is .325. Multiplying the U(0, 5) density by two increases the\n",
      "height of this density to .4, which is well above the maximum for f(x) and\n",
      "therefore makes m × g(x) a true envelope function. Figure 4.3 shows the\n",
      "density and envelope functions and graphically depicts the process of rejection\n",
      "sampling.\n",
      "\n",
      "In the first step, when we are sampling from the envelope function, we are\n",
      "choosing a location on the x axis in the graph (see top graph in Figure 4.3).\n",
      "The process of constructing the ratio R and comparing it with a uniform\n",
      "deviate is essentially a process of locating a point in the y direction once\n",
      "the x coordinate is chosen and then deciding whether it is under the density\n",
      "of interest. This becomes more apparent if we rearrange the ratio and the\n",
      "inequality with u:\n",
      "\n",
      "f(z) <=>︸ ︷︷ ︸ m× g(z)× u.\n",
      "?\n",
      "\n",
      "m × g(z) × u provides us a point in the y dimension that falls somewhere\n",
      "between 0 and m× g(z). This can be easily seen by noting that m× g(z)× u\n",
      "is really simply providing a random draw from the U(0, g(z)) distribution:\n",
      "The value of this computation when u = 0 is 0; its value when u = 1 is\n",
      "m × g(z) (see middle graph in Figure 4.3). In the last step, in which we\n",
      "decide whether to accept z as a draw from f(x), we are simply determining\n",
      "whether the y coordinate falls below the f(x) curve (see bottom graph in\n",
      "Figure 4.3). Another way to think about this process is that the ratio tells us\n",
      "the proportion of times we will accept a draw at a given value of x as coming\n",
      "from the density of interest.\n",
      "\n",
      "The following R program simulates 1,000 draws from the density f(x) =\n",
      "(1/40)(2x+3) using rejection sampling. The routine also keeps a count of how\n",
      "many total draws from g(x) must be made in order to obtain 1,000 draws from\n",
      "f(x).\n",
      "\n",
      "#R program for rejection method of sampling\n",
      "\n",
      "count=0; k=1; f=matrix(NA,1000)\n",
      "\n",
      "while(k<1001)\n",
      "\n",
      "{\n",
      "\n",
      "z=runif(1,min=0,max=5)\n",
      "\n",
      "r=((1/40)*(2*z+3))/(2*.2)\n",
      "\n",
      "\n",
      "\n",
      "86 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "−1 0 1 2 3 4 5 6\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "Step 1\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      " a\n",
      "n\n",
      "\n",
      "d\n",
      " g\n",
      "\n",
      "(x\n",
      ") Choose z from g(x) [say this point is '*']\n",
      "\n",
      "f(x)\n",
      "g(x)\n",
      "\n",
      "−1 0 1 2 3 4 5 6\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "Step 2\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      " a\n",
      "n\n",
      "\n",
      "d\n",
      " g\n",
      "\n",
      "(x\n",
      ") Draw u~U(0,1) and multiply by  m*g(z) to locate vertical point [say '*']\n",
      "\n",
      "−1 0 1 2 3 4 5 6\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "Step 3\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      " a\n",
      "n\n",
      "\n",
      "d\n",
      " g\n",
      "\n",
      "(x\n",
      ") Is u*(m*g(z)) above (reject) or below (accept) f(z)? [here, it is above, so we reject]\n",
      "\n",
      "Fig. 4.3. The three-step process of rejection sampling.\n",
      "\n",
      "if(r>runif(1,min=0,max=1))\n",
      "\n",
      "{f[k]=z; k=k+1}\n",
      "\n",
      "count=count+1\n",
      "\n",
      "}\n",
      "\n",
      "Figure 4.4 shows the results of a run of this algorithm. The histogram of\n",
      "the sample of 1,000 draws very closely matches the density of interest.\n",
      "\n",
      "Rejection sampling is a powerful method of sampling from densities for\n",
      "which inversion sampling does not work. It can be used to sample from any\n",
      "density, and it can be used to sample from multivariate densities. In the multi-\n",
      "variate case, we first choose an X—now a random vector, rather than a single\n",
      "point—from a multivariate enveloping function, and then we proceed just as\n",
      "before.\n",
      "\n",
      "\n",
      "\n",
      "4.3 Two basic sampling methods 87\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "Z\n",
      "\n",
      "f(\n",
      "Z\n",
      "\n",
      ")\n",
      "\n",
      "Fig. 4.4. Sample of 1,000 draws from density using rejection sampling with theo-\n",
      "retical density superimposed.\n",
      "\n",
      "Rejection sampling does have some limitations. First, finding an enveloping\n",
      "function m×g(x) may not be an easy task. For example, it may be difficult to\n",
      "find an envelope with values that are greater at all points of support for the\n",
      "density of interest. Consider trying to use a uniform density as an envelope\n",
      "for sampling from a normal density. The domain of x for the normal density\n",
      "runs from −∞ to +∞, but there is no corresponding uniform density. In the\n",
      "limit, a U(−∞,+∞) density would have an infinitely low height, which would\n",
      "make g(x) fall below f(x) in the center of the distribution, regardless of the\n",
      "constant multiple m chosen. Second, the algorithm may not be very efficient.\n",
      "If the enveloping function is considerably higher than f(x) at all points, the\n",
      "algorithm will reject most attempted draws, which implies that an incredible\n",
      "number of draws may need to be made before finding a single value from f(x).\n",
      "In theory, the efficiency of a rejection sampling routine is calculable before\n",
      "implementing it. In the case above, the total area under the enveloping curve\n",
      "is 2 (5×.4), but the total area under the density of interest is 1 (by definition of\n",
      "a density function). Thus, the algorithm used should accept about 50% of the\n",
      "draws from g(x). In fact, in the case shown and discussed above, it took 2,021\n",
      "\n",
      "\n",
      "\n",
      "88 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "attempts to obtain 1,000 draws from f(x), which is a rejection rate of 50.5%.\n",
      "These two limitations make rejection sampling, although possible, increasingly\n",
      "difficult as the dimensionality increases in multivariate distributions.\n",
      "\n",
      "4.4 Introduction to MCMC sampling\n",
      "\n",
      "The limitations of inversion and rejection sampling make the prospects of\n",
      "using these simple methods daunting in complex statistical analyses involv-\n",
      "ing high-dimensional distributions. Although rejection sampling approaches\n",
      "can be refined to be more efficient, they are still not very useful in and of\n",
      "themselves in real-world statistical modeling. Fortunately, over the last few\n",
      "decades, MCMC methods have been developed that facilitate sampling from\n",
      "complex distributions. Furthermore, aside from allowing sampling from com-\n",
      "plex distributions, these methods provide several additional benefits, as we\n",
      "will be discussing in the remaining chapters.\n",
      "\n",
      "MCMC sampling provides a method to sample from multivariate densities\n",
      "that are not easy to sample from, often by breaking these densities down\n",
      "into more manageable univariate or multivariate densities. The basic MCMC\n",
      "approach provides a prescription for (1) sampling from one or more dimensions\n",
      "of a posterior distribution and (2) moving throughout the entire support of a\n",
      "posterior distribution. In fact, the name “Markov chain Monte Carlo” implies\n",
      "this process. The “Monte Carlo” portion refers to the random simulation\n",
      "process. The “Markov chain” portion refers to the process of sampling a new\n",
      "value from the posterior distribution, given the previous value: This iterative\n",
      "process produces a Markov chain of values that constitute a sample of draws\n",
      "from the posterior.\n",
      "\n",
      "4.4.1 Generic Gibbs sampling\n",
      "\n",
      "The Gibbs sampler is the most basic MCMC method used in Bayesian statis-\n",
      "tics. Although Gibbs sampling was developed and used in physics prior to\n",
      "1990, its widespread use in Bayesian statistics originated in 1990 with its in-\n",
      "troduction by Gelfand and Smith (1990). As will be discussed more in the next\n",
      "chapter, the Gibbs sampler is a special case of the more general Metropolis-\n",
      "Hastings algorithm that is useful when (1) sampling from a multivariate pos-\n",
      "terior is not feasible, but (2) sampling from the conditional distributions for\n",
      "each parameter (or blocks of them) is feasible. A generic Gibbs sampler follows\n",
      "the following iterative process (j indexes the iteration count):\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 89\n",
      "\n",
      "0. Assign a vector of starting values, S, to the parameter vector:\n",
      "Θj=0 = S.\n",
      "\n",
      "1. Set j = j + 1.\n",
      "2. Sample (θj1 | θ\n",
      "\n",
      "j−1\n",
      "2 , θ\n",
      "\n",
      "j−1\n",
      "3 . . . θ\n",
      "\n",
      "j−1\n",
      "k ).\n",
      "\n",
      "3. Sample (θj2 | θ\n",
      "j\n",
      "1, θ\n",
      "\n",
      "j−1\n",
      "3 . . . θ\n",
      "\n",
      "j−1\n",
      "k ).\n",
      "\n",
      "...\n",
      "...\n",
      "\n",
      "k. Sample (θjk | θ\n",
      "j\n",
      "1, θ\n",
      "\n",
      "j\n",
      "2, . . . , θ\n",
      "\n",
      "j\n",
      "k−1).\n",
      "\n",
      "k+1. Return to step 1.\n",
      "\n",
      "In other words, Gibbs sampling involves ordering the parameters and sampling\n",
      "from the conditional distribution for each parameter given the current value of\n",
      "all the other parameters and repeatedly cycling through this updating process.\n",
      "Each “loop” through these steps is called an “iteration” of the Gibbs sampler,\n",
      "and when a new sampled value of a parameter is obtained, it is called an\n",
      "“updated” value.\n",
      "\n",
      "For Gibbs sampling, the full conditional density for a parameter needs only\n",
      "to be known up to a normalizing constant. As we discussed in Chapters 2 and\n",
      "3, this implies that we can use the joint density with the other parameters\n",
      "set at their current values. This fact makes Gibbs sampling relatively simple\n",
      "for most problems in which the joint density reduces to known forms for each\n",
      "parameter once all other parameters are treated as fixed.\n",
      "\n",
      "4.4.2 Gibbs sampling example using the inversion method\n",
      "\n",
      "Here, I provide a simple example of Gibbs sampling based on the bivariate\n",
      "plane distribution developed in Chapter 2 f(x, y) = (1/28)(2x + 3y + 2). The\n",
      "conditional distribution for x was:\n",
      "\n",
      "f(x | y) =\n",
      "f(x, y)\n",
      "f(y)\n",
      "\n",
      "=\n",
      "2x + 3y + 2\n",
      "\n",
      "6y + 8\n",
      ",\n",
      "\n",
      "and the conditional distribution for y was:\n",
      "\n",
      "f(y | x) =\n",
      "f(x, y)\n",
      "f(x)\n",
      "\n",
      "=\n",
      "2x + 3y + 2\n",
      "\n",
      "4x + 10\n",
      ".\n",
      "\n",
      "Thus, a Gibbs sampler for sampling x and y in this problem would follow\n",
      "these steps:\n",
      "\n",
      "1. Set j = 0 and establish starting values. Here, let’s set xj=0 = −5 and\n",
      "yj=0 = −5.\n",
      "\n",
      "2. Sample xj+1 from f(x | y = yj).\n",
      "3. Sample yj+1 from f(y | x = xj+1).\n",
      "4. Increment j = j + 1 and return to step 2 until j = 2000.\n",
      "\n",
      "\n",
      "\n",
      "90 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "How do we sample from these conditional distributions? We know what they\n",
      "are, but they certainly are not standard distributions. Since they are not\n",
      "standard distributions, but since these conditionals are univariate and F−1()\n",
      "can be calculated for each one, we can use an inversion subroutine to sample\n",
      "from each conditional density. How do we find the inverses in this bivariate\n",
      "density? Recall that inversion sampling requires first drawing a u ∼ U(0, 1)\n",
      "random variable and then inverting this draw using F−1. Thus, to find the\n",
      "inverse of the conditional density for y|x, we need to solve:\n",
      "\n",
      "u =\n",
      "∫ z\n",
      "\n",
      "0\n",
      "\n",
      "2x + 3y + 2\n",
      "4x + 10\n",
      "\n",
      "for z. Given that this is the conditional density for y, x is fixed and can be\n",
      "treated as a constant, and we obtain:\n",
      "\n",
      "u(4x + 10) = (2x + 2)y + (3/2)y2\n",
      "∣∣z\n",
      "0\n",
      ".\n",
      "\n",
      "Thus:\n",
      "\n",
      "u(4x + 10) = (2x + 2)z + (3/2)z2.\n",
      "\n",
      "After multiplying through by (2/3) and rearranging terms, we get:\n",
      "\n",
      "(2/3)u(4x + 10) = z2 + (2/3)(2x + 2)z.\n",
      "\n",
      "We can then complete the square in z and solve for z to obtain:\n",
      "\n",
      "z =\n",
      "√\n",
      "\n",
      "(2/3)u(4x + 10) + ((1/3)(2x + 2))2 − (1/3)(2x + 2).\n",
      "\n",
      "Given a current value for x and a random draw u, z is a random draw from\n",
      "the conditional density for y|x. A similar process can be undertaken to find\n",
      "the inverse for x|y (see Exercises).\n",
      "\n",
      "Below is an R program that implements the Gibbs sampling:\n",
      "\n",
      "#R program for Gibbs sampling using inversion method\n",
      "\n",
      "x=matrix(-5,2000); y=matrix(-5,2000)\n",
      "\n",
      "for(i in 2:2000)\n",
      "\n",
      "{\n",
      "\n",
      "#sample from x | y\n",
      "\n",
      "u=runif(1,min=0, max=1)\n",
      "\n",
      "x[i]=sqrt(u*(6*y[i-1]+8)+(1.5*y[i-1]+1)*(1.5*y[i-1]+1))\n",
      "\n",
      "-(1.5*y[i-1]+1)\n",
      "\n",
      "#sample from y | x\n",
      "\n",
      "u=runif(1,min=0,max=1)\n",
      "\n",
      "y[i]=sqrt((2*u*(4*x[i]+10))/3 +((2*x[i]+2)/3)*((2*x[i]+2)/3))\n",
      "\n",
      "- ((2*x[i]+2)/3)\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 91\n",
      "\n",
      "This program first sets the starting values for x and y equal to −5. Then,\n",
      "x is updated using the current value of y. Then, y is updated using the just-\n",
      "sampled value of x. (Notice how x[i] is computed using y[i-1], whereas\n",
      "y[i] is sampled using x[i].) Both are updated using the inversion method\n",
      "of sampling discussed above.\n",
      "\n",
      "This algorithm produces samples from the marginal distributions for both\n",
      "x and y, but we can also treat pairs of x and y as draws from the joint density.\n",
      "We will discuss the conditions in which we can do this in greater depth shortly.\n",
      "Generally, however, of particular interest are the marginal distributions for\n",
      "parameters, since we are often concerned with testing hypotheses concerning\n",
      "one parameter, net of the other parameters in a model. Figure 4.5 shows a\n",
      "“trace plot” of both x and y as well as the marginal densities for both variables.\n",
      "The trace plot is simply a two-dimensional plot in which the x axis represents\n",
      "the iteration of the algorithm, and the y axis represents the simulated value\n",
      "of the random variable at each particular iteration. Heuristically, we can then\n",
      "take the trace plot, turn it on its edge (a 90 degree clockwise turn), and allow\n",
      "the “ink” to fall down along the y-axis and “pile-up” to produce a histogram\n",
      "of the marginal density. Places in the trace plot that are particularly dark\n",
      "represent regions of the density in which the algorithm simulated frequently;\n",
      "lighter areas are regions of the density that were more rarely visited by the\n",
      "algorithm. Thus, the “ink” will pile-up higher in areas for which the variable\n",
      "of interest has greater probability. Histograms of these marginal densities are\n",
      "shown to the right of their respective trace plots, with the theoretical marginal\n",
      "densities derived in Chapter 2 superimposed. Realize that these marginals are\n",
      "unnormalized, because the leading 1/28 normalizing constant cancels in both\n",
      "the numerator and the denominator.\n",
      "\n",
      "Notice that, although the starting values were very poor (−5 is not a valid\n",
      "point in either dimension of the density), the algorithm converged very rapidly\n",
      "to the appropriate region—[0, 2]. It generally takes a number of iterations for\n",
      "an MCMC algorithm to find the appropriate region—and, more theoretically,\n",
      "for the Markov chain produced by the algorithm to sample from the appro-\n",
      "priate “target” distribution. Thus, we generally discard a number of early\n",
      "iterations before making calculations (called the “burn-in”). The marginal\n",
      "densities, therefore, are produced from only the last 1,500 iterations of the\n",
      "algorithm.\n",
      "\n",
      "The histograms for the marginal densities show that the algorithm samples\n",
      "appropriately from the densities of interest. Of course, there is certainly some\n",
      "error—observe how the histograms tend to be a little too low or high here\n",
      "and there. This reflects sampling error, and such error is reduced by sampling\n",
      "more values (e.g., using 5,000 draws, rather than 2,000); we will return to this\n",
      "issue in the next chapter.\n",
      "\n",
      "Aside from examining the marginal distributions for x and y, we can also\n",
      "examine the joint density. Figure 4.6 shows a two-dimensional trace plot, taken\n",
      "at several stages. The upper left figure shows the state of the algorithm after\n",
      "5 iterations; the upper right figure shows the state after 25 iterations; the\n",
      "\n",
      "\n",
      "\n",
      "92 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "0 500 1000 1500 2000\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "S\n",
      "a\n",
      "\n",
      "m\n",
      "p\n",
      "\n",
      "le\n",
      "d\n",
      "\n",
      " X\n",
      "\n",
      "−0.5 0.5 1.5 2.5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "0 500 1000 1500 2000\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "S\n",
      "a\n",
      "\n",
      "m\n",
      "p\n",
      "\n",
      "le\n",
      "d\n",
      "\n",
      " Y\n",
      "\n",
      "−0.5 0.5 1.5 2.5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "y\n",
      "\n",
      "f(\n",
      "y)\n",
      "\n",
      "Fig. 4.5. Results of Gibbs sampler using the inversion method for sampling from\n",
      "conditional densities.\n",
      "\n",
      "lower left figure shows it after 100 iterations; and the lower right figure shows\n",
      "it after the 2,000 iterations. Here again, we see that the algorithm, although\n",
      "starting with poor starting values, converged rapidly to the appropriate two-\n",
      "dimensional, partial plane region represented by f(x, y).\n",
      "\n",
      "After sampling from the distribution for x and y, we can now summarize\n",
      "our knowledge of the density. The theoretical mean for x can be found by\n",
      "taking the marginal for x (f(x) = (1/28)(4x + 10)) and by integrating across\n",
      "all values for x:\n",
      "\n",
      "µx =\n",
      "∫ 2\n",
      "\n",
      "0\n",
      "\n",
      "x× f(x)dx = 1.095.\n",
      "\n",
      "A similar calculation for y yields a theoretical mean of 1.143. The empirical\n",
      "estimates of the means, based on the last 1,500 draws from the marginal\n",
      "distributions for the variables (discarding the first 500 as the burn-in) are\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 93\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "Fig. 4.6. Results of Gibbs sampler using the inversion method for sampling from\n",
      "conditional densities: Two-dimensional view after 5, 25, 100, and 2,000 iterations.\n",
      "\n",
      "x̄ = 1.076 and ȳ = 1.158. The discrepancy between the theoretical and the\n",
      "empirical means is attributable to sampling error in the MCMC algorithm. A\n",
      "longer run would reduce the error, although, even with 1,500 simulated draws,\n",
      "the discrepancies here are minimal (less than 2% for both x and y).\n",
      "\n",
      "4.4.3 Example repeated using rejection sampling\n",
      "\n",
      "In the Gibbs sampling algorithm we just discussed, we used the inversion\n",
      "method for sampling from the conditional distributions of x and y. It is often\n",
      "the case that using the inversion method may not be feasible, for several rea-\n",
      "sons. First, the conditionals in the Gibbs sampler may not be univariate. That\n",
      "is, we do not have to break our conditional distributions into univariate con-\n",
      "ditional densities; we may choose multivariate conditional densities, as we will\n",
      "see in Chapter 7. Second, F ()−1 may not be calculable, even in one dimension.\n",
      "\n",
      "\n",
      "\n",
      "94 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "For example, if the distribution were bivariate normal, the conditionals would\n",
      "be univariate normal, and F ()−1 cannot be analytically computed.2 Third,\n",
      "even if the inverse of the density is calculable, the normalizing constant in the\n",
      "conditional may not be easily computable. The inversion algorithm technically\n",
      "requires the complete computation of F ()−1, which, in this case, requires us to\n",
      "know both the numerator and the denominator of the formulas for the condi-\n",
      "tional distributions. It is often the case that we do not know the exact formula\n",
      "for a conditional distribution, but instead, we know the conditional only up\n",
      "to a normalizing (proportionality) constant. Generally speaking, conditional\n",
      "distributions are proportional to the joint distribution evaluated at the point\n",
      "of conditioning. So, for example, in the example discussed above, if we know\n",
      "y = q, then the following is true:\n",
      "\n",
      "f(x | y = q) = (1/28)×\n",
      "2x + 3q + 2\n",
      "\n",
      "6q + 8\n",
      "∝ 2x + 3q + 2.\n",
      "\n",
      "Notice that (1/28)(6q + 8) is not contained in the final proportionality; the\n",
      "reason is that this factor is simply a constant that scales this slice of the joint\n",
      "density so that its integral is 1. However, this constant is not necessary for\n",
      "Gibbs sampling to work! Why not? Because the Gibbs sampler will only set\n",
      "y = q in direct proportion to its relative frequency in the joint density. Put\n",
      "another way, the Gibbs sampler will visit y = q as often as it should under\n",
      "the joint density. This result is perhaps easier to see in a contingency table;\n",
      "consider the example displayed in Table 4.1.\n",
      "\n",
      "Table 4.1. Cell counts and marginals for a hypothetical bivariate dichotomous\n",
      "distribution.\n",
      "\n",
      "x = 0 x = 1 x|y = k\n",
      "\n",
      "y = 0 a b a + b\n",
      "\n",
      "y = 1 c d c + d\n",
      "\n",
      "y|x = m a + c b + d a + b + c + d\n",
      "\n",
      "In this example, if we follow a Gibbs sampling strategy, we would choose\n",
      "a starting value for x and y; suppose we chose 0 for each. If we started with\n",
      "y = 0, we would then select x = 0 with probability a/(a + b) and x = 1\n",
      "with probability b/(a + b). Once we had chosen our x, if x had been 0, we\n",
      "would then select y = 0 with probability a/(a+ c) and y = 1 with probability\n",
      "c/(a + c). On the other hand, if we had selected x = 1, we would then select\n",
      "\n",
      "2 Again, we do have efficient algorithms for computing this integral, but it cannot\n",
      "be directly analytically computed.\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 95\n",
      "\n",
      "y = 0 with probability b/(b + d) and y = 1 with probability d/(b + d). Thus,\n",
      "we would be selecting y = 0 with total probability\n",
      "\n",
      "p(y = 0) = p(y = 0 | x = 0)p(x = 0) + p(y = 0 | x = 1)p(x = 1).\n",
      "\n",
      "So,\n",
      "\n",
      "p(y = 0) =\n",
      "(\n",
      "\n",
      "a\n",
      "\n",
      "a + c\n",
      "\n",
      ")(\n",
      "a + c\n",
      "\n",
      "a + b + c + d\n",
      "\n",
      ")\n",
      "+\n",
      "(\n",
      "\n",
      "b\n",
      "\n",
      "b + d\n",
      "\n",
      ")(\n",
      "b + d\n",
      "\n",
      "a + b + c + d\n",
      "\n",
      ")\n",
      "=\n",
      "\n",
      "a + b\n",
      "a + b + c + d\n",
      "\n",
      ".\n",
      "\n",
      "This proportion reflects exactly how often we should choose y = 0, given\n",
      "the marginal distribution for y in the contingency table. Thus, the normalizing\n",
      "constant is not relevant, because the Gibbs sampler will visit each value of\n",
      "one variable in proportion to its relative marginal frequency, which leads us to\n",
      "then sample the other variable, conditional on the first, with the appropriate\n",
      "relative marginal frequency.\n",
      "\n",
      "Returning to the example at hand, then, we simply need to know what\n",
      "the conditional distribution is proportional to in order to sample from it.\n",
      "Here, if we know y = q, then f(x | y = q) ∝ 2x + 3q + 2. Because we do not\n",
      "necessarily always know this normalizing constant, using the inversion method\n",
      "of sampling will not work.3 However, we can simulate from this density using\n",
      "rejection sampling. Recall from the discussion of rejection sampling that we\n",
      "need an enveloping function g(x) that, when multiplied by a constant m,\n",
      "returns a value that is greater than f(x) for all x. With an unnormalized\n",
      "density, only m must be adjusted relative to what it would be under the\n",
      "normalized density in order to ensure this rule is followed. In this case, if we\n",
      "will be sampling from the joint density, we can use a uniform density on the\n",
      "[0, 2] interval multiplied by a constant m that ensures that the density does\n",
      "not exceed m × .5 (.5 is the height of the U(0,2) density). The joint density\n",
      "reaches a maximum where x and y are both 2; that peak value is 12. Thus,\n",
      "if we set m = 25, the U(0, 2) density multiplied by m will always be above\n",
      "the joint density. And, we can ignore the normalizing constants, including\n",
      "the leading (1/28) in the joint density and the 1/(6y + 8) in the conditional\n",
      "for x and the 1/(4x + 10) in the conditional for y. As exemplified above, the\n",
      "Gibbs sampler will sample from the marginals in the correct proportion to\n",
      "their relative frequency in the joint density. Below is a Gibbs sampler that\n",
      "simulates from f(x, y) using rejection sampling:\n",
      "3 The normalizing constant must be known one way or another. Certainly, we can\n",
      "\n",
      "perform the integration we need to compute F−1 so long as the distribution is\n",
      "proper. However, if we do not know the normalizing constant, the integral will\n",
      "differ from 1, which necessitates that our uniform draw representing the area\n",
      "under the curve be scaled by the inverse of the normalizing constant in order to\n",
      "represent the area under the unnormalized density fully.\n",
      "\n",
      "\n",
      "\n",
      "96 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "#R program for Gibbs sampling using rejection sampling\n",
      "\n",
      "x=matrix(-1,2000); y=matrix(-1,2000)\n",
      "\n",
      "for(i in 2:2000)\n",
      "\n",
      "{\n",
      "\n",
      "#sample from x | y using rejection sampling\n",
      "\n",
      "z=0\n",
      "\n",
      "while(z==0)\n",
      "\n",
      "{\n",
      "\n",
      "u=runif(1,min=0, max=2)\n",
      "\n",
      "if( ((2*u)+(3*y[i-1])+2) > (25*runif(1,min=0,max=1)*.5))\n",
      "\n",
      "{x[i]=u; z=1}\n",
      "\n",
      "}\n",
      "\n",
      "#sample from y | x using rejection sampling\n",
      "\n",
      "z=0\n",
      "\n",
      "while(z==0)\n",
      "\n",
      "{\n",
      "\n",
      "u=runif(1,min=0,max=2)\n",
      "\n",
      "if( ((2*x[i])+(3*u)+2) > (25*runif(1,min=0,max=1)*.5))\n",
      "\n",
      "{y[i]=u; z=1}\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "In this program, the overall Gibbs sampling process is the same as for the\n",
      "inversion sampling approach; the only difference is that we are now using re-\n",
      "jection sampling to sample from the unnormalized conditional distributions.\n",
      "One consequence of switching sampling methods is that we have now had to\n",
      "use better starting values (−1 here versus −5 under inversion sampling). The\n",
      "reason for this is that the algorithm will never get off the ground otherwise.\n",
      "Notice that the first item to be selected is x[2]. If y[1] is -5, the first condi-\n",
      "tional statement (if . . .) will never be true: The value on the left side of the\n",
      "expression, ((2*u)+(3*y[i-1])+2), can never be positive, but the value on\n",
      "the right, (25*runif(1,min=0,max=1)*.5), will always be positive. So, the\n",
      "algorithm will “stick” in the first while loop.\n",
      "\n",
      "Figures 4.7 and 4.8 are replications of the previous two figures produced\n",
      "under rejection sampling. The overall results appear the same. For example,\n",
      "the mean for x under the rejection sampling approach was 1.085, and the\n",
      "mean for y was 1.161, which are both very close to those obtained using the\n",
      "inversion method.\n",
      "\n",
      "4.4.4 Gibbs sampling from a real bivariate density\n",
      "\n",
      "The densities we examined in the examples above were very basic densities\n",
      "(linear and planar) and are seldom used in social science modeling. In this\n",
      "section, I will discuss using Gibbs sampling to sample observations from a\n",
      "density that is commonly used in social science research—the bivariate normal\n",
      "density. As discussed in Chapter 2, the bivariate normal density is a special\n",
      "case of the multivariate normal density in which the dimensionality of the\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 97\n",
      "\n",
      "0 500 1000 1500 2000\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "S\n",
      "a\n",
      "\n",
      "m\n",
      "p\n",
      "\n",
      "le\n",
      "d\n",
      "\n",
      " X\n",
      "\n",
      "−0.5 0.5 1.5 2.5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "0 500 1000 1500 2000\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "S\n",
      "a\n",
      "\n",
      "m\n",
      "p\n",
      "\n",
      "le\n",
      "d\n",
      "\n",
      " Y\n",
      "\n",
      "−0.5 0.5 1.5 2.5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "y\n",
      "\n",
      "f(\n",
      "y)\n",
      "\n",
      "Fig. 4.7. Results of Gibbs sampler using rejection sampling to sample from condi-\n",
      "tional densities.\n",
      "\n",
      "density is 2, and the variables—say x and y—in this density are related by\n",
      "the correlation parameter ρ. For the sake of this example, we will use the\n",
      "standard bivariate normal density—that is, the means and variances of both\n",
      "x and y are 0 and 1, respectively—and we will assume that ρ is a known\n",
      "constant (say, .5). The pdf in this case is:\n",
      "\n",
      "f(x, y|ρ) =\n",
      "1\n",
      "\n",
      "2π\n",
      "√\n",
      "\n",
      "1− ρ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "x2 − 2ρxy + y2\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "In order to use Gibbs sampling for sampling values of x and y, we need to\n",
      "determine the full conditional distributions for both x and y, that is, f(x|y)\n",
      "and f(y|x). I have suppressed the conditioning on ρ in these densities, simply\n",
      "because ρ is a known constant in this problem.\n",
      "\n",
      "\n",
      "\n",
      "98 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x (after 5 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x (after 25 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x (after 100 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−5 0 5 10\n",
      "\n",
      "−\n",
      "5\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "x (after 2000 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "Fig. 4.8. Results of Gibbs sampler using rejection sampling to sample from condi-\n",
      "tional densities: Two-dimensional view after 5, 25, 100, and 2,000 iterations.\n",
      "\n",
      "As we discussed above, Gibbs sampling does not require that we know\n",
      "the normalizing constant; we only need to know to what density each con-\n",
      "ditional density is proportional. Thus, we will drop the leading constant\n",
      "(1/(2π\n",
      "\n",
      "√\n",
      "1− ρ2)). The conditional for x then requires that we treat y as\n",
      "\n",
      "known. If y is known, we can reexpress the kernel of the density as\n",
      "\n",
      "f(x|y) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "x2 − x(2ρy)\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "y2\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      ",\n",
      "\n",
      "and we can drop the latter exponential containing y2, because it is simply a\n",
      "proportionality constant with respect to x. Thus, we are left with the left-hand\n",
      "exponential. If we complete the square in x, we obtain\n",
      "\n",
      "f(x|y) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "(x2 − x(2ρy) + (ρy)2 − (ρy)2)\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 99\n",
      "\n",
      "which reduces to\n",
      "\n",
      "f(x|y) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "(x− ρy)2 − (ρy)2\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Given that both ρ and y are constants in the conditional for x, the latter term\n",
      "on the right in the numerator can be extracted just as y2 was above, and we\n",
      "are left with:\n",
      "\n",
      "f(x|y) ∝ exp\n",
      "{\n",
      "−\n",
      "\n",
      "(x− ρy)2\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Thus, the full conditional for x can be seen as proportional to a univariate\n",
      "normal density with a mean of ρy and a variance of (1− ρ2). We can find the\n",
      "full conditional for y exactly the same way. By symmetry, the full conditional\n",
      "for y will be proportional to a univariate normal density with a mean of ρx\n",
      "and the same variance.\n",
      "\n",
      "Writing a Gibbs sampler to sample from this bivariate density, then, is\n",
      "quite easy, especially given that R (and most languages) have efficient algo-\n",
      "rithms for sampling from normal distributions (rnorm in R). Below is an R\n",
      "program that does such sampling:\n",
      "\n",
      "#R program for Gibbs sampling from a bivariate normal pdf\n",
      "\n",
      "x=matrix(-10,2000); y=matrix(-10,2000)\n",
      "\n",
      "for(j in 2:2000)\n",
      "\n",
      "{\n",
      "\n",
      "#sampling from x|y\n",
      "\n",
      "x[j]=rnorm(1,mean=(.5*y[j-1]),sd=sqrt(1-.5*.5))\n",
      "\n",
      "#sampling from y|x\n",
      "\n",
      "y[j]=rnorm(1,mean=(.5*x[j]),sd=sqrt(1-.5*.5))\n",
      "\n",
      "}\n",
      "\n",
      "This algorithm is quite similar to the Gibbs sampler shown previously for\n",
      "the bivariate planar density. The key difference is that the conditionals are\n",
      "normal; thus, x and y are updated using the rnorm random sampling function.\n",
      "\n",
      "Figure 4.9 shows the state of the algorithm after 10, 50, 200, and 2,000\n",
      "iterations. As the figure shows, despite the poor starting values of −10 for both\n",
      "x and y, the algorithm rapidly converged to the appropriate region (within\n",
      "10 iterations).\n",
      "\n",
      "Figure 4.10 contains four graphs. The upper graphs show the marginal\n",
      "distributions for x and y for the last 1,500 iterations of the algorithm, with\n",
      "the appropriate “true” marginal distributions superimposed. As these graphs\n",
      "show, the Gibbs sampler appears to have generated samples from the appro-\n",
      "priate marginals. In fact, the mean and standard deviation for x are .059 and\n",
      ".984, respectively, which are close to their true values of 0 and 1. Similarly, the\n",
      "mean and standard deviation for y were .012 and .979, which are also close to\n",
      "their true values.\n",
      "\n",
      "\n",
      "\n",
      "100 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "−10 −6 −2 0 2 4\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "−\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "x (after 10 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−10 −6 −2 0 2 4\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "−\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "x (after 50 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−10 −6 −2 0 2 4\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "−\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "x (after 200 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "−10 −6 −2 0 2 4\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "−\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "2\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "x (after 2000 iterations)\n",
      "\n",
      "y\n",
      "\n",
      "Fig. 4.9. Results of Gibbs sampler for standard bivariate normal distribution with\n",
      "correlation r = .5: Two-dimensional view after 10, 50, 200, and 2,000 iterations.\n",
      "\n",
      "As I said earlier, we are typically interested in just the marginal distri-\n",
      "butions. However, I also stated that the samples of x and y can also be\n",
      "considered—after a sufficient number of burn-in iterations—as a sample from\n",
      "the joint density for both variables. Is this true? The lower left graph in the\n",
      "figure shows a contour plot for the true standard bivariate normal distribution\n",
      "with correlation r = .5. The lower right graph shows this same contour plot\n",
      "with the Gibbs samples superimposed. As the figure shows, the countour plot\n",
      "is completely covered by the Gibbs samples.\n",
      "\n",
      "4.4.5 Reversing the process: Sampling the parameters given the data\n",
      "\n",
      "Sampling data from densities, conditional on the parameters of the density,\n",
      "as we did in the previous section is an important process, but the process of\n",
      "Bayesian statistics is about sampling parameters conditional on having data,\n",
      "\n",
      "\n",
      "\n",
      "4.4 Introduction to MCMC sampling 101\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "x\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "y\n",
      "\n",
      "f(\n",
      "y)\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "● ●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "● ●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "Fig. 4.10. Results of Gibbs sampler for standard bivariate normal distribution:\n",
      "Upper left and right graphs show marginal distributions for x and y (last 1,500\n",
      "iterations); lower left graph shows contour plot of true density; and lower right\n",
      "graph shows contour plot of true density with Gibbs samples superimposed.\n",
      "\n",
      "not about sampling data conditional on knowing the parameters. As I have\n",
      "repeatedly said, however, from the Bayesian perspective, both data and pa-\n",
      "rameters are considered random quantities, and so sampling the parameters\n",
      "conditional on data is not a fundamentally different process than sampling\n",
      "data conditional on parameters. The main difference is simply in the mathe-\n",
      "matics we need to apply to the density to express it as a conditional density\n",
      "for the parameters rather than for the data. We first saw this process in the\n",
      "previous chapter when deriving the conditional posterior distribution for the\n",
      "mean parameter from a univariate normal distribution.\n",
      "\n",
      "Let’s first consider a univariate normal distribution example. In the pre-\n",
      "vious chapter, we derived two results for the posterior distributions for the\n",
      "mean and variance parameters (assuming a reference prior of 1/σ2). In one, we\n",
      "showed that the posterior density could be factored to produce (1) a marginal\n",
      "posterior density for σ2 that was an inverse gamma distribution, and (2) a\n",
      "conditional posterior density for µ that was a normal distribution:\n",
      "\n",
      "\n",
      "\n",
      "102 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "p(σ2|X) ∝ IG ((n− 1)/2 , (n− 1)var(x)/2)\n",
      "p(µ|σ2, X) ∝ N\n",
      "\n",
      "(\n",
      "x̄ , σ2/n\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "In the second derivation for the posterior distribution for σ2, we showed\n",
      "that the conditional (not marginal) distribution for σ2 was also an inverse\n",
      "gamma distribution, but with slightly different parameters:\n",
      "\n",
      "p(σ2|µ,X) ∝ IG\n",
      "(\n",
      "n/2 ,\n",
      "\n",
      "∑\n",
      "(xi − µ)2/2\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "Both of these derivations lend themselves easily to Gibbs sampling. Under\n",
      "the first derivation, we could first sample a vector of values for σ2 from the\n",
      "marginal distribution and then sample a value for µ conditional on each value\n",
      "of σ2 from its conditional distribution. Under the second derivation, we would\n",
      "follow the iterative process shown in the previous sections, first sampling a\n",
      "value for σ2 conditional on µ, then sampling a value for µ conditional on the\n",
      "new value for σ2, and so on.\n",
      "\n",
      "In practice, the first approach is more efficient. However, some situations\n",
      "may warrant the latter approach (e.g., when missing data are included). Here,\n",
      "I show both approaches in estimating the average years of schooling for the\n",
      "adult U.S. population in 2000. The data for this example are from the 2000\n",
      "National Health Interview Survey (NHIS), a repeated cross-sectional survey\n",
      "conducted annually since 1969. The data set is relatively large by social science\n",
      "standards, consisting of roughly 40,000 respondents in each of many years. In\n",
      "2000, after limiting the data to respondents 30 years and older and deleting\n",
      "observations missing on education, I obtained an analytic sample of 17,946\n",
      "respondents. Mean educational attainment in the sample was 12.69 years (s.d.\n",
      "= 3.16 years), slightly below the mean of 12.74 from the 2000 U.S. Census.4\n",
      "\n",
      "Below is an R program that first samples 2,000 values of the variance of\n",
      "educational attainment (σ2) from its inverse gamma marginal distribution\n",
      "and then, conditional on each value for σ2, samples µ from the appropriate\n",
      "normal distribution:\n",
      "\n",
      "#R: sampling from marginal for variance and conditional for mean\n",
      "\n",
      "x<-as.matrix(read.table(\"c:\\\\education.dat\",header=F)[,1])\n",
      "\n",
      "sig<-rgamma(2000,(length(x)-1)/2 , rate=((length(x)-1)*var(x)/2))\n",
      "\n",
      "sig<-1/sig\n",
      "\n",
      "mu<-rnorm(2000,mean=mean(x),sd=(sqrt(sig/length(x))))\n",
      "\n",
      "4 In calculating the mean from the census, I recoded the census categories for (1)\n",
      "under 9 years; (2) 9-12 years, no diploma; (3) high-school graduate or equivalent;\n",
      "(4) some college, no degree; (5) Associate degree; (6) Bachelor degree; and (4)\n",
      "graduate or professional degree to the midpoint for years of schooling and created\n",
      "a ceiling of 17 years, which is the upper limit for the NHIS.\n",
      "\n",
      "\n",
      "\n",
      "4.5 Conclusions 103\n",
      "\n",
      "This program is remarkably short, first reading the data into a vector X\n",
      "and then generating 2,000 draws from a gamma distribution with the appro-\n",
      "priate shape and scale parameters. These draws are then inverted, because R\n",
      "has no direct inverse gamma distribution; thus, I make use of the fact that, if\n",
      "1/x is gamma distributed with parameters a and b, then x is inverse gamma\n",
      "distributed with the same parameters. Finally, the program samples µ from\n",
      "its appropriate normal distribution.\n",
      "\n",
      "Below is the R program for the alternative approach in which µ and σ are\n",
      "sequentially sampled from their conditional distributions:\n",
      "\n",
      "#R: sampling from conditionals for both variance and mean\n",
      "\n",
      "x<-as.matrix(read.table(\"c:\\\\education.dat\",header=F)[,1])\n",
      "\n",
      "mu=matrix(0,2000); sig=matrix(1,2000)\n",
      "\n",
      "for(i in 2:2000)\n",
      "\n",
      "{\n",
      "\n",
      "sig[i]=rgamma(1,(length(x)/2),rate=sum((x-mu[i-1])^2)/2)\n",
      "\n",
      "sig[i]=1/sig[i]\n",
      "\n",
      "mu[i]=rnorm(1,mean=mean(x),sd=sqrt(sig[i]/length(x)))\n",
      "\n",
      "}\n",
      "\n",
      "Under this approach, we must select starting values for µ and σ2; here I\n",
      "use 0 and 1, respectively (assigned when the matrices are defined in R), which\n",
      "are far from their estimates based on the sample means. This approach also\n",
      "necessitates looping, as we saw in the planar density earlier.\n",
      "\n",
      "Figure 4.11 shows the results of both algorithms. The first 1,000 draws have\n",
      "been discarded from each run, because the poor starting values in the second\n",
      "algorithm imply that convergence is not immediate. In contrast, under the\n",
      "first method, convergence is immediate; the first 1,000 are discarded simply\n",
      "to have comparable sample sizes. As the figure shows, the results are virtually\n",
      "identical for the two approaches.\n",
      "\n",
      "Numerically, the posterior means for µ under the two approaches were both\n",
      "12.69, and the posterior means for σ2 were 10.01 and 10.00, respectively (the\n",
      "means for\n",
      "\n",
      "√\n",
      "σ2 were both 3.16). These results are virtually identical to the\n",
      "\n",
      "sample estimates of these parameters, as they should be. A remaining question\n",
      "may be: What are the reasonable values for mean education in the population?\n",
      "In order to answer this question, we can construct a 95% “empirical probability\n",
      "interval” for µ by taking the 25th and 975th sorted values of µ from our\n",
      "Gibbs samples. For both approaches, the resulting interval is [12.64 , 12.73],\n",
      "which implies that the true population mean for years of schooling falls in this\n",
      "interval with probability .95.\n",
      "\n",
      "4.5 Conclusions\n",
      "\n",
      "As we have seen in the last two chapters, the Bayesian approach to inference\n",
      "involves simply summarizing the posterior density using basic sample statistics\n",
      "\n",
      "\n",
      "\n",
      "104 4 Modern Model Estimation Part 1: Gibbs Sampling\n",
      "\n",
      "12.5 12.6 12.7 12.8 12.9\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "µ\n",
      "\n",
      "f(µ\n",
      ")\n",
      "\n",
      "9.5 10.0 10.5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "σ2\n",
      "\n",
      "f(σ\n",
      "2\n",
      ")\n",
      "\n",
      "Fig. 4.11. Samples from posterior densities for a mean and variance parameter\n",
      "for NHIS years of schooling data under two Gibbs sampling approaches: The solid\n",
      "lines are the results for the marginal-for-σ2-but conditional-for-µ approach; and the\n",
      "dashed lines are the results for the full conditionals approach.\n",
      "\n",
      "like the mean, median, variance, and various quantiles of the distribution.\n",
      "When posterior densities are such that these integral-based statistics cannot\n",
      "be directly computed—e.g., when they are multivariate—modern Bayesian\n",
      "statistics turns to sampling from the posterior density and to computing these\n",
      "quantities just as we would when we have a sample of data.\n",
      "\n",
      "Gibbs sampling provides a fairly easy method for sampling from multivari-\n",
      "ate densities, so long as we can derive the appropriate conditional densities.\n",
      "In most problems, this reduces simply to (1) treating other variables as fixed\n",
      "in the joint density, and (2) determining how to sample from the resulting\n",
      "conditional density. Sometimes, the conditional densities take known forms,\n",
      "as they did in our normal distribution example. Other times, the conditional\n",
      "densities may be derivable, but they may take unknown forms, as they did in\n",
      "our linear and planar distributions examples. In the latter case, we may turn\n",
      "to inversion or rejection sampling for sampling from the conditionals with\n",
      "unknown forms.\n",
      "\n",
      "\n",
      "\n",
      "4.6 Exercises 105\n",
      "\n",
      "In some cases, however, inversion of a conditional density may not be pos-\n",
      "sible, and rejection sampling may be difficult or very inefficient. In those cases,\n",
      "Bayesians can turn to another method—the Metropolis-Hastings algorithm.\n",
      "Discussion of that method is the topic of the next chapter. For alternative and\n",
      "more in-depth and theoretical expositions of the Gibbs sampler, I recommend\n",
      "the entirety of Gilks, Richardson, and Spiegelhalter 1996 in general and Gilks\n",
      "1996 in particular. I also recommend a number of additional readings in the\n",
      "concluding chapter of this book.\n",
      "\n",
      "4.6 Exercises\n",
      "\n",
      "1. Find the inverse distribution function (F−1) for y|x in the bivariate planar\n",
      "density; that is, show how a U(0, 1) sample must be transformed to be a\n",
      "draw from y|x.\n",
      "\n",
      "2. Develop a rejection sampler for sampling data from the bivariate planar\n",
      "density f(x) ∝ 2x + 3y + 2.\n",
      "\n",
      "3. Develop an inversion sampler for sampling data from the linear density\n",
      "f(x) ∝ 5x + 2. (Hint: First, find the normalizing constant, and then find\n",
      "the inverse function).\n",
      "\n",
      "4. Develop an appropriate routine for sampling the λ parameter from the\n",
      "Poisson distribution voting example in the previous chapter.\n",
      "\n",
      "5. Develop an appropriate routine for sampling 20 observations (data points)\n",
      "from an N(0, 1) distribution. Then, reverse the process using these data\n",
      "to sample from the posterior distribution for µ and σ2. Use the noninfor-\n",
      "mative prior p(µ , σ2) ∝ 1/σ2, and use either Gibbs sampler described\n",
      "in the chapter. Next, plot the posterior density for µ, and superimpose\n",
      "an appropriate t distribution over this density. How close is the match?\n",
      "Discuss.\n",
      "\n",
      "6. As we have seen throughout this chapter, computing integrals (e.g., the\n",
      "mean and variance) using sampling methods yields estimates that are not\n",
      "exact in finite samples but that become better and better estimates as the\n",
      "sample size increases. Describe how we might quantify how much sampling\n",
      "error is involved in estimating quantities using sampling methods (Hint:\n",
      "Consider the Central Limit Theorem).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "\n",
      "Modern Model Estimation Part 2:\n",
      "Metroplis–Hastings Sampling\n",
      "\n",
      "Gibbs sampling is a very powerful tool, and to date, it has been used by statis-\n",
      "ticians performing Bayesian analyses perhaps more than any other Markov\n",
      "chain Monte Carlo (MCMC) method. However, Gibbs sampling has several\n",
      "limitations. First, there are cases in which a conditional distribution cannot\n",
      "be easily derived or determined from the joint density. Such cases are rare,\n",
      "as we discussed previously: Conditional densities are proportional to the joint\n",
      "density. Nonetheless, consider the simple generic linear density discussed be-\n",
      "fore:\n",
      "\n",
      "f(x|m, b) =\n",
      "2(mx + b)\n",
      "\n",
      "(s− r)[m(s + r) + 2b]\n",
      ".\n",
      "\n",
      "With the boundaries s and r as known constants, the likelihood function for\n",
      "m and b would be:\n",
      "\n",
      "L(m, b|X) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "mxi + b\n",
      "m(s + r) + 2b\n",
      "\n",
      ". (5.1)\n",
      "\n",
      "Even ignoring the denominator, the repeated multiplication of the numerator\n",
      "becomes messy. For example, consider just the first two terms:\n",
      "\n",
      "L(m, b|X) ∝\n",
      "2∏\n",
      "\n",
      "i=1\n",
      "\n",
      "(mxi + b)\n",
      "\n",
      "= (mx1 + b)(mx2 + b)\n",
      "\n",
      "= m2x1x2 + mx1b + mx2b + b\n",
      "2.\n",
      "\n",
      "Given that almost every term contains both m and b, and given that these\n",
      "terms are additive, it would be extremely difficult (if not impossible) to deter-\n",
      "mine the conditionals for either parameter, almost regardless of what type of\n",
      "prior density one chose for the parameters. Fortunately, the most common dis-\n",
      "tributions used in social science research are exponential family distributions\n",
      "\n",
      "\n",
      "\n",
      "108 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "(e.g., the normal and Poisson distributions) that do not generally present this\n",
      "type of problem.\n",
      "\n",
      "A second, more common case in which Gibbs sampling may not be optimal\n",
      "is if a conditional density is not of a known form, inversion sampling from it\n",
      "is impossible, or it is difficult to find an appropriate envelope for rejection\n",
      "sampling. For example, determining the maximum value of a density in a\n",
      "multivariate model may be analytically difficult. We will discuss this case in\n",
      "greater depth below.\n",
      "\n",
      "A third case in which Gibbs sampling may be limited in its usefulness is if\n",
      "Gibbs sampling is simply inefficient for the problem at hand. We will discuss\n",
      "this case in the next chapter and in Chapter 10, but essentially, there are\n",
      "cases in which sampling from the conditional distributions may lead to very\n",
      "slow “mixing” of the algorithm. That is, rather than converging rapidly to the\n",
      "center of the distribution of interest and sampling rapidly throughout it, the\n",
      "algorithm may stick in a low-density area, moving slowly to the main support\n",
      "of the density and moving slowly within it once it reaches it.\n",
      "\n",
      "In each of these cases, an alternative to Gibbs sampling is the more generic\n",
      "Metropolis-Hastings (MH) algorithm. In this chapter, I first describe the MH\n",
      "algorithm in some detail. I then apply it to two generic examples in which\n",
      "Gibbs sampling would difficult to implement.\n",
      "\n",
      "5.1 A generic MH algorithm\n",
      "\n",
      "The MH algorithm is an algorithm that generates samples from a probability\n",
      "distribution, using the full joint density function (see Hastings 1970 for the\n",
      "original exposition; see Gilks, Richardson, and Spiegelhalter 1996 for a pre-\n",
      "sentation of various MH algorithms). A key advantage to the MH algorithm\n",
      "over other methods of sampling, like inversion and rejection sampling, is that\n",
      "it will work with multivariate distributions (unlike inversion sampling), and\n",
      "we do not need an enveloping function (as in rejection sampling).\n",
      "\n",
      "A basic MH algorithm consists of the following steps:\n",
      "\n",
      "1. Establish starting values S for the parameter: θj=0 = S. Set j = 1.\n",
      "2. Draw a “candidate” parameter, θc from a “proposal density,” α(.).\n",
      "3. Compute the ratio R = f(θ\n",
      "\n",
      "c)α(θj−1|θc)\n",
      "f(θj−1)α(θc|θj−1) .\n",
      "\n",
      "4. Compare R with a U(0, 1) random draw u. If R > u, then set θj = θc.\n",
      "Otherwise, set θj = θj−1.\n",
      "\n",
      "5. Set j = j + 1 and return to step 2 until enough draws are obtained.\n",
      "\n",
      "In the first step, one must establish starting values for the parameters, just\n",
      "as in Gibbs sampling. Starting values can be obtained via maximum likelihood\n",
      "estimation or via some other (even arbitrary) method. MCMC theory says\n",
      "that the algorithm’s stationary distribution—that is, the distribution to which\n",
      "the Markov chain produced by the algorithm converges—will be the posterior\n",
      "\n",
      "\n",
      "\n",
      "5.1 A generic MH algorithm 109\n",
      "\n",
      "distribution of interest, regardless of the starting values chosen (Tierney 1996).\n",
      "However, particularly poor starting values may cause the algorithm to reject\n",
      "many candidates and hence not move quickly toward the main support of the\n",
      "posterior, leading to extremely long run times. We will discuss how to assess\n",
      "and address this problem, as well as others, in the next chapter.\n",
      "\n",
      "In step 2, a candidate value for the parameter (θc) is obtained by sim-\n",
      "ulating a value for it from a proposal density [α(.)]. The simulated value is\n",
      "considered a “candidate,” because it is not automatically accepted as a draw\n",
      "from the distribution of interest; it must be evaluated for acceptance just as in\n",
      "rejection sampling. Indeed, this step is somewhat akin to drawing a candidate\n",
      "from an enveloping function in rejection sampling, except that the proposal\n",
      "density need not actually envelop the posterior distribution. Instead, a pro-\n",
      "posal density can take any form that is easy to sample from (normal, uniform,\n",
      "etc.). Given that the proposal density is not the density of interest, we must\n",
      "check to determine whether the candidate parameter can be considered to be\n",
      "from the target distribution, just as we did in rejection sampling.\n",
      "\n",
      "Often, we may use a symmetric proposal density (e.g., a normal or uni-\n",
      "form) centered over the current value of the parameter (θj−1). For example,\n",
      "using a normal proposal density, the candidate would be drawn from a normal\n",
      "distribution with a mean equal to θj−1 and some variance: θc = θj−1+N(0, c),\n",
      "where c is a constant (more on the choice for c later). This approach yields\n",
      "a “random walk Metropolis algorithm,” which is the main algorithm dis-\n",
      "cussed/used in this book because of its simplicity and effectiveness.\n",
      "\n",
      "The use of symmetric proposals centered over the previous value of the\n",
      "parameter is not necessary, but understanding asymmetry in the context of\n",
      "MH algorithms requires some discussion. In the MH algorithm, asymmetry\n",
      "means that α(θc|θj−1) 6= α(θj−1|θc). In other words, there is greater proba-\n",
      "bility that either the candidate would be proposed when the chain is in state\n",
      "θj−1 than θj−1 would be proposed when the chain is in state θc, or vice versa.\n",
      "To clarify this point, see Figure 5.1. The figure shows normal proposal densi-\n",
      "ties centered over both the candidate and the previous values. Because these\n",
      "densities themselves are symmetric and centered over the candidate and pre-\n",
      "vious values, the height of the proposal density at the candidate value, when\n",
      "the proposal is centered over the previous value, is the same as the height of\n",
      "the proposal density at the previous value, when the proposal is centered over\n",
      "the candidate value. This result implies that the chain is just as likely to move\n",
      "from the candidate to the previous value as it is to move from the previous\n",
      "value to the candidate.\n",
      "\n",
      "When might a proposal density be asymmetric, or how may asymme-\n",
      "try arise? First, we can use proposals that are asymmetric densities, like\n",
      "the lognormal distribution. We have not discussed this distribution, but if\n",
      "ln(x) ∼ N(µ, σ2), then x ∼ LogN(µ, σ2). The distribution is therefore skewed\n",
      "right (consider the distribution of income and ln(income), for instance). Con-\n",
      "\n",
      "\n",
      "\n",
      "110 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "θj−1 θc\n",
      "\n",
      "α(θc | θj−1)α(θj−1 | θc)\n",
      "\n",
      "Symmetric Proposals:\n",
      "\n",
      "α(θj−1 | θc)=α(θc | θj−1)\n",
      "\n",
      "α(θc)α(θj−1)\n",
      "\n",
      "Fig. 5.1. Example of symmetric proposals centered over the previous and candidate\n",
      "values of the parameters.\n",
      "\n",
      "sider using a lognormal proposal centered so that θ is the mode of the density.1\n",
      "\n",
      "Figure 5.2 shows this case. Given the right-hand skew/asymmetry of the pro-\n",
      "posal distributions, the probability of moving from θj−1 to θc is not the same\n",
      "as moving from θc to θj−1.\n",
      "\n",
      "A second way in which asymmetry may occur is if we use proposal densities\n",
      "that are fixed, regardless of the current state of the chain. For example, we\n",
      "could specify the proposal distribution to be an N(0, 100) distribution, with\n",
      "this proposal not changing from one iteration of the algorithm to the next. In\n",
      "\n",
      "1 The mode (MO) of a lognormal density is equal to exp(µ−σ2), and so the proposal\n",
      "is: α(θc|θj−1) = (θj−1 −MO) + LogN(µ, σ2).\n",
      "\n",
      "\n",
      "\n",
      "5.1 A generic MH algorithm 111\n",
      "\n",
      "θj−1 θc\n",
      "\n",
      "Asymmetry: α(θj−1 | θc) < α(θc | θj−1)\n",
      "\n",
      "α(θc)\n",
      "\n",
      "α(θj−1)\n",
      "\n",
      "Fig. 5.2. Example of asymmetric proposals centered at the mode over the previous\n",
      "and candidate values of the parameters.\n",
      "\n",
      "other words, the proposal could remain N(0, 100) regardless of whether θ is\n",
      "currently 12 or 55 (just to pick some random values). This approach would\n",
      "make the algorithm quite similar to a rejection sampler.2\n",
      "\n",
      "A final way in which asymmetry may occur, and one that is most relevant\n",
      "for the types of models discused in this book, is if the density for a parameter\n",
      "is bounded. For example, a variance parameter cannot take a value less than 0.\n",
      "If we are using a uniform proposal to update the parameter, however, it might\n",
      "\n",
      "2 In fact, this sampler is called the independence sampler and may not work well at\n",
      "all if the posterior density is not enveloped by the proposal—see Gilks, Richard-\n",
      "son, and Spiegelhalter (1996).\n",
      "\n",
      "\n",
      "\n",
      "112 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "be possible for an illegal value to be proposed: Whenever the chain wanders\n",
      "toward the boundary, the left-hand side of the proposal density may overlap 0.\n",
      "We may choose to allow the proposal of invalid candidates and simply realize\n",
      "that, when a candidate is selected that is less than 0, f(θc) = 0, and so the\n",
      "chain does not move (i.e., we automatically set θj = θj−1). Alternatively,\n",
      "we may choose to change the proposal density to exclude values below the\n",
      "boundary and thus increase the efficiency of the algorithm (i.e., no illegitimate\n",
      "values will be proposed as candidates). However, if we follow this strategy, we\n",
      "are actually increasing the probability that the algorithm will select values\n",
      "away from the boundary more often than it should. To see how asymmetry\n",
      "occurs in this case, refer to Figure 5.3. The figure shows the chain in two\n",
      "states—θj−1 and θc. When the chain is in state θj−1, it can only propose\n",
      "candidates that are greater than 0, and hence, the proposal is both asymmetric\n",
      "around θj−1 and tall, because the area under the density must be 1. When the\n",
      "chain is in state θc, on the other hand, the boundary constraint is not an issue,\n",
      "and therefore the density is symmetric and short. The upside to imposing the\n",
      "boundary constraint is that we will not propose illegitimate candidates. The\n",
      "downside is that we now need to compute the latter half of the ratio R to\n",
      "compensate for the asymmetry.\n",
      "\n",
      "Generally speaking, with large data sets and with the models discussed in\n",
      "this book, although boundary constraints exist, they generally do not pose\n",
      "much of a problem: It is a rare event when a parameter approaches the\n",
      "boundary. Thus, the efficiency gain by using an asymmetric proposal like\n",
      "in Figure 5.3 is often minimal.\n",
      "\n",
      "Returning to step 3, when asymmetric proposals are used (or asymme-\n",
      "try arises via the second or third cases), a correction factor in the ratio R\n",
      "helps adjust for the asymmetry. This correction factor is the latter half of the\n",
      "ratio in step 3. The first part of this ratio (called an “importance ratio”)—\n",
      "the f(θc)/f(θj−1)—is simply the ratio of the unnormalized posterior density\n",
      "evaluated at the candidate parameter value (θc) to the posterior density eval-\n",
      "uated at the previous parameter value (θj−1). The second part of the ratio\n",
      "[the α(θj−1|θc)/α(θc|θj−1)] is the ratio of the proposal densities evaluated at\n",
      "the candidate and previous points. The (a|b) means the probability (really, the\n",
      "density height of α) that a candidate value a would be proposed, given that\n",
      "the chain is in location b. This ratio adjusts for the fact that, with asymmetric\n",
      "proposals, some candidate values may be selected more often than others, and\n",
      "it makes the algorithm a Metropolis-Hastings algorithm rather than simply a\n",
      "Metropolis algorithm.\n",
      "\n",
      "In step 4, we simulate a draw u from a U(0, 1) density and compare it with\n",
      "our ratio R. If R > u, we accept the candidate as a draw from our posterior\n",
      "density p(.). Otherwise, we retain the previous parameter value. Let’s consider\n",
      "this step in some detail. An alternative way that this step is often presented is\n",
      "to say “accept the candidate with probability min(R, 1)” (Johnson and Albert\n",
      "1999). In this representation, the “min” function is included as a formality\n",
      "to indicate that probabilities cannot exceed 1. Thus, again, the comparison\n",
      "\n",
      "\n",
      "\n",
      "5.1 A generic MH algorithm 113\n",
      "\n",
      "θj−1 θc\n",
      "\n",
      "Proposal height at θj−1\n",
      "\n",
      "Proposal height at θc\n",
      "\n",
      "Asymmetry: α(θc | θj−1) > α(θj−1 | θc)\n",
      "Boundary at 0\n",
      "\n",
      "Fig. 5.3. Example of asymmetry in proposals due to a boundary constraint on the\n",
      "parameter space.\n",
      "\n",
      "is between R and some random probability (hence, the U(0, 1) random draw\n",
      "u). If R is greater than 1, then the candidate will be accepted. If R is large,\n",
      "but less than 1, then the candidate will almost certainly be accepted. If R is\n",
      "small, then the candidate will occasionally/rarely be accepted. Finally, if R\n",
      "is 0, the candidate will not be accepted. This step ensures that the accepted\n",
      "candidates come from the distribution of interest, p(.).\n",
      "\n",
      "5.1.1 Relationship between Gibbs and MH sampling\n",
      "\n",
      "As you might guess, Gibbs sampling and MH are related approaches to sim-\n",
      "ulation. First, the Gibbs sampler can be viewed as a special case of the MH\n",
      "\n",
      "\n",
      "\n",
      "114 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "algorithm in which the proposal densities for each parameter/random variable\n",
      "are the full conditionals. In Gibbs sampling, every candidate is selected; there\n",
      "is no rejection of candidates. The reason is that the ratio R is always 1. Why?\n",
      "Consider the components of the ratio R:\n",
      "\n",
      "R =\n",
      "f(θc)α(θj−1|θc)\n",
      "\n",
      "f(θj−1)α(θc|θj−1)\n",
      ". (5.2)\n",
      "\n",
      "The first term in the numerator is the posterior distribution evaluated at the\n",
      "candidate value. The second term in the numerator is the posterior probability\n",
      "of returning to the previous point given that we are at the current point. In the\n",
      "denominator, the first expression is equivalent to this latter value, because the\n",
      "proposal density is already the full conditional for the parameter. Similarly,\n",
      "the second expression in the denominator is equivalent to the first term in the\n",
      "numerator. Thus f(θc) = α(θc|θj−1), and vice versa. Since the parameter is\n",
      "accepted with probability min(1, R), and it is always true that R = 1, every\n",
      "draw is accepted.\n",
      "\n",
      "The 100% acceptance rate makes the Gibbs sampler more efficient, and\n",
      "therefore generally faster than MH algorithms, just as the inversion sampling\n",
      "method is more efficient than the rejection sampling method. However, as\n",
      "mentioned previously and discussed later in the book, there are cases in which\n",
      "the MH algorithm is more efficient, and there are cases in which the difficulty\n",
      "of implementing Gibbs sampling—in terms of deriving conditionals—overrides\n",
      "its efficiency. I have sometimes found it easier to use a random walk metropolis\n",
      "algorithm and to let the computer labor more intensively than for me to labor\n",
      "more intensively to derive complex conditionals and let the computer work\n",
      "less!\n",
      "\n",
      "A second link between Gibbs sampling and MH sampling is that they can\n",
      "be combined to sample from multivariate densities. Recall that the Gibbs sam-\n",
      "pler splits the joint density into conditional densities. In the previous chapter,\n",
      "we sampled from each conditional using the inversion method in one example\n",
      "and rejection sampling in the next. There is no inherent reason we cannot\n",
      "use different sampling methods for sampling from different conditionals; for\n",
      "example, using inversion sampling for sampling from x|y and rejection sam-\n",
      "pling for sampling from y|x. By the same token, there is no inherent reason we\n",
      "cannot use an MH subalgorithm to sample from one of the conditional distri-\n",
      "butions. This approach has been termed “Metropolis within Gibbs” sampling\n",
      "(see Gilks 1996).\n",
      "\n",
      "We can also view such a hybrid approach as “Gibbs within Metropolis”\n",
      "sampling, if we view the overall algorithm as an MH algorithm with the com-\n",
      "ponent parameters updated one-at-a-time. Indeed, we should recall that Gibbs\n",
      "sampling is simply a special case of MH sampling in the first place, and so\n",
      "the entire algorithm is always an MH algorithm!\n",
      "\n",
      "Because the MH algorithm does not require that all parameters in a prob-\n",
      "lem be updated simultaneously, updating parameters one-at-a-time in an MH\n",
      "algorithm makes MH sampling seem like Gibbs sampling. However, the MH\n",
      "\n",
      "\n",
      "\n",
      "5.2 Example: MH sampling when conditional densities are difficult to derive 115\n",
      "\n",
      "algorithm differs from the Gibbs sampler in two important ways. First, the\n",
      "candidate parameter is not automatically accepted, because it comes from a\n",
      "proposal density and not from the appropriate conditional distribution. Sec-\n",
      "ond, the proposal density is not an enveloping function as in the rejection\n",
      "sampling routine discussed above. In short, all Gibbs sampling is MH sam-\n",
      "pling, but not all MH sampling is Gibbs sampling.\n",
      "\n",
      "Some of the algorithms discussed in the later chapters of this book will\n",
      "involve mixtures of Gibbs and MH steps, so it is important to realize that their\n",
      "combination is possible and sometimes even a desirable alternative to one or\n",
      "the other approach. For the models presented in the later chapters, virtually\n",
      "all of the algorithms that involve MH steps technically will be random walk\n",
      "metropolis algorithms involving symmetric proposals.\n",
      "\n",
      "5.2 Example: MH sampling when conditional densities\n",
      "are difficult to derive\n",
      "\n",
      "In Chapter 2 I suggested that a linear density may fit the 2000 GSS item\n",
      "regarding the importance of expressing unpopular views (“free speech”) better\n",
      "than some other distribution. As we discussed at the beginning of this chapter,\n",
      "it would be quite difficult to determine the conditional distributions for the\n",
      "linear density parameters, m and b, and so Gibbs sampling would not be\n",
      "a good option for sampling from the posterior density. For that matter, a\n",
      "classical analysis involving finding the maximum likelihood estimate of these\n",
      "parameters would be difficult also. Therefore, here I develop an MH algorithm\n",
      "for sampling from the posterior density for m and b. First, I assume uniform\n",
      "prior distributions for the parameters such that p(m) ∝ U(−∞,∞) and p(b) ∝\n",
      "U(0,∞). Although these priors are improper (they cannot integrate to 1), the\n",
      "posterior distribution is proper, subject to an important constraint: m and\n",
      "b must be perfectly related. The possible responses to the free speech item\n",
      "range from 0 to 5.3 Thus, if b = 0, m must equal 2 in order for the area\n",
      "under the (sampling) density to equal 1. Similarly, if b = 5, m must be −2.\n",
      "More generally, b = (2 − (s − r)2m)/2(s − r), (find this) and so, once m is\n",
      "known, b is automatically determined. This finding makes an MH algorithm a\n",
      "simple one-parameter process. Below is a random walk metropolis algorithm\n",
      "that samples from the posterior distribution for m (and by default, b):\n",
      "\n",
      "#R program for Random Walk Metropolis algorithm\n",
      "\n",
      "m<-matrix(0,5000); b<-matrix(.2,5000); z<-matrix(0,1377)\n",
      "\n",
      "z[1:17]=0; z[18:32]=1; z[33:103]=2\n",
      "\n",
      "z[104:425]=3; z[426:826]=4; z[827:1337]=5\n",
      "\n",
      "3 The original metric was integers ranging from 1 to 6, but I have recoded to the\n",
      "(0,5) interval and have made a standard assumption that responses are continu-\n",
      "ously, rather than discretely, distributed for simplicity.\n",
      "\n",
      "\n",
      "\n",
      "116 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "acctot=0\n",
      "\n",
      "for(i in 2:5000)\n",
      "\n",
      "{\n",
      "\n",
      "m[i]=m[i-1]+rnorm(1,mean=0,sd=.002)\n",
      "\n",
      "b[i]=(2-25*m[i])/10\n",
      "\n",
      "acc=tot=1\n",
      "\n",
      "for(j in 1:1377)\n",
      "\n",
      "{tot=tot*((z[j]*m[i]+b[i])/(z[j]*m[i-1]+b[i-1]))}\n",
      "\n",
      "if(runif(1,min=0,max=1)>tot || b[i]<0)\n",
      "\n",
      "{m[i]=m[i-1]; b[i]=b[i-1]; acc=0}\n",
      "\n",
      "acctot=acctot+acc\n",
      "\n",
      "if(i%%10==0){print(c(i,m[i],b[i],acctot/i))}\n",
      "\n",
      "}\n",
      "\n",
      "The first line of this program establishes (1) the vectors and starting values\n",
      "for the slope (m) and intercept (b) parameters (step 1 of the general MH\n",
      "algorithm described above), and (2) the vector for the data (z). The next\n",
      "two lines constitute the data, with 17/1337 persons responding “0”, 15/1337\n",
      "persons responding “1,” and so on. The third line sets a variable acctot equal\n",
      "to 0; this variable keeps a tally of the number of candidate parameter values\n",
      "that are accepted over the course of the algorithm. (We will discuss the need\n",
      "for this quantity in some depth later).\n",
      "\n",
      "The subsequent lines of the program constitute the iterative looping for\n",
      "the MH algorithm. First, at each iteration, a candidate parameter value for\n",
      "m is generated using a normal proposal density centered over the previous\n",
      "value for m (m[i]=m[i-1]+rnorm(1,mean=0,sd=.005) ), and b is computed\n",
      "(b[i]=(2-25*m[i])/10 ), given the candidate value for m (step 2 of the\n",
      "general MH algorithm). The next line sets variables acc and tot equal to\n",
      "1. acc is an indicator for whether a candidate is accepted. By default, the\n",
      "algorithm assumes the candidate is accepted; acc is set to 0 later in the\n",
      "event the candidate is rejected. tot is initialized at 1; this variable is used to\n",
      "tabulate the ratio R.\n",
      "\n",
      "The next two lines loop over the data in generating the unnormalized\n",
      "posterior density value at the current and previous values for the parameters.\n",
      "This is the ratio R from step 3 of the general MH algorithm.4\n",
      "\n",
      "In accordance with Step 4 of the general MH algorithm, the following lines\n",
      "(1) compare this ratio to a U(0, 1) draw (say u), and (2) set the values of m\n",
      "and b to their previous values if R < u or b < 0 (the prior constraint on b).\n",
      "\n",
      "4 As written, the program computes this ratio one person at a time and sequentially\n",
      "multiplies the results together, because computing the entire posterior density at\n",
      "either the candidate or previous value yields a number too large for the computer\n",
      "to handle. That is,\n",
      "\n",
      "Qn\n",
      "i=1\n",
      "\n",
      "Ai/\n",
      "Qn\n",
      "\n",
      "i=1\n",
      "Bi =\n",
      "\n",
      "Qn\n",
      "i=1\n",
      "\n",
      "Ai/Bi, but the former leads to\n",
      "overflows in the numerator and denominator, whereas the latter does not.\n",
      "\n",
      "\n",
      "\n",
      "5.2 Example: MH sampling when conditional densities are difficult to derive 117\n",
      "\n",
      "At this point, if the candidate values are rejected, the indicator acc is set to\n",
      "0.\n",
      "\n",
      "For monitoring the acceptance rate, acc is then added to acctot. The last\n",
      "few lines print the current results to the screen, including the current values\n",
      "of the parameters and the updated acceptance rate.\n",
      "\n",
      "Figure 5.4 shows the results of the algorithm. The upper graph shows a\n",
      "trace plot of the parameter m across the run of the algorithm. As the figure\n",
      "shows, m converged very rapidly from its poor starting value of 0 to a region\n",
      "around .07. The lower graph is a histogram of the last 3,000 sampled values of\n",
      "m, with a vertical reference line at the mean of m (m̄ = .0697, s.d. = .0013).\n",
      "The mean for b was .0257 (s.d. = .003), and the overall acceptance rate of\n",
      "the algorithm was 56%. The posterior mean values for m and b were used in\n",
      "Figure 2.3 to construct the superimposed “best linear fit” line.\n",
      "\n",
      "0 1000 2000 3000 4000 5000\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "4\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "\n",
      "0.060 0.065 0.070 0.075 0.080\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      "3\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "m\n",
      "\n",
      "f(\n",
      "m\n",
      "\n",
      ")\n",
      "\n",
      "Fig. 5.4. Trace plot and histogram of m parameter from linear density model for\n",
      "the 2000 GSS free speech data.\n",
      "\n",
      "\n",
      "\n",
      "118 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "5.3 Example: MH sampling for a conditional density\n",
      "with an unknown form\n",
      "\n",
      "In Chapter 2, in addition to suggesting that a linear density may fit the free\n",
      "speech item better than a normal distribution, we also discussed that a planar\n",
      "density may fit the two GSS items concerning the importance of free speech\n",
      "and political participation better than some other distribution. A limitation\n",
      "of the planar density we discussed, however, is that it does not allow us to\n",
      "consider that the two items may be related and to estimate the extent of the\n",
      "relationship. Yet, the two certainly may be related, and they appear to be,\n",
      "given the peaks along the diagonal that the planar density is unable to model.\n",
      "\n",
      "A density that does allow us to estimate the relationship between the\n",
      "two items is the bivariate normal distribution, which we have discussed in\n",
      "the previous chapter. For this example, we will assume that the data from\n",
      "Table 2.1 follow a bivariate normal distribution, and we will estimate the\n",
      "parameter ρ, which is the correlation between the two variables. In order to\n",
      "keep the mathematics relatively simple at this point, we will standardize the\n",
      "variables and assume that the means (µx and µy) and variances (σ2x and σ\n",
      "\n",
      "2\n",
      "y)\n",
      "\n",
      "for each variable are 0 and 1, respectively. In that case, the likelihood function\n",
      "for the data is:\n",
      "\n",
      "L(ρ|x, y) =\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "\n",
      "2π\n",
      "√\n",
      "\n",
      "1− ρ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "[\n",
      "x2i − 2ρxiyi + y\n",
      "\n",
      "2\n",
      "i\n",
      "\n",
      "]}\n",
      ".\n",
      "\n",
      "In order to make the analysis fully Bayesian, we need to specify an appro-\n",
      "priate prior distribution for the sole parameter ρ. In the univariate normal\n",
      "distribution, 1/σ2 is a reference prior; in the bivariate normal distribution,\n",
      "1/|Σ|(3/2) is an analogous prior (the Jeffreys prior; see Gelman et al. 1995),\n",
      "where Σ is the covariance matrix of x and y. Given that we have standard-\n",
      "ized the variables and have assumed each variance parameter is known to be\n",
      "1, the prior is: 1/(1 − ρ2)(3/2). Under this prior, our posterior density for n\n",
      "observations is simply:\n",
      "\n",
      "f(ρ|x, y) ∝\n",
      "1\n",
      "\n",
      "(1− ρ2)(3/2)\n",
      "nY\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "\n",
      "2π\n",
      "p\n",
      "\n",
      "1− ρ2\n",
      "exp\n",
      "\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "2(1− ρ2)\n",
      "ˆ\n",
      "x\n",
      "\n",
      "2\n",
      "i − 2ρxiyi + y\n",
      "\n",
      "2\n",
      "i\n",
      "\n",
      "˜ff\n",
      ".\n",
      "\n",
      "This posterior density can be simplified somewhat by carrying out the multi-\n",
      "plication and combining like terms as:\n",
      "\n",
      "f(ρ|x, y) ∝\n",
      "1\n",
      "\n",
      "(1− ρ2)(n+3)/2\n",
      "exp\n",
      "\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "hX\n",
      "x\n",
      "\n",
      "2\n",
      "i − 2ρ\n",
      "\n",
      "X\n",
      "xiyi +\n",
      "\n",
      "X\n",
      "y\n",
      "2\n",
      "i\n",
      "\n",
      "iff\n",
      ".\n",
      "\n",
      "However, because the −1/2(1 − ρ2) is multiplicative within the exponential,\n",
      "there is nothing more that can be done to simplify the posterior density. As\n",
      "\n",
      "\n",
      "\n",
      "5.3 Example: MH sampling for a conditional density with an unknown form 119\n",
      "\n",
      "it stands, this posterior density for ρ is not a known form, and so Gibbs\n",
      "sampling is not a straightforward option (but see Albert 1992 for an approach\n",
      "to using Gibbs sampling to sample a correlation parameter via transformation\n",
      "of variables in an unrelated model).5 Below is an R program that uses MH\n",
      "sampling to sample from the posterior density for ρ. To save space, I have\n",
      "omitted a number of initial lines that define the data (from Table 2.1):\n",
      "\n",
      "#MH algorithm for sampling a correlation parameter\n",
      "\n",
      "#x and y are the data, already stored in memory\n",
      "\n",
      "lnpost<-function(r)\n",
      "\n",
      "{-690*log(1-r^2)-.5*(sum(x^2)-2*r*sum(x*y)+sum(y^2))/(1-r^2)}\n",
      "\n",
      "corr=matrix(0,10000); acctot=0\n",
      "\n",
      "for(i in 2:10000)\n",
      "\n",
      "{\n",
      "\n",
      "corr[i]=corr[i-1]+runif(1,min=-.07,max=.07)\n",
      "\n",
      "acc=1\n",
      "\n",
      "if(abs(corr[i])>1){acc=0; corr[i]=corr[i-1]}\n",
      "\n",
      "if((lnpost(corr[i])-lnpost(corr[i-1]))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{corr[i]=corr[i-1]; acc=0}\n",
      "\n",
      "acctot=acctot+acc\n",
      "\n",
      "if(i%%100==0){print(c(i,corr[i],acctot/i))}\n",
      "\n",
      "}\n",
      "\n",
      "The first line of this program creates a function (lnpost) that evaluates\n",
      "the log of the posterior density, given the data and a specified value for the\n",
      "correlation coefficient (r). Here, I have introduced a new approach—using the\n",
      "log of the posterior density rather than the posterior density. With a sample\n",
      "as large as this one (n = 1, 377), evaluating the posterior itself would generate\n",
      "an underflow problem because of the large negative exponents involved in the\n",
      "posterior. Evaluating the log of the posterior resolves this problem. However,\n",
      "it requires a change later in the program as I will discuss.\n",
      "\n",
      "After the matrix corr[] is created to store the samples of ρ, and the\n",
      "acceptance rate total is established, the main loop begins. As with the linear\n",
      "density example in the previous section, the parameter is first updated using\n",
      "a uniform proposal density. The candidate is assumed to be accepted (acc=1),\n",
      "but it is then evaluated twice for rejection. It is first evaluated to determine\n",
      "whether it is a legitimate value for a correlation (within the [−1, 1] interval).\n",
      "Next, it is evaluated using the standard step 3 of the generic MH algorithm:\n",
      "If R < u, the candidate is rejected. However, given that we are comparing log\n",
      "posterior values, the ratio R becomes a subtraction, and it must be compared\n",
      "with the log of a uniform draw, rather than simply a uniform draw. As before,\n",
      "5 In fact, univariate sampling is really not Gibbs sampling, but this finding—that\n",
      "\n",
      "the conditional is an unknown form—would hold if the mean and variance pa-\n",
      "rameters were being estimated rather than assumed to be known quantities.\n",
      "\n",
      "\n",
      "\n",
      "120 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "the acceptance rate is then updated, and the program occasionally prints some\n",
      "output to the screen for monitoring.\n",
      "\n",
      "Figure 5.5 provides a graphic display of the results. I show a trace plot of\n",
      "ρ across the 10,000 iterations of the algorithm (upper figure) and a histogram\n",
      "of the last several thousand samples of ρ, with a reference line superimposed\n",
      "at the posterior mean for ρ (ρ̄ = .4504; s.d.(ρ) = .02) (lower figure). As\n",
      "with the m parameter from the linear density example, ρ appears to have\n",
      "rapidly converged from its starting value of 0 to its posterior distribution.\n",
      "The results suggest that there is a moderately strong, positive relationship\n",
      "between people’s beliefs in the importance of free speech and of participation\n",
      "in the political process.\n",
      "\n",
      "0 2000 4000 6000 8000 10000\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "Iteration\n",
      "\n",
      "ρ\n",
      "\n",
      "0.3 0.4 0.5 0.6 0.7\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "ρ\n",
      "\n",
      "f(ρ\n",
      ")\n",
      "\n",
      "Fig. 5.5. Trace plot and histogram of ρ parameter from bivariate normal density\n",
      "model for the 2000 GSS free speech and political participation data.\n",
      "\n",
      "\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full multiparameter model 121\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full\n",
      "multiparameter model\n",
      "\n",
      "In the preceding section, we standardized the data and assumed the mean\n",
      "and variance parameters in the bivariate normal distribution were known.\n",
      "The result is that our posterior mean estimate for ρ is almost identical to\n",
      "what we would have obtained if we had just computed the Pearson corre-\n",
      "lation: The Pearson correlation for these data was also .4504. Although I\n",
      "did not present an empirical 95% interval for ρ derived from the MH al-\n",
      "gorithm above, this interval was [.408, .485]. A comparable interval can be\n",
      "constructed for the Pearson correlation. Specifically, the traditional way to\n",
      "construct such an interval is to use Fisher’s z transformation of the corre-\n",
      "lation: z = .5 × (ln(1 + ρ)− ln(1− ρ)) . Whereas the correlation is bounded\n",
      "on the [−1, 1] interval and therefore tends to have a skewed distribution,\n",
      "Fisher’s z is approximately normal, with an approximate standard devia-\n",
      "tion of σz = 1/\n",
      "\n",
      "√\n",
      "n− 3. Thus, a 95% interval for z can be constructed as\n",
      "\n",
      "z ± 1.96 × σz. Once the interval is obtained, the upper and lower bounds\n",
      "can be transformed back to the original ρ scale (see Snedecor and Cochran\n",
      "1980). For this particular problem, the resulting interval is [.407, .492], which\n",
      "is a result comparable with but broader than the one we obtained using the\n",
      "MH algorithm.\n",
      "\n",
      "A key problem with our Bayesian interval estimate is that we have com-\n",
      "pletely ignored uncertainty in the values of the other parameters in the model:\n",
      "µx, µy, σ\n",
      "\n",
      "2\n",
      "x, and σ\n",
      "\n",
      "2\n",
      "y. As a result, our interval is almost certainly too small.\n",
      "\n",
      "Given that we are almost certainly not interested in any parameter other than\n",
      "ρ, the other parameters are sometimes called “nuissance parameters.” They\n",
      "are not of fundamental interest, but they must be dealt with in order to obtain\n",
      "a valid interval estimate for ρ.\n",
      "\n",
      "A straightforward solution exists under the Bayesian approach using an\n",
      "MH algorithm. Here, I show how Gibbs sampling steps and MH sampling\n",
      "steps can be combined in order to obtain a more appropriate interval from\n",
      "the marginal posterior distribution of ρ.\n",
      "\n",
      "Assuming the Jeffreys prior 1/|Σ|3/2 as before, the complete posterior\n",
      "density f(ρ, µx, µy, σ2x, σ\n",
      "\n",
      "2\n",
      "y|x, y) is proportional to:\n",
      "\n",
      "1\n",
      "|Σ|3/2\n",
      "\n",
      "n∏\n",
      "i=1\n",
      "\n",
      "1\n",
      "\n",
      "2πσxσy\n",
      "√\n",
      "\n",
      "1− ρ2\n",
      "×\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "[\n",
      "(xi − µx)2\n",
      "\n",
      "σ2x\n",
      "−\n",
      "\n",
      "2ρ(xi − µx)(yi − µy)\n",
      "σxσy\n",
      "\n",
      "+\n",
      "(yi − µy)2\n",
      "\n",
      "σ2y\n",
      "\n",
      "]}\n",
      ".\n",
      "\n",
      "This posterior density can be simplified somewhat by carrying out the multi-\n",
      "plication:\n",
      "\n",
      "(σ\n",
      "2\n",
      "x)\n",
      "−(3+n)/2\n",
      "\n",
      "(σ\n",
      "2\n",
      "y)\n",
      "−(3+n)/2\n",
      "\n",
      "(1− ρ2)−(3+n)/2×\n",
      "\n",
      "\n",
      "\n",
      "122 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "exp\n",
      "\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "2(1− ρ2)\n",
      "\n",
      "» P\n",
      "(xi − µx)2\n",
      "\n",
      "σ2x\n",
      "−\n",
      "\n",
      "2ρ\n",
      "P\n",
      "\n",
      "(xi − µx)(yi − µy)\n",
      "σxσy\n",
      "\n",
      "+\n",
      "\n",
      "P\n",
      "(yi − µy)2\n",
      "\n",
      "σ2y\n",
      "\n",
      "–ff\n",
      ".\n",
      "\n",
      "(5.3)\n",
      "\n",
      "5.4.1 The conditionals for µx and µy\n",
      "\n",
      "In order to develop a Gibbs sampling routine (with MH steps for simulating σ2x,\n",
      "σ2y, and ρ), we need to derive the conditionals for the parameters in the model.\n",
      "This process is tedious, although not fundamentally difficult. In deriving the\n",
      "conditional for µx, as before we can eliminate multiplicative terms that are\n",
      "constant with respect to µx. The remaining terms are:\n",
      "\n",
      "p(µx|µy, σ2x, σ\n",
      "2\n",
      "y, ρ, x, y) ∝\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "[∑\n",
      "(xi − µx)2\n",
      "\n",
      "σ2x\n",
      "−\n",
      "\n",
      "2ρ\n",
      "∑\n",
      "\n",
      "(xi − µx)(yi − µy)\n",
      "σxσy\n",
      "\n",
      "]}\n",
      ".\n",
      "\n",
      "This result can be simplified by expanding the quadratic expressions and\n",
      "again eliminating terms that do not involve µx as proportionality constants.\n",
      "Following this strategy leaves us with:\n",
      "\n",
      "p(µx|µy, σ2x, σ\n",
      "2\n",
      "y, ρ, x, y) ∝\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "[\n",
      "n\n",
      "\n",
      "σ2x\n",
      "(µ2x − 2x̄µx)−\n",
      "\n",
      "2nρ\n",
      "σxσy\n",
      "\n",
      "(µxµy − ȳµx)\n",
      "]}\n",
      "\n",
      ".\n",
      "\n",
      "In order to conserve space, I do not elaborate on all the remaining steps.\n",
      "However, if we take this expression, (1) factor an n from the numerator, (2)\n",
      "multiply through by σ2xσy to eliminate the denominator, and (3) rearrange\n",
      "terms, we obtain:\n",
      "\n",
      "exp\n",
      "{(\n",
      "\n",
      "−\n",
      "n\n",
      "\n",
      "2σ2x(1− ρ2)\n",
      "\n",
      ")[\n",
      "µ2x − 2µx\n",
      "\n",
      "(\n",
      "x̄ +\n",
      "\n",
      "σxρ\n",
      "\n",
      "σy\n",
      "(µy − ȳ)\n",
      "\n",
      ")]}\n",
      ".\n",
      "\n",
      "As in the previous chapter, we can now complete the square in µx, but even\n",
      "without doing this, we may recognize that the conditional for µx is:\n",
      "\n",
      "µx|µy, σx, σy, ρ, x, y ∼ N\n",
      "(\n",
      "\n",
      "x̄ +\n",
      "σx\n",
      "σy\n",
      "\n",
      "ρ(µy − ȳ) ,\n",
      "σ2x(1− ρ2)\n",
      "\n",
      "n\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "Suppose, momentarily that we know ρ = 0. In that case, the conditional\n",
      "posterior for µx would reduce to the posterior for the mean in the previous\n",
      "chapter:\n",
      "\n",
      "µx|ρ = 0 ∼ N\n",
      "(\n",
      "x̄ , σ2x/n\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "By symmetry, we can obtain a comparable conditional distribution for µy;\n",
      "only the x and y subscripts must be changed. What remains is determining the\n",
      "conditional distributions for the two variance parameters and the correlation.\n",
      "\n",
      "\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full multiparameter model 123\n",
      "\n",
      "5.4.2 The conditionals for σ2x, σ\n",
      "2\n",
      "y, and ρ\n",
      "\n",
      "The derivations for the conditional distributions for the variance parameters\n",
      "and correlation parameter are not as straightforward as the derivations for the\n",
      "mean parameters. First, as we noted before, because ρ appears in multiplica-\n",
      "tive form within the exponential, there are no terms that can be separated\n",
      "from this parameter, and so there is little simplication that can be done to\n",
      "clarify the distribution for ρ. Only the leading terms (σ2x)\n",
      "\n",
      "(3+n)/2(σ2y)\n",
      "(3+n)/2\n",
      "\n",
      "outside the exponential can be eliminated as proportionality constants.\n",
      "Second, using scalar notation with the model parameterized in terms of\n",
      "\n",
      "the correlation parameter ρ rather than the covariance parameter σxy, the\n",
      "univariate conditional distributions for the variance parameters are also diffi-\n",
      "cult to derive. For example, in terms of the conditional for σ2x, let’s begin with\n",
      "the full posterior density found in Equation 5.3. In this expression, the lead-\n",
      "ing multiplicative terms not involving σ2x can be eliminated as constants, and\n",
      "within the exponential, the additive terms not involving σ2x can be eliminated.\n",
      "This leaves us with the conditional posterior:\n",
      "\n",
      "p(σ2x|µx, µy, σ\n",
      "2\n",
      "y, ρ, x, y) ∝\n",
      "\n",
      "(σ2x)\n",
      "−(3+n)/2 exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)\n",
      "\n",
      "[∑\n",
      "(xi − µx)2\n",
      "\n",
      "σ2x\n",
      "−\n",
      "\n",
      "2ρ\n",
      "∑\n",
      "\n",
      "(xi − µx)(yi − µy)\n",
      "σxσy\n",
      "\n",
      "]}\n",
      ".\n",
      "\n",
      "Next, we can multiply the terms in the exponential through by σ2xσy to elim-\n",
      "inate the denominator to obtain (within the exponential):\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2(1− ρ2)σ2xσy\n",
      "\n",
      "[\n",
      "σy\n",
      "∑\n",
      "\n",
      "(xi − µx)2 − 2σxρ\n",
      "∑\n",
      "\n",
      "(xi − µx)(yi − µy)\n",
      "]}\n",
      "\n",
      ".\n",
      "\n",
      "If we rearrange terms a little, we obtain:\n",
      "\n",
      "p(σ2x|µx, µy, σ\n",
      "2\n",
      "y, ρ, x, y) ∝\n",
      "\n",
      "(σ2x)\n",
      "−(n+1)/2+1 exp\n",
      "\n",
      "{\n",
      "−\n",
      "[\n",
      "σy\n",
      "∑\n",
      "\n",
      "(xi − µx)2 − 2σxρ\n",
      "∑\n",
      "\n",
      "(xi − µx)(yi − µy)\n",
      "2(1− ρ2)σy\n",
      "\n",
      "]\n",
      "/σ2x\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "In this representation, the distribution for σ2 almost appears to be inverse\n",
      "gamma with parameters α = (n + 1)/2 and β equal to the expression in\n",
      "brackets within the exponential. However, there is an additional σx term in the\n",
      "expression that cannot be factored out. A similar expression can be found for\n",
      "σ2y. In fact, we could use the matrix representation for the multivariate normal\n",
      "distribution and derive the conditional distribution for the entire covariance\n",
      "matrix Σ, and we will do so in the next section. For now, however, consider\n",
      "that it is simpler, at this point, to use MH steps to update the variance\n",
      "parameters as well as the correlation parameter.\n",
      "\n",
      "\n",
      "\n",
      "124 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "5.4.3 The complete MH algorithm\n",
      "\n",
      "Below is a hybrid algorithm with Gibbs sampling steps for updating the mean\n",
      "parameters µx and µy and with MH steps for updating σ2x and σ\n",
      "\n",
      "2\n",
      "y:\n",
      "\n",
      "# R program for simulating parameters from BVN distribution\n",
      "\n",
      "# x and y are the data, already stored in memory\n",
      "\n",
      "lnpost<-function(ar,amx,amy,asx,asy,ax,ay,axy)\n",
      "\n",
      "{return(-690*log((1-ar^2)*asx*asy)\n",
      "\n",
      "+(-.5/(1-ar^2))*(ax/asx - 2*ar*axy/sqrt(asx*asy) + ay/asy))}\n",
      "\n",
      "mnx=mean(x); mny=mean(y); accr=0; accx=0; accy=0\n",
      "\n",
      "mx=matrix(0,10000); my=matrix(0,10000);\n",
      "\n",
      "sx=matrix(1,10000); sy=matrix(1,10000); r=matrix(0,10000)\n",
      "\n",
      "for(i in 2:10000){\n",
      "\n",
      "#sample mx from normal\n",
      "\n",
      "mx[i]<-rnorm(1,mean=mnx+(r[i-1]*sx[i-1]*(my[i-1]-mny))/sy[i-1]\n",
      "\n",
      ",sd=sqrt(sx[i-1]*(1-r[i-1]^2)/1377))\n",
      "\n",
      "#sample my from normal\n",
      "\n",
      "my[i]<-rnorm(1,mean=mny+(r[i-1]*sy[i-1]*(mx[i]-mnx))/sx[i-1]\n",
      "\n",
      ",sd=sqrt(sy[i-1]*(1-r[i-1]^2)/1377))\n",
      "\n",
      "#update sums of squares\n",
      "\n",
      "sx2=sum((x-mx[i])^2); sy2=sum((y-my[i])^2);\n",
      "\n",
      "sxy=sum((x-mx[i])*(y-my[i]))\n",
      "\n",
      "#sample sx\n",
      "\n",
      "sx[i]=sx[i-1]+runif(1,min=-.1,max=.1); acc=1\n",
      "\n",
      "if(sx[i]<0){acc=0; sx[i]=sx[i-1]}\n",
      "\n",
      "if((lnpost(r[i-1],mx[i],my[i],sx[i],sy[i-1],sx2,sy2,sxy)\n",
      "\n",
      "-lnpost(r[i-1],mx[i],my[i],sx[i-1],sy[i-1],sx2,sy2,sxy))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{acc=0; sx[i]=sx[i-1]}\n",
      "\n",
      "accx=accx+acc\n",
      "\n",
      "#sample sy\n",
      "\n",
      "sy[i]=sy[i-1]+runif(1,min=-.1,max=.1); acc=1\n",
      "\n",
      "if(sy[i]<0){acc=0; sy[i]=sy[i-1]}\n",
      "\n",
      "if((lnpost(r[i-1],mx[i],my[i],sx[i],sy[i],sx2,sy2,sxy)\n",
      "\n",
      "-lnpost(r[i-1],mx[i],my[i],sx[i],sy[i-1],sx2,sy2,sxy))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{acc=0; sy[i]=sy[i-1]}\n",
      "\n",
      "accy=accy+acc\n",
      "\n",
      "#sample r from full posterior using MH step\n",
      "\n",
      "\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full multiparameter model 125\n",
      "\n",
      "r[i]=r[i-1]+runif(1,min=-.05,max=.05); acc=1\n",
      "\n",
      "if(abs(r[i])>1){acc=0; r[i]=r[i-1]}\n",
      "\n",
      "if((lnpost(r[i],mx[i],my[i],sx[i],sy[i],sx2,sy2,sxy)\n",
      "\n",
      "-lnpost(r[i-1],mx[i],my[i],sx[i],sy[i],sx2,sy2,sxy))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{acc=0; r[i]=r[i-1]}\n",
      "\n",
      "accr=accr+acc\n",
      "\n",
      "if(i%%100==0){print(c(i,accr/i,accx/i,accy/i,\n",
      "\n",
      "mx[i],my[i],sx[i],sy[i],r[i]),digits=4)}\n",
      "\n",
      "This program is intimidatingly long. However, if we consider each section\n",
      "of the program separately, it is really fairly simple. For the sake of brevity, I\n",
      "will not discuss the entire program line-by-line; instead, I will briefly describe\n",
      "the separate sections.\n",
      "\n",
      "At the very beginning of the program, I define the various variables to be\n",
      "used in the program, and I create a function (lnpost) that is used in the MH\n",
      "steps for updating the variance and correlation parameters. This function is\n",
      "the log posterior density, which is evaluated at established values of the data\n",
      "and parameters. Once again, I use the log posterior density, because evaluating\n",
      "the density itself (in this form) produces computational underflow issues.\n",
      "\n",
      "The next two sections simply sample the mean parameters from appropri-\n",
      "ate normal distributions derived in the previous section, conditional on the\n",
      "current values of all other parameters in the model. The next, brief section\n",
      "updates the sums of squares (sums of the square deviations of the data from\n",
      "the current values of the mean parameters). The next three sections update\n",
      "the variances and correlation, given the current values for the means, sums\n",
      "of squares, and other remaining parameters. These sections are remarkably\n",
      "similar, except that (1) the candidate generation step changes, and (2) the\n",
      "lnpost function is fed different variables. Notice that I have created separate\n",
      "acceptance rate variables—one for each variance and correlation—and that I\n",
      "use a logged value of a uniform random number to compare with the ratio of\n",
      "the logged posterior densities.\n",
      "\n",
      "Figure 5.6 shows a trace plot and histogram of the samples from the\n",
      "marginal posterior distribution of the correlation parameter—the key param-\n",
      "eter of interest in the model. As the figure shows, the algorithm rapidly con-\n",
      "verged to the .45 region (the Pearson correlation). Overall, the posterior means\n",
      "for all parameters were very close to their sample counterparts: µ̄x = x̄ = 2.19;\n",
      "µ̄y = ȳ = 2.02; σ̄2x = s\n",
      "\n",
      "2\n",
      "x = 1.43; σ̄2y = s\n",
      "\n",
      "2\n",
      "y = 1.14; and ρ̄ = r = .450. Finally,\n",
      "\n",
      "we can construct a new interval estimate for ρ that reflects our uncertainty\n",
      "in all the parameters. This interval is [.406 , .491], which is about 10% larger\n",
      "than the estimate we obtained when we assumed the other parameters were\n",
      "fixed at their sample values, and this estimate more closely matches the one\n",
      "obtained using the z transformation of the Pearson correlation.\n",
      "\n",
      "\n",
      "\n",
      "126 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "0 2000 4000 6000 8000 10000\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "Iteration\n",
      "\n",
      "ρ\n",
      "\n",
      "Fig. 5.6. Trace plot and histogram of ρ from bivariate normal model for the 2000\n",
      "GSS free speech and political participation data.\n",
      "\n",
      "5.4.4 A matrix approach to the bivariate normal distribution\n",
      "problem\n",
      "\n",
      "As we discussed, the bivariate normal distribution is simply the multivariate\n",
      "normal distribution with dimensions equal to two. Although I presented a\n",
      "scalar version of the density function, the matrix representation shown in\n",
      "Chapter 2 can also be used to construct the posterior density. Under that\n",
      "approach, given the same prior as used above, the posterior density is:\n",
      "\n",
      "p(µ, Σ|X, Y ) ∝\n",
      "\n",
      "1\n",
      "\n",
      "|Σ|3/2\n",
      "nY\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "\n",
      "|Σ|1/2\n",
      "exp\n",
      "\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "[(xi − µx) (yi − µy)]Σ−1[(xi − µx) (yi − µx)]T\n",
      "\n",
      "ff\n",
      "More succinctly:\n",
      "\n",
      "p(µ, Σ|X, Y ) ∝\n",
      "1\n",
      "\n",
      "|Σ|(n+3)/2\n",
      "exp\n",
      "\n",
      "\n",
      "−\n",
      "\n",
      "1\n",
      "\n",
      "2\n",
      "tr(SΣ\n",
      "\n",
      "−1\n",
      ")\n",
      "\n",
      "ff\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "5.4 Extending the bivariate normal example: The full multiparameter model 127\n",
      "\n",
      "where S is the sums of squares matrix [(X−µx) (Y −µy)]T [(X−µx) (Y −µy)].\n",
      "Note that X and Y are n× 1 column vectors, and so S is 2× 2. The diagonal\n",
      "elements of this matrix are the numerators for the variance; the off-diagonal\n",
      "element is the numerator of the covariance between x and y.\n",
      "\n",
      "In this representation of the posterior density, it is clear that, conditional\n",
      "on values of µx, µy, and the data, the covariance matrix Σ follows an inverse\n",
      "Wishart distribution with n degrees of freedom and scale matrix S.\n",
      "\n",
      "What are the conditional distributions for the mean parameters? The re-\n",
      "sult we obtained in the previous chapter for the univariate normal distri-\n",
      "bution problem is extendable. In the univariate case, the posterior density\n",
      "for µ is N(x̄, σ2/n). In the multivariate case, the posterior distribution is\n",
      "N(X̄, (1/n)Σ), where X̄ is a vector of sample means.\n",
      "\n",
      "A Gibbs sampler can be established that first samples from the conditional\n",
      "distribution for the mean vector and that second samples from the condtional\n",
      "distribution for the covariance matrix. Below is the R program that performs\n",
      "such sampling. Notice that this program is considerably shorter than the pre-\n",
      "vious one; there are several reasons for this, including the use of R’s matrix\n",
      "operators, the use of built-in random sampling functions, and the simultane-\n",
      "ous updating of vectors/matrices of parameters.\n",
      "\n",
      "Once the sample loop begins, the mean parameters (m[]) are sampled\n",
      "from a multivariate normal distribution with a mean vector equal to the sam-\n",
      "ple mean vector and a covariance matrix equal to the current value of the\n",
      "covariance matrix Σ divided by the sample size.6\n",
      "\n",
      "Next, given a value for the means, the sums of squares matrix is con-\n",
      "structed, and a value for Σ is generated from the inverse Wishart distribution.\n",
      "\n",
      "#R program for matrix approach to sampling BVN parameters\n",
      "\n",
      "#matrix d is the data, already stored in memory\n",
      "\n",
      "m=matrix(0,10000,2)\n",
      "\n",
      "s=array(1,dim=c(10000,2,2)); s[,1,2]=s[,2,1]=0\n",
      "\n",
      "sc=matrix(0,2,2)\n",
      "\n",
      "corr=matrix(0,10000)\n",
      "\n",
      "e=d\n",
      "\n",
      "mn=matrix(c(mean(d[,1]),mean(d[,2])),2)\n",
      "\n",
      "for(i in 2:10000)\n",
      "\n",
      "{\n",
      "\n",
      "#simulate m\n",
      "\n",
      "u=rnorm(2,mean=0,sd=1)\n",
      "\n",
      "6 Sampling from a multivariate normal distribution requires the use of a Cholesky\n",
      "decomposition, which is essentially a square root of the matrix. In R, the function\n",
      "chol returns an upper triangular matrix, which, when multiplied by itself trans-\n",
      "posed, equals the original matrix. To obtain draws from a multivariate normal\n",
      "distribution, we generate a k×1 column vector of independent N(0, 1) draws and\n",
      "then multiply this vector by the Cholesky factor.\n",
      "\n",
      "\n",
      "\n",
      "128 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "m[i,]=t(mn) + t(u)%*%chol(s[i-1,,]/1377)\n",
      "\n",
      "#simulate s\n",
      "\n",
      "e[,1]=d[,1]-m[i,1]; e[,2]=d[,2]-m[i,2]\n",
      "\n",
      "sc=t(e)%*%(e)\n",
      "\n",
      "s[i,,]=riwish(1377,sc)\n",
      "\n",
      "corr[i]=s[i,1,2]/sqrt(s[i,1,1]*s[i,2,2])\n",
      "\n",
      "if(i%%100==0){print(c(i,corr[i]))}\n",
      "\n",
      "}\n",
      "\n",
      "The covariance matrix Σ contains the covariance between x and y but not\n",
      "the correlation. Yet we are really interested in the correlation; the covariance\n",
      "is scale-dependent, and so it tells us little about the extent of the relationship\n",
      "between the two survey items. Obtaining a distribution for the correlation,\n",
      "however, is not a problem, given that it is a simple transformation of sampled\n",
      "model parameters: We know that ρ = σxy/σxσy. So, the next-to-last line in the\n",
      "program computes the correlation at every iteration, which yields a posterior\n",
      "distribution for the correlation. The posterior mean for the correlation using\n",
      "this algorithm was .450, and a 95% probability interval for the correlation was\n",
      "[.408, .491]. These results were quite consistent with those obtained under the\n",
      "univariate approach.\n",
      "\n",
      "This computation highlights an important feature of Bayesian statistics\n",
      "using MCMC methods: the ability to obtain distributions for quantities that\n",
      "are not directly estimated within a model but are functions of model param-\n",
      "eters. We will discuss this process of inference for unestimated parameters in\n",
      "greater depth in the examples in subsequent chapters.\n",
      "\n",
      "5.5 Conclusions\n",
      "\n",
      "In this chapter, we discussed the MH algorithm as an alternative approach\n",
      "to Gibbs sampling when Gibbs sampling cannot be easily employed. Some of\n",
      "the examples showed full MH algorithms, while others involved combinations\n",
      "of MH and Gibbs sampling steps. In general, when conditional distributions\n",
      "can be derived, and samples can be directly simulated from them, the Gibbs\n",
      "sampler is faster and more efficient than the MH algorithm. The primary\n",
      "trade-off between the two sampling approaches is that Gibbs sampling involves\n",
      "considerably more mathematical overhead work in deriving the conditional\n",
      "distributions, whereas the MH algorithm involves lengthier programming steps\n",
      "but only requires specification of the unnormalized joint posterior distribution\n",
      "for the parameters.\n",
      "\n",
      "A secondary trade-off, however, is that the MH algorithm can be difficult\n",
      "to implement because it is sensitive to specification of the proposal densities\n",
      "\n",
      "\n",
      "\n",
      "5.6 Exercises 129\n",
      "\n",
      "from which candidate parameters are simulated. The consequence is that mon-\n",
      "itoring the performance of an MH algorithm—and making adjustments—is, in\n",
      "many ways, more important than monitoring the performance of a Gibbs sam-\n",
      "pling routine. To be sure, both types of algorithms require careful monitoring\n",
      "to ensure that they are “converging” and “mixing” appropriately. The next\n",
      "chapter discusses how to conduct such monitoring and then describes some\n",
      "approaches to evaluating overall model fit before constructing summaries of\n",
      "individual model parameters.\n",
      "\n",
      "5.6 Exercises\n",
      "\n",
      "1. In the final example, I introduced a possibly unfamiliar matrix algebra\n",
      "identity in showing the posterior density for Σ. Let A be a k × 1 column\n",
      "vector, and let B be a k × k matrix. Show that AT BA = tr(AB) when\n",
      "k = 3.\n",
      "\n",
      "2. Derive the relationship shown between m and b in the linear density as\n",
      "shown in Section 5.2, and show how m can also be represented as a func-\n",
      "tion of b (the opposite approach to that shown in the example). Run the\n",
      "algorithm as it is presented in the text, and then set up the MH algorithm\n",
      "to sample from the posterior for b, rather than m. Compare the results\n",
      "and discuss.\n",
      "\n",
      "3. Rerun the MH algorithm for the linear density in Section 5.2, but first\n",
      "attempt to allow m and b to both be updated independently. That is, ignore\n",
      "the constraint that b is completely determined by m. What happens?\n",
      "Next, continue to allow both parameters to be updated, but change the\n",
      "posterior density by adopting a proper prior distribution. Try independent\n",
      "beta distributions for both parameters, and then try a bivariate normal\n",
      "distribution with correlation ρ = .5. What happens? Discuss.\n",
      "\n",
      "4. Return to the final bivariate normal distribution example. Instead of de-\n",
      "riving the conditional distributions for some of the parameters, develop\n",
      "an MH algorithm to sample all the parameters.\n",
      "\n",
      "5. Develop an MH algorithm to sample the parameters mx, my, and b from\n",
      "the planar density using the data from Table 2.1. Note that a constraint\n",
      "similar to the one imposed in the linear density example needs to be en-\n",
      "forced. That is, once two parameters are determined, the third parameter\n",
      "is fixed.\n",
      "\n",
      "6. The last bivariate normal algorithm presented (the Gibbs sampler) is the\n",
      "multivariate analog to the univariate Gibbs sampler for sampling the mean\n",
      "and variance parameters from a univariate normal distribution. In Chap-\n",
      "ter 4, I also presented a Gibbs sampler using the marginal distribution\n",
      "(not the conditional distribution) for σ2. In that algorithm, we generated\n",
      "a sequence of draws for σ2, and then we simulated values for µ condi-\n",
      "tional on these samples for σ2. A similar process can be performed in the\n",
      "multivariate case; the only changes include (1) that the scale matrix S\n",
      "\n",
      "\n",
      "\n",
      "130 5 Modern Model Estimation Part 2: Metroplis–Hastings Sampling\n",
      "\n",
      "is constructed once, using the sample means rather than the parameters\n",
      "µx and µy, and (2) the degrees of freedom for the inverse Wishart distri-\n",
      "bution are one fewer. Construct this Gibbs sampler and compare results\n",
      "with those obtained using the algorithm presented in the chapter.\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "\n",
      "Evaluating Markov Chain Monte Carlo\n",
      "(MCMC) Algorithms and Model Fit\n",
      "\n",
      "In the previous two chapters, we used Gibbs sampling and Metropolis-Hastings\n",
      "(MH) sampling to make inference for parameters. Making inference, however,\n",
      "should come after (1) we have determined that the algorithm worked cor-\n",
      "rectly, and (2) we have decided that the model we chose is acceptable for our\n",
      "purposes. These two issues are the focus of this chapter.\n",
      "\n",
      "The first part of this chapter addresses the first concern in discussing the\n",
      "convergence and mixing of MCMC algorithms. This part should not be con-\n",
      "sidered an exhaustive exposition of the topic; as I stated, many of the recent\n",
      "advances in MCMC methods have been in this area. However, the approaches\n",
      "I present to evaluating algorithm performance are the most common ones\n",
      "used (see Liu 2001 and Robert and Casella 1999). In the previous chapters,\n",
      "I showed the basics of MCMC implementation, but I left these technical is-\n",
      "sues unaddressed. However, because software development is largely left to\n",
      "the researcher estimating Bayesian models, assessing how well an MCMC al-\n",
      "gorithm performs is crucial to conducting a responsible Bayesian analysis and\n",
      "to making appropriate inferences.\n",
      "\n",
      "The second part of the chapter discusses three approaches to evaluating the\n",
      "fit of models and to selecting a model as “best.” Specifically, I discuss posterior\n",
      "predictive distributions, Bayes factors, and Bayesian model averaging. I devote\n",
      "relatively little attention to the latter two methods. Bayes factors require the\n",
      "computation of the marginal likelihood of the data (the denominator of Bayes’\n",
      "full formula for probability distributions), which is a complex integral that\n",
      "is not a by-broduct of MCMC estimation and is generally quite difficult to\n",
      "compute. Hence, additional methods are needed to compute it, and such is\n",
      "beyond the scope of this book (see Chen, Shao, and Ibrahim 2000). Bayesian\n",
      "model averaging (BMA) avoids the need for selecting models essentially by\n",
      "combining the results of multiple models into a single model. BMA therefore\n",
      "may not be used often in a social science setting in which we are generally\n",
      "interested in testing a single, specific model to evaluate a hypothesis.\n",
      "\n",
      "\n",
      "\n",
      "132 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "6.1 Why evaluate MCMC algorithm performance?\n",
      "\n",
      "In classical statistics, the estimation routines that are used to produce max-\n",
      "imum likelihood or other (e.g., least squares) estimates have already been\n",
      "developed, debugged, tested, and retested, and they are largely out of view\n",
      "of the researcher who uses them. For example, if a researcher is using SAS\n",
      "or STATA to estimate an ordinary least squares (OLS) regression model, the\n",
      "researcher does not need to know how to compute (XT X)−1(XT Y ) to use the\n",
      "software and obtain the OLS estimates. In contrast, in a Bayesian analysis\n",
      "involving MCMC methods, the researcher does need to know this informa-\n",
      "tion. Thus, the researcher must be attuned to programming errors and other\n",
      "issues that are involved in constructing estimation routines. The trade-off for\n",
      "this extra work is that there is much more flexibility in model development,\n",
      "inference, and evaluation of model fit than a classical analysis generally can\n",
      "offer without the researcher investing considerable programming effort.\n",
      "\n",
      "Aside from basic programming mistakes that can render an MCMC al-\n",
      "gorithm useless, there are two primary concerns with the implementation of\n",
      "any MCMC algorithm: convergence and mixing. We must make sure that the\n",
      "algorithm produces a Markov chain that “converges” to the appropriate den-\n",
      "sity (the posterior density) and that “mixes” well throughout the support of\n",
      "the density. Unlike routines used to find maximum likelihood estimates, which\n",
      "converge to a point, MCMC algorithms must converge to, and sample thor-\n",
      "oughly from, a density. Thus, even if an MCMC algorithm converges to the\n",
      "appropriate region, we must also be sure that the algorithm “moves around”\n",
      "throughout the density once it has converged and samples from all areas of\n",
      "the density as it should before the run of the algorithm ends.\n",
      "\n",
      "6.2 Some common problems and solutions\n",
      "\n",
      "Convergence and mixing may be affected by a number of factors, including\n",
      "especially the following:\n",
      "\n",
      "• The starting values for the parameters.\n",
      "• The shape of the posterior distribution.\n",
      "• The choice of proposal density in an MH algorithm.\n",
      "\n",
      "As with maximum likelihood estimation algorithms, starting values may\n",
      "make a difference in the performance of an MCMC algorithm. Although\n",
      "MCMC theory shows that an MCMC algorithm will, in the limit, converge\n",
      "on the posterior distribution of interest, there is no guarantee that it will in\n",
      "any run of finite length (see Tierney 1996 for discussion of theory relevant to\n",
      "MCMC methods; see Bremaud 1999 for a broader theoretical exposition). For\n",
      "example, if it takes 10,000 iterations for an MCMC algorithm to converge, but\n",
      "the algorithm is only run for 1,000 iterations, the algorithm certainly will not\n",
      "have converged. Furthermore, the algorithm obviously will not have mixed\n",
      "\n",
      "\n",
      "\n",
      "6.2 Some common problems and solutions 133\n",
      "\n",
      "well, either. In brief, if the starting values are particularly poor (e.g., far from\n",
      "the center of the target distribution), the run may end before convergence is\n",
      "even obtained, let alone before the algorithm has thoroughly sampled from\n",
      "the target distribution. In some cases, particularly poor starting values may\n",
      "produce an algorithm that never even “gets off the ground.” For example,\n",
      "as I mentioned in the last chapter, the second Gibbs sampler for the planar\n",
      "density required reasonable starting values in order for the algorithm to begin\n",
      "sampling; without a good starting value, the algorithm could not simulate a\n",
      "single legitimate value for x.\n",
      "\n",
      "An obvious solution to the problem of having poor starting values is sim-\n",
      "ply to find better ones! This may be easier said than done; however, for most\n",
      "models common in the social sciences, we can generally use maximum likeli-\n",
      "hood estimates from similar models as our starting values. For example, in the\n",
      "multivariate probit models discussed later in the book, we could use maximum\n",
      "likelihood estimates from univariate probit models as our starting values. A\n",
      "second solution to the problem may be to run the algorithm for more itera-\n",
      "tions. In the limit, the algorithm should converge on the target distribution.\n",
      "\n",
      "The shape of the posterior distribution may also affect convergence and\n",
      "mixing. If the posterior is multimodal, for example, an MCMC algorithm may\n",
      "converge rapidly on one mode, but it may not mix well across modes. There are\n",
      "a number of solutions to this problem (some more complicated than others); a\n",
      "simple one might be to expand the width/variance of the proposal density so\n",
      "that it is possible to jump from one mode to another. A second solution may be\n",
      "to find a better model, that is, to incorporate better predictors (assuming the\n",
      "model is a regression model). One cause of multimodality may be the omission\n",
      "of an important variable (like gender). Theoretically, posterior distributions\n",
      "tend to be asymptotically normal, and with the models discussed in this book,\n",
      "multimodality may seldom be a problem if most or all relevant variables are\n",
      "included.\n",
      "\n",
      "Another feature of a posterior distribution that may slow convergence and\n",
      "mixing is strong posterior correlation of the model parameters. In a simple\n",
      "regression model, for example, the intercept and slope parameters will often\n",
      "be highly negatively correlated. Strong correlations between parameters may\n",
      "cause slow convergence, because it may be difficult—especially when the pro-\n",
      "posal densities are broad—for the algorithm to move from its starting values.\n",
      "Similarly, if the starting values are very good (e.g., at the maximum likelihood\n",
      "estimates), the algorithm may not mix well, because it may be difficult for the\n",
      "algorithm to move away from them. This problem is often easy to diagnose,\n",
      "because it will yield a very low acceptance rate (see below), but it may not\n",
      "be very easy to remedy. There are essentially three solutions to the problem\n",
      "of highly correlated parameters: transforming the data, reparameterizing the\n",
      "model, and modifying the proposal densities.\n",
      "\n",
      "In the simple regression model example mentioned above, an easy solu-\n",
      "tion is to transform the data by centering the variables, where centering is\n",
      "simply the process of subtracting the mean of each variable from all of the\n",
      "\n",
      "\n",
      "\n",
      "134 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "observations on that variable. Centering often works to reduce posterior cor-\n",
      "relation of the parameters in regression models as well as hierarchical/growth\n",
      "models. However, in a regression model, centering will only reduce the corre-\n",
      "lation between the intercept and the slope parameters (and not between slope\n",
      "parameters).\n",
      "\n",
      "The latter two solutions to the problem of strong posterior correlations\n",
      "between parameters are essentially flip sides of the same coin. Reparameter-\n",
      "ization is the process of transforming the model parameters so that they are\n",
      "uncorrelated. For example, Gilks and Roberts (1996) show that transforming\n",
      "two highly correlated parameters X1 and X2 by constructing new variables\n",
      "Y1 = X1 +X2 and Y2 = 3(X1X2), simulating from the distribution for Y1 and\n",
      "Y2, and then transforming the simulated samples back to get samples from\n",
      "the distributions for the original variables, leads to more rapid convergence\n",
      "and mixing. Alternative reparameterizations are possible, of course, so long\n",
      "as the new parameters are uncorrelated or weakly correlated.\n",
      "\n",
      "Reparameterization may be quite difficult. First, if we are estimating a\n",
      "regression model with a large number of predictors, transforming all the re-\n",
      "gression coefficients may be extremely tedious, and finding a transformation\n",
      "that reduces or eliminates strong posterior correlations may be a hit-or-miss\n",
      "proposition. Second, when we transform a distribution from one parameter-\n",
      "ization to another, we must include the Jacobian of the transformation in\n",
      "the new distribution, where the Jacobian is essentially a scalar or matrix that\n",
      "represents how the variables in one space relate to one another relative to how\n",
      "the variables in the original space related to one another. The Jacobian in a\n",
      "high-dimensional transformation will be a matrix, and its derivation may be\n",
      "complex, especially when compared with simply changing the proposal den-\n",
      "sities (see DeGroot 1986 and/or a calculus text discussing transformations of\n",
      "multivariable equations).\n",
      "\n",
      "The final solution I discuss—modifying the proposal densities—is much\n",
      "easier than reparameterizing the model. One reason that a model may not\n",
      "converge rapidly or mix well when there are strong posterior correlations be-\n",
      "tween the parameters is that the proposal densities do not closely match the\n",
      "shape of the posterior density, and so the algorithm produces a large number\n",
      "of rejected candidates. Finding a multivariate proposal density with correla-\n",
      "tions that match those in the set of parameters that are highly correlated will\n",
      "generally lead to more rapid convergence and better mixing, because having\n",
      "a similar shape for the proposal and posterior allows the proposals to have\n",
      "greater density where they should: at regions where the posterior is more\n",
      "dense. Ultimately, altering the proposal densities is an equivalent strategy\n",
      "to reparameterization: Under reparameterization, the proposals remain con-\n",
      "stant, while the posterior is modified to more closely match the shape of the\n",
      "proposals; under proposal modification, the posterior remains constant, while\n",
      "the proposals are changed to more closely match the shape of the posterior.\n",
      "I personally prefer the latter approach, because it is simpler and does not re-\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 135\n",
      "\n",
      "quire additional steps at the end to transform the samples back to the original\n",
      "parameterization.\n",
      "\n",
      "Finally, the choice of proposal density may affect convergence and mixing\n",
      "even when strong posterior correlations are not present. The general rule is\n",
      "that, the less similar the proposal density is to the posterior density, the worse\n",
      "the convergence and mixing problems. In my own experience, using random\n",
      "walk metropolis algorithms (often with some Gibbs sampling steps) to esti-\n",
      "mate basic models that are commonly used in the social sciences, changing the\n",
      "proposal density is usually not necessary when strong posterior correlations\n",
      "between parameters are not evident. When it is necessary to change the pro-\n",
      "posal, it is usually not because of the fundamental shape of the proposal but\n",
      "rather the scale, in terms of its width or variance. This problem—having too\n",
      "narrow or too wide of a proposal density—is usually quite easy to diagnose,\n",
      "because the acceptance rate of the algorithm will either be too small or too\n",
      "large.\n",
      "\n",
      "6.3 Recognizing poor performance\n",
      "\n",
      "In the previous section, I described some common problems that produce non-\n",
      "convergence and poor mixing and discussed some relatively simple approaches\n",
      "to improving performance. But, how do we diagnose when convergence and\n",
      "mixing problems are present?\n",
      "\n",
      "Since the original development of MCMC methods, a number of methods\n",
      "have been proposed to evaluate the convergence (and mixing) of MCMC algo-\n",
      "rithms. I will discuss several that appear to be most useful. It is important to\n",
      "note at the outset, however, that there is no definitive way of assessing conver-\n",
      "gence and mixing for problems that involve analytically intractable densities,\n",
      "and thus, a combination of methods should be employed to satisfy a researcher\n",
      "that convergence has been obtained. As with any statistical enterprise, MCMC\n",
      "estimation is to a large extent an art that is helped with experience.\n",
      "\n",
      "6.3.1 Trace plots\n",
      "\n",
      "The first, and probably most common, method of assessing convergence and\n",
      "mixing is the use of the trace plot. Trace plots were introduced earlier, and\n",
      "as discussed, they are simply plots of the sampled values from an algorithm\n",
      "at each iteration, with the x axis referencing the iteration of the algorithm\n",
      "and the y axis referencing the sampled value (parameter or data point). With\n",
      "a trace plot, a lack of convergence is evidenced by trending in the sampled\n",
      "values such that the algorithm never levels-off to a stable, stationary state.\n",
      "\n",
      "As an example, I constructed an MH algorithm for sampling the slope\n",
      "parameters m1 and m2 and the intercept parameter b from a planar den-\n",
      "sity applied to the free speech and political participation items described in\n",
      "Chapter 2. Figure 6.1 shows a trace plot of the first 1000 samples for the m2\n",
      "\n",
      "\n",
      "\n",
      "136 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "parameter (the slope parameter for the free speech item). The algorithm does\n",
      "not appear to have converged prior to the 400th iteration; instead, clear down-\n",
      "ward trending is present from the starting value of 0 down to approximately\n",
      "−.012. The algorithm may have converged somewhere around the 400th iter-\n",
      "ation (reference the dotted vertical line). Notice that the sampled values for\n",
      "the m2 parameter do not evidence any clear, general trending from iteration\n",
      "400 or so through iteration 1,000.\n",
      "\n",
      "0 200 400 600 800 1000\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "2\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "8\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "2\n",
      "\n",
      "Possible convergence here\n",
      "\n",
      "Certainly not converged yet\n",
      "\n",
      "Fig. 6.1. Trace plot for first 1,000 iterations of an MH algorithm sampling param-\n",
      "eters from planar density for GSS free speech and political participation data.\n",
      "\n",
      "I say that the algorithm may have converged, because we do not have\n",
      "enough evidence to conclude that it has. Figure 6.2 is a trace plot of the first\n",
      "4,000 iterations, with vertical lines referencing iterations 400-1,000 shown in\n",
      "the previous figure. From this view, especially given the upward trending in\n",
      "the last few hundred iterations, it is unclear whether the algorithm, in fact,\n",
      "had converged by iteration 400.\n",
      "\n",
      "The figure also demonstrates that, even if the algorithm had converged\n",
      "by iteration 400 or so, it certainly had not thoroughly mixed through the\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 137\n",
      "\n",
      "0 1000 2000 3000 4000\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "2\n",
      "\n",
      "Fig. 6.2. Trace plot for first 4,000 iterations of an MH algorithm sampling param-\n",
      "eters from planar density for GSS free speech and political participation data.\n",
      "\n",
      "1000th iteration. That is, assume that the algorithm had converged. The 600\n",
      "sampled values from iteration 401 to 1,000 are restricted to a fairly narrow\n",
      "range, compared with the range of values sampled over the additional 3,000\n",
      "iterations. Specifically, the ratio of the ranges for the sampled values from\n",
      "iterations 401 to 4,000 to the sampled values from iterations 401 to 1,000\n",
      "was 2.48. Similarly, the ratio of the standard deviations of the sampled values\n",
      "for iterations 401 to 4,000 versus 401 to 1,000 was 2.00. In other words, the\n",
      "density implied by the values for m2 sampled from iterations 401 to 4000 is\n",
      "more than twice as broad as the density implied by the values for m2 sampled\n",
      "from iterations 401 to 1,000. This result means that the algorithm certainly\n",
      "has not mixed thoroughly prior to iteration 1,000.\n",
      "\n",
      "On the other hand, the means of the two samples differ by only about\n",
      "1%, which suggests that perhaps the algorithm had at least converged to the\n",
      "appropriate region (and perhaps in distribution) by iteration 400. Figure 6.3\n",
      "shows a trace plot of the algorithm across 50,000 iterations, with horizontal\n",
      "reference lines at the means for each of the three samples. The lines are quite\n",
      "\n",
      "\n",
      "\n",
      "138 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "close together, differing by only about 1%, which suggests the algorithm prob-\n",
      "ably did converge early. Indeed, the means are so close that the mean for the\n",
      "entire sample of 50,000 iterations cannot be seen—it is identical to the mean\n",
      "of the first 600 iterations after the 400th. However, the standard deviations\n",
      "for the sample consisting of iterations 401-4,000 and the sample consisting of\n",
      "iterations 401-50,000 differ by about 28%, which that the algorithm had not\n",
      "mixed sufficiently over the first few thousand iterations.\n",
      "\n",
      "0 1000 2000 3000 4000 5000\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "4\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "2\n",
      "\n",
      "Fig. 6.3. Trace plot for all 50,000 iterations of an MH algorithm sampling param-\n",
      "eters from planar density for GSS free speech and political participation data with\n",
      "means superimposed.\n",
      "\n",
      "Trace plots need not be limited to examining a parameter itself, but rather,\n",
      "we may choose to monitor a sample statistic like the mean of a parameter (see\n",
      "also Robert and Casella 1999). Whereas the parameter itself should converge\n",
      "to a flat region and then wander around that region, a statistic like the mean\n",
      "of a parameter should, in the limit, converge to a flat line. If the starting\n",
      "values for a parameter are poor, it may take some time for the mean of a pa-\n",
      "rameter to overcome the distortion/bias caused by the poorly sampled early\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 139\n",
      "\n",
      "values, but in the long run, the mean will eventually stabilize, provided the\n",
      "algorithm in fact has converged. Instead of examining a trace plot of the mean\n",
      "of the distribution, we may wish to consider a trace plot of means computed\n",
      "from different parts of the sample. For example, we could compute the mean\n",
      "for every “batch” of 1,000 iterations and plot them to determine whether\n",
      "these batch means evidence any trending. Figure 6.4 is a plot of both of these\n",
      "approaches. The solid line shows the mean of all sampled values up to the iter-\n",
      "ation shown on the x axis. The asterisks are the means of 1,000-item batches.\n",
      "The figure shows that the cumulative mean appeared to have converged by\n",
      "iteration 10,000, whereas the batch means suggest earlier convergence.\n",
      "\n",
      "0 10000 20000 30000 40000 50000\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "Iteration or Block\n",
      "\n",
      "C\n",
      "u\n",
      "\n",
      "m\n",
      "u\n",
      "\n",
      "la\n",
      "tiv\n",
      "\n",
      "e\n",
      " (\n",
      "\n",
      "_\n",
      "_\n",
      "\n",
      "_\n",
      ") \n",
      "\n",
      "o\n",
      "r \n",
      "\n",
      "B\n",
      "a\n",
      "\n",
      "tc\n",
      "h\n",
      "\n",
      " (\n",
      "*)\n",
      "\n",
      " M\n",
      "e\n",
      "\n",
      "a\n",
      "n\n",
      "\n",
      "Fig. 6.4. Trace plot of cumulative and batch means for MH algorithm sampling\n",
      "parameters from planar density for GSS free speech and political participation data.\n",
      "\n",
      "Figure 6.5 shows a plot of the cumulative standard deviation for iterations\n",
      "after the 10, 000th (after the cumulative mean had leveled off). The figure\n",
      "shows that the standard deviation levels off around iteration 25,000 (see ver-\n",
      "tical reference line).\n",
      "\n",
      "\n",
      "\n",
      "140 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "10000 20000 30000 40000 50000\n",
      "\n",
      "0\n",
      "  \n",
      " e\n",
      "\n",
      "+\n",
      "0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "4\n",
      "4\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "4\n",
      "8\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "4\n",
      "1\n",
      "\n",
      "  \n",
      " e\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      "3\n",
      "\n",
      "Iteration\n",
      "\n",
      "C\n",
      "u\n",
      "\n",
      "m\n",
      "u\n",
      "\n",
      "la\n",
      "tiv\n",
      "\n",
      "e\n",
      " σ\n",
      "\n",
      "m\n",
      "2\n",
      " (\n",
      "\n",
      "It\n",
      "e\n",
      "\n",
      "ra\n",
      "tio\n",
      "\n",
      "n\n",
      "s \n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      ",0\n",
      "0\n",
      "\n",
      "0\n",
      "+\n",
      "\n",
      ")\n",
      "\n",
      "Fig. 6.5. Trace plot of cumulative standard deviation of m2 from MH algorithm\n",
      "sampling parameters from planar density for GSS free speech and political partici-\n",
      "pation data.\n",
      "\n",
      "Taken together, these results suggest that we should probably discard the\n",
      "first 25,000 iterations of the algorithm as the “burn-in” period prior to conver-\n",
      "gence and base inference on the last 25,000 samples. Before we do so, however,\n",
      "we should also examine similar plots for the other parameters in the model.\n",
      "\n",
      "Visual inspection of trace plots, although perhaps the most common way\n",
      "of assessing convergence and mixing, are notoriously problematic. For ex-\n",
      "ample, if we simply change the scale of the trace plot, the appearance of\n",
      "convergence and good mixing may be affected. One way the scale may be\n",
      "changed—assuming some default method for determining the range of the y\n",
      "axis is used—is if the starting values for the algorithm were extremely poor. In\n",
      "such a case, the algorithm may have appeared to converge because there may\n",
      "be a long, clearly observable trend up to a point, followed by a leveling-off,\n",
      "but the plot’s scale may be so small that we cannot see whether there is still\n",
      "some shallower trending occurring. We may also be unable to see whether the\n",
      "algorithm is moving around rapidly beyond the leveling-off point, especially\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 141\n",
      "\n",
      "if we have run the algorithm for a large number of iterations and the scale in\n",
      "the x dimension is compressed.\n",
      "\n",
      "More problematic than simple issues of scale, especially with multimodal\n",
      "posterior densities, is that the trace plot may suggest convergence, when in\n",
      "fact only one mode of the posterior has been explored. Furthermore, even\n",
      "if the posterior is unimodal, if the algorithm does not mix rapidly, it may\n",
      "have appeared to converge when in fact it has only converged in one part\n",
      "of the density (possibly a tail). It is important to note that this problem is\n",
      "not unique to the use of trace plots: There is no measure that can tell us\n",
      "definitively whether an algorithm has mixed thoroughly.\n",
      "\n",
      "Nonetheless, even if satisfactory trace plots should not be the last criterion\n",
      "for declaring convergence and good mixing, they should probably be the first.\n",
      "A trace plot generally provides an immediate means for recognizing that an\n",
      "algorithm has not converged—or is not converging—and/or is not mixing well,\n",
      "and thus, it can help us decide to stop an algorithm, make a modification,\n",
      "and restart it much sooner than using other methods may.\n",
      "\n",
      "6.3.2 Acceptance rates of MH algorithms\n",
      "\n",
      "Beyond visual inspection of trace plots, several numerical methods and tests\n",
      "may be used in assessing MCMC performance. First, researchers using MCMC\n",
      "methods other than Gibbs sampling typically monitor the acceptance rate of\n",
      "the algorithm. In a Gibbs sampler, the acceptance rate is 1 and, hence, is\n",
      "noninformative. However, in an MH algorithm, rejection of candidates is pos-\n",
      "sible, and hence, the rejection/acceptance rate should be monitored. Needless\n",
      "to say, an algorithm that never accepts a candidate cannot converge, nor will\n",
      "it mix well. At the other extreme, an algorithm that accepts every candidate\n",
      "is not necessarily performing any better. In fact, having an extremely high\n",
      "acceptance rate is a good indication that the algorithm is moving too slowly\n",
      "toward convergence, and, if it has converged, it is mixing very slowly. An\n",
      "acceptance rate of around 50% or slightly lower is ideal (see Johnson and Al-\n",
      "bert 1999). More generally, a rate somewhere between 25% and 75% is often\n",
      "acceptable.\n",
      "\n",
      "Two factors determine the acceptance rate of an MH algorithm: (1) the\n",
      "size of the “jumps” from the sampled value at iteration j and the candidate\n",
      "value, and (2) the shape of the proposal density relative to that of the pos-\n",
      "terior density. The jump size is determined by the variance or width of the\n",
      "proposal density. Consider, for example, the posterior density for the param-\n",
      "eter m2 from the planar density obtained from the MH algorithm discussed\n",
      "in the previous section. The proposal density I used in the MH algorithm for\n",
      "this parameter (as well as the m1 parameter) was a U(−.0003, .0003) density.\n",
      "Overall, this proposal density (along with that for m1) led to an acceptance\n",
      "rate of 37.5%. However, what if I had used a U(−.003, .003) proposal den-\n",
      "sity? Figure 6.6 shows the posterior density for m2 (obtained from the last\n",
      "25,000 iterations of the MH algorithm), as well as the two proposal densities.\n",
      "\n",
      "\n",
      "\n",
      "142 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "The U(−.0003, .0003) proposal that was used is quite narrow relative to the\n",
      "posterior density. The result, as we saw in the previous section, is that the\n",
      "algorithm takes very small steps in this dimension—it takes a large number of\n",
      "iterations before the algorithm mixes thoroughly. The U(−.003, .003) proposal\n",
      "is much wider than the proposal density that was used, which means it may\n",
      "move from one end of the posterior density more quickly. However, as the fig-\n",
      "ure shows, this proposal is substantially wider than the posterior for m2, and\n",
      "thus, many candidates are likely to be rejected, especially when the current\n",
      "sampled value (θj−1) is in the tail of the distribution. The result is that the\n",
      "algorithm “sticks” in one place for long periods of time before moving.\n",
      "\n",
      "−0.016 −0.014 −0.012 −0.010\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "m2\n",
      "\n",
      "f(m\n",
      "2\n",
      ")\n",
      "\n",
      "posterior for m2\n",
      "\n",
      "U(−.0003,.0003) proposal\n",
      "\n",
      "U(−.003,.003) proposal\n",
      "\n",
      "θj−1\n",
      "\n",
      "Fig. 6.6. Posterior density for m2 parameter from planar density MH algorithm\n",
      "and two proposal densities: U(−.0003, .0003) and U(−.003, .003).\n",
      "\n",
      "Figure 6.7 shows the first 1000 iterations of the algorithm when the broader\n",
      "proposal density is used. As the figure demonstrates, the algorithm quickly\n",
      "moved to the center of the posterior density, but then it very rarely moved.\n",
      "The acceptance rate confirms this numerically: The acceptance rate for the\n",
      "first 1,000 iterations was only 3.2%, but the acceptance rate for the first 18\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 143\n",
      "\n",
      "iterations was 50% (the rate steadily declined after iteration 18). Thus, the\n",
      "broader proposal density produced even slower mixing, ultimately, than the\n",
      "narrower proposal.\n",
      "\n",
      "0 200 400 600 800 1000\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "4\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "6\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "2\n",
      "\n",
      "Fig. 6.7. Trace plot of the first 1,000 iterations of an MH algorithm for the planar\n",
      "density with a (relatively) broad U(−.003, .003) proposal density.\n",
      "\n",
      "The shape of the proposal density relative to that of the posterior den-\n",
      "sity also influences the acceptance rate. In the planar density example, the\n",
      "proposal density was uniform, but the posterior is symmetric and bell-shaped\n",
      "(approximately normal). If we were to change the proposal density to a normal\n",
      "density with a width approximately equal to that of the U(−.003, .003) den-\n",
      "sity (see Figure 6.8), the acceptance rate would increase. In fact, I obtained\n",
      "an acceptance rate of 8.3% with this normal proposal, more than twice that\n",
      "obtained using the broad uniform proposal.\n",
      "\n",
      "This acceptance rate is still quite low, despite the fact that the proposal\n",
      "shape for m2 almost perfectly matches the shape of the marginal posterior\n",
      "for m2. To increase the acceptance rate, we could consider using a normal\n",
      "proposal density with smaller variance. However, as shown in the previous\n",
      "\n",
      "\n",
      "\n",
      "144 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "−0.016 −0.014 −0.012 −0.010\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "m2\n",
      "\n",
      "f(m\n",
      "2\n",
      ")\n",
      "\n",
      "posterior for m2\n",
      "\n",
      "N(0,sd=.00085) proposal\n",
      "\n",
      "U(−.003,.003) proposal\n",
      "\n",
      "θj−1\n",
      "\n",
      "Fig. 6.8. Posterior density for m2 parameter from planar density MH algorithm\n",
      "and two proposal densities: U(−.003, .003) and N(0, .00085).\n",
      "\n",
      "section, using a substantially smaller uniform proposal, while increasing the\n",
      "acceptance rate, led to very slow mixing so that the algorithm needed to be\n",
      "run for 50,000 iterations to obtain an adequate sample from the posterior.\n",
      "\n",
      "What we have not considered is the correlation between the parameters\n",
      "m1 and m2, which is another issue we should consider when selecting a pro-\n",
      "posal density. As we discussed in the previous section, a common source of\n",
      "poor mixing and/or slow convergence in MH algorithms is strong posterior\n",
      "correlation of the parameters. In this particular model, the posterior correla-\n",
      "tion of the parameters is greater than −.9, which suggests that uncorrelated\n",
      "proposal densities for the two slope parameters may be problematic.\n",
      "\n",
      "Figure 6.9 is a two-dimensional trace plot of the two slope parameters from\n",
      "the original MH algorithm, with the contours of two proposal densities super-\n",
      "imposed (at the center of the posterior): a bivariate normal with 0 correlation\n",
      "(as used previously) and a bivariate normal with −.9 correlation. As the fig-\n",
      "ure shows, the proposal density with correlation 0 does not match the shape\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 145\n",
      "\n",
      "of the posterior very well, while the one with the strong negative correlation\n",
      "does.\n",
      "\n",
      "−0.010 −0.005 0.000 0.005\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "m1\n",
      "\n",
      "m\n",
      "2 Normal proposal, r=0\n",
      "\n",
      "Normal proposal, r=−.9\n",
      "\n",
      "Fig. 6.9. Two-dimensional trace plot for initial run of MH algorithm sampling\n",
      "parameters from planar density for GSS free speech and political participation data:\n",
      "Two possible bivariate normal proposal densities are superimposed.\n",
      "\n",
      "The fact that the 0-correlation proposal does not match the posterior\n",
      "very well suggests that this proposal will frequently propose poor candidates,\n",
      "leading to a high rejection rate, slow convergence, and poor mixing, as we\n",
      "have already observed.\n",
      "\n",
      "I reran the MH algorithm, using the bivariate normal proposal with cor-\n",
      "relation −.9 (and variance equal to the variance of the parameters obtained\n",
      "from the original run of the MH algorithm). This algorithm had an accep-\n",
      "tance rate of approximately 25%, converged quickly, and mixed much more\n",
      "rapidly than the algorithm using the other proposals. The acceptance rate is\n",
      "still somewhat low, and so we may consider reducing the scale of the proposal.\n",
      "In a final run of the algorithm, I used a bivariate normal proposal with corre-\n",
      "lation −.9 and standard deviations that that were half the size of those in the\n",
      "\n",
      "\n",
      "\n",
      "146 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "previous run. The acceptance rate for this run was 44%. Figure 6.10 shows\n",
      "a trace plot of the m2 parameter from this run. As the figure suggests, the\n",
      "algorithm converged quickly and mixed rapidly. In fact, after discarding the\n",
      "first 1,000 samples, the variance of the sampled values of m2 converged very\n",
      "rapidly, which suggests rapid and thorough mixing.\n",
      "\n",
      "0 1000 2000 3000 4000 5000\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "1\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "m\n",
      "2\n",
      "\n",
      "Fig. 6.10. Trace plot for 5000 iterations of an MH algorithm sampling parameters\n",
      "from planar density for GSS free speech and political participation data: Bivariate\n",
      "normal proposal density with correlation −.9.\n",
      "\n",
      "6.3.3 Autocorrelation of parameters\n",
      "\n",
      "MCMC algorithms, while producing samples from distributions, do not pro-\n",
      "duce independent samples. Instead, MCMC algorithms produce samples that\n",
      "are autocorrelated—recall that the entire basis of MCMC sampling is that\n",
      "each sampled value depends (only) on the value sampled immediately prior\n",
      "(the Markov property). Slow mixing—as evidenced by acceptance rates that\n",
      "are too high or too low—tends to exacerbate autocorrelation.\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 147\n",
      "\n",
      "The key problem with autocorrelation is that variance estimates will be\n",
      "incorrect; they will tend to be too small, just as standard errors are biased\n",
      "downward in a classical model that ignores dependence between observations.\n",
      "There are at least two simple ways adjustments can be made to compensate\n",
      "for autocorrelation. First, one can take every kth sampled value, where k is the\n",
      "number of lags beyond which autocorrelation is not a problem. This approach\n",
      "is called “thinning the chain,” and is computationally the easiest to perform\n",
      "(simply save every kth sampled value). Second, one can use the “batch means”\n",
      "method. Under the batch means approach, rather than discarding k−1 out of\n",
      "every k sampled values, one computes the means of every block of k sampled\n",
      "values and treats the batch mean as the sampled value (see Figure 6.4, for\n",
      "example).\n",
      "\n",
      "Under either approach, determining k—the number of lags beyond which\n",
      "the autocorrelation of sampled values is small enough to ignore—is relatively\n",
      "straightforward. As in time series analysis, we can compute the autocorrelation\n",
      "function at each lag (1+) and decide the number of iterations we need to\n",
      "skip in order to have a nonsignificant autocorrelation. The autocorrelation\n",
      "parameter for lag L is computed much the same as a standard correlation:\n",
      "\n",
      "ACFL =\n",
      "(\n",
      "\n",
      "T\n",
      "\n",
      "T − L\n",
      "\n",
      ") ∑T−L\n",
      "t=1 (xt − x̄)(xt+L − x̄)∑T\n",
      "\n",
      "t=1(xt − x̄)2\n",
      ". (6.1)\n",
      "\n",
      "In this computation, xt refers to the sampled value of x at iteration t, T is the\n",
      "total number of sampled values, x̄ is the mean of all the sampled values, and\n",
      "L is the lag. The fraction T/(T − L) is an adjustment for the fact that the\n",
      "denominator contains more terms than the numerator, since the denominator\n",
      "is summed across all iterations, but the numerator cannot be.\n",
      "\n",
      "Figure 6.11 is a plot of the autocorrelation function for the original MH\n",
      "algorithm (which used a pair of uniform proposal densities) and the final one\n",
      "using the bivariate normal proposal density with correlation −.9. Both plots\n",
      "show strong autocorrelation, but the bivariate normal proposal evidences less\n",
      "autocorrelation. Inference should be made after retaining only every 20th (or\n",
      "so) sample. Figure 6.12 shows the marginal posterior density for m2 from\n",
      "this MH algorithm after discarding the first 2,000 samples and saving every\n",
      "20th thereafter. The posterior mean for the parameter was −.0122, and the\n",
      "posterior standard deviation was .00077. These results are similar to those\n",
      "obtained from each of the previous algorithms, but with the better proposal,\n",
      "we were able to obtain them with a shorter run than the MH algorithms using\n",
      "the poorer proposal densities required.\n",
      "\n",
      "6.3.4 “R̂” and other calculations\n",
      "\n",
      "One approach that is very useful for examining for convergence and thorough\n",
      "mixing of MCMC algorithms is running multiple instances of algorithms from\n",
      "highly dispersed starting values for the parameters and comparing the results.\n",
      "\n",
      "\n",
      "\n",
      "148 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "0 10 20 30 40\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "0 10 20 30 40\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "Fig. 6.11. Autocorrelation plots of parameter m2 from MH algorithms for the\n",
      "planar density: Upper figure is for the MH algorithm with independent uniform\n",
      "proposals for m1 and m2; and lower figure is for the MH algorithm with a bivariate\n",
      "normal proposal with correlation −.9.\n",
      "\n",
      "Historically, there has been considerable debate regarding whether one should\n",
      "run multiple instances of an algorithm and compare them, or whether one\n",
      "should run a single, but very long, instance to see whether the algorithm settles\n",
      "in one place for a long time but then eventually moves to another location\n",
      "(see Gelman 1996 for discussion). Under that approach, the chain can also be\n",
      "decomposed into segments, and the means of each segment can be compared\n",
      "numerically to determine whether there is any trending that suggests lack of\n",
      "convergence (e.g., by plotting the segment means or conducting time series\n",
      "regressions as we did in the previous section) and possibly poor mixing (e.g.,\n",
      "by conducting ANOVA or dummy regression using the segment means, within-\n",
      "segment variance, and total variance—this strategy is akin to the calculation\n",
      "of R discussed below). In many problems, with the incredible increase in\n",
      "computing power over the last decade, there is often no reason not to do\n",
      "both: Run multiple, long chains.\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 149\n",
      "\n",
      "−0.015 −0.014 −0.013 −0.012 −0.011 −0.010 −0.009\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "0\n",
      "3\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "m2\n",
      "\n",
      "f(m\n",
      "2\n",
      ")\n",
      "\n",
      "Fig. 6.12. Histogram of marginal posterior density for m2 parameter: Dashed ref-\n",
      "erence line is the posterior mean.\n",
      "\n",
      "How do we determine starting values for multiple chains? One approach is\n",
      "to start all parameters at 0 for one chain (if such is sensible—e.g., for regression\n",
      "parameters, this approach may be reasonable; for a variance parameter, it may\n",
      "not be), use maximum likelihood estimates from a similar (or identical) model\n",
      "as starting values in another, and use wildly inappropriate starting values for\n",
      "the parameters for a third. If all three chains converge to the same location,\n",
      "it may indicate convergence.\n",
      "\n",
      "What else can we do besides simply using different starting values? We can\n",
      "often do more than simply modify starting values in an MCMC algorithm.\n",
      "For example, we can modify the proposal densities (either their form or their\n",
      "width/variance) used in each algorithm. If we are updating parameters se-\n",
      "quentially and not all simultaneously, we can switch the order of updating.\n",
      "In other words, we can play around with the fundamentals of the program\n",
      "conducting the simulation. Ultimately, the point is that, if the algorithms are\n",
      "converging and mixing well, they should all yield similar results.\n",
      "\n",
      "How do we detect convergence from the results? Once a set of chains has\n",
      "been produced, we need to have some tools for evaluting the results. One way\n",
      "\n",
      "\n",
      "\n",
      "150 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "to compare multiple chains is to overlay all of them on a single trace plot. If\n",
      "the traces are indistinguishable, then the algorithm may have converged and\n",
      "mixed throughout the density of interest.\n",
      "\n",
      "As an example, let’s consider again the MH algorithm for the bivariate\n",
      "normal model for the free speech/political participation data. In the original\n",
      "MH model, we simulated µx and µy from their full conditional distributions\n",
      "(Gibbs sampling steps), but we simulated the variance parameters σ2x and σ\n",
      "\n",
      "2\n",
      "y\n",
      "\n",
      "as well as the correlation parameter ρ using MH steps with uniform proposal\n",
      "densities. I have rerun the MH algorithm three times, using different sets of\n",
      "starting values for the variance and correlation parameters. In the first run,\n",
      "I used σ2x = σ\n",
      "\n",
      "2\n",
      "y = 1 and ρ = 0 as starting values. In the second, I used\n",
      "\n",
      "σ2x = σ\n",
      "2\n",
      "y = 10 and ρ = −.99. In the third, I used σ2x = σ2y = .01 and ρ = .99.\n",
      "\n",
      "These starting values are highly dispersed: The starting values for ρ span the\n",
      "possible range for this parameter, and the starting values for the variance\n",
      "parameters span a large range of possible values, starting near the lowest\n",
      "possible value (0).\n",
      "\n",
      "Figure 6.13 shows a trace plot of the sampled values of ρ for the first\n",
      "600 iterations of the MH algorithm under each set of starting values. As the\n",
      "figure reveals, all three runs of the algorithm converge to a common region\n",
      "within little more than 400 iterations. Beyond 400 iterations, the simulation\n",
      "sequences become indistinguishable.\n",
      "\n",
      "Figure 6.14 shows a two-dimensional trace plot of the sampled values of\n",
      "the variance parameters. This figure provides a picture consistent with that\n",
      "for ρ: Regardless of starting values for the variances, the algorithms converged\n",
      "rapidly to a common bivariate region for these parameters.\n",
      "\n",
      "The acceptance rates for the three parameters, across the three algorithms,\n",
      "were quite stable at 55%, 64%, and 57% after the first 1,000 iterations (dif-\n",
      "fering across runs by less than one percentage point), providing additional\n",
      "evidence that convergence had been reached relatively early, and that thor-\n",
      "ough mixing had occurred after the first 1,000 or so iterations.\n",
      "\n",
      "Another approach involving multiple chains is to compare numerical re-\n",
      "sults: Are means, variances, etc. similar across the different chains? They are\n",
      "almost certain to not be identical, but are they within the limits of MCMC\n",
      "sampling error?1 In the current example, the means for ρ for the three runs\n",
      "were .4504, .4495, and .4490, with posterior standard deviations of .0215,\n",
      "\n",
      "1 MCMC algorithms produce samples from a distribution. Running an algorithm\n",
      "multiple times will produce different samples and, thus, different means, vari-\n",
      "ances, and other statistics. The classical Central Limit Theorem says that these\n",
      "statistics will be normally distributed with a mean equal to the population mean\n",
      "and a standard deviation equal to the population standard deviation divided by\n",
      "the square root of the sample size. Thus, we can estimate MCMC error by com-\n",
      "puting the posterior standard deviation and dividing it by the square root of the\n",
      "MCMC sample size. The posterior mean should vary by a factor of this quantity.\n",
      "Specifically, 95% of MCMC runs should produce posterior means within ±1.96\n",
      "standard errors.\n",
      "\n",
      "\n",
      "\n",
      "6.3 Recognizing poor performance 151\n",
      "\n",
      "0 100 200 300 400 500 600\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      ".0\n",
      "−\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "Iteration\n",
      "\n",
      "ρ\n",
      "\n",
      "Chain 1 Start: ρ=0\n",
      "\n",
      "Chain 3 Start: ρ=0.99\n",
      "\n",
      "Chain 2 Start: ρ=− 0.99\n",
      "\n",
      "Fig. 6.13. Trace plot of first 600 sampled values of ρ from MH algorithms with\n",
      "three different starting values (GSS political participation and free speech data).\n",
      "\n",
      ".0214, and .0209, respectively. Thus, the posterior means are all within MCMC\n",
      "sampling fluctuation of each other, providing some indication that the algo-\n",
      "rithm converged and mixed thoroughly.\n",
      "\n",
      "A common aproach used to determine convergence and adequate mix-\n",
      "ing is the calculation of the “Scale reduction factor” R̂ (often called the\n",
      "“Gelman-Rubin convergence diagnostic.” This is not the ratio R from the\n",
      "MH algorithm!) (see Gelman 1996). When m chains are run, each of length\n",
      "n, we can compute the mean of a parameter θ for each chain (θ̄j), the overall\n",
      "mean of the parameter if we combined all chains (θ̄), the within-chain vari-\n",
      "ance [(1/(m(n − 1))\n",
      "\n",
      "∑m\n",
      "i=1\n",
      "\n",
      "∑n\n",
      "j=1(θij − θ̄i)\n",
      "\n",
      "2], and the between-chain variance\n",
      "[n/(m− 1)\n",
      "\n",
      "∑m\n",
      "i=1(θ̄i − θ̄)\n",
      "\n",
      "2]. Gelman (1996) shows that the total variance of θ,\n",
      "then, is (n− 1)/n× within variance + (1/n)× between variance.\n",
      "\n",
      "As the chains converge, the variance between the chains should decrease,\n",
      "implying that the within variance approaches the total variance. Thus, the\n",
      "scale reduction factor can be computed as:\n",
      "\n",
      "√\n",
      "R̂ =\n",
      "\n",
      "√\n",
      "Total/Within. This fac-\n",
      "\n",
      "tor should be close to 1 when convergence has been reached. Continuing with\n",
      "\n",
      "\n",
      "\n",
      "152 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "0.0 0.5 1.0 1.5 2.0 2.5 3.0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "2\n",
      ".5\n",
      "\n",
      "3\n",
      ".0\n",
      "\n",
      "σx\n",
      "2\n",
      "\n",
      "σ\n",
      "y2\n",
      "\n",
      "Chain 2 Start: σx\n",
      "2=σy\n",
      "\n",
      "2=10\n",
      "\n",
      "Chain 3 Start: σx\n",
      "2=σy\n",
      "\n",
      "2=0.01\n",
      "\n",
      "Chain 1 Start: σx\n",
      "2=σy\n",
      "\n",
      "2=1\n",
      "\n",
      "Fig. 6.14. Two-dimensional trace plot of sampled values of σ2x and σ\n",
      "2\n",
      "y from MH\n",
      "\n",
      "algorithms with three different starting values (GSS political participation and free\n",
      "speech data).\n",
      "\n",
      "the bivariate normal model example, Figure 6.15 shows the R̂ statistic for\n",
      "ρ, σ2x, and σ\n",
      "\n",
      "2\n",
      "y computed across the first 500 iterations of the three MH al-\n",
      "\n",
      "gorithms. The plot shows that R̂ rapidly declined to around 1 in the first\n",
      "200 iterations. By 400 iterations, the value of R̂ for all three parameters had\n",
      "leveled off at about 1. By iteration 500, the three values are indistinguishable\n",
      "and negligibly different from 1. These results are consistent with those pre-\n",
      "sented in the trace plots: By iteration 600, all three algorithms had converged\n",
      "to a common location. Beyond iteration 600, through the end of the 10,000\n",
      "iteration run, all three algorithms appeared to mix thoroughly.\n",
      "\n",
      "The R̂ statistic can be computed “on the fly,” that is, during the course of\n",
      "the run of an algorithm updating three separate chains and can therefore help\n",
      "us determine when we have run the algorithm a sufficient number of iterations.\n",
      "However, this statistic has its limitations, just like any other. An R̂ of approx-\n",
      "imately 1 does not guarantee that the algorithms have converged nor mixed\n",
      "thoroughly. It may be that our starting values were not sufficiently dispersed,\n",
      "\n",
      "\n",
      "\n",
      "6.4 Evaluating model fit 153\n",
      "\n",
      "0 100 200 300 400 500\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "R̂\n",
      "\n",
      "ρ\n",
      "σx\n",
      "\n",
      "2\n",
      "\n",
      "σy\n",
      "2\n",
      "\n",
      "Fig. 6.15. Trace plot of the scale reduction factor R̂ for ρ, σ2x, and σ\n",
      "2\n",
      "y across the\n",
      "\n",
      "first 500 iterations of the MH algorithms.\n",
      "\n",
      "and that all of our algorithms became stuck in the closest mode without\n",
      "exploring the entire posterior density. Nonetheless, this statistic, along with\n",
      "the additional evidence (stable and similar acceptance rates, trace plots, con-\n",
      "sistent means and variances of parameters, etc.) together provide consistent\n",
      "evidence that suggests our algorithms converged and mixed well.\n",
      "\n",
      "6.4 Evaluating model fit\n",
      "\n",
      "Once we have determined that the algorithm we used converged to the appro-\n",
      "priate distribution and mixed well so that we have an adequate sample from\n",
      "the posterior distribution, our next step should be to determine whether the\n",
      "model fit the data well enough to justify drawing inference about the param-\n",
      "eters. At this point, if we have estimated several different models, we can also\n",
      "begin to decide which is the best model.\n",
      "\n",
      "In standard likelihood analyses we typically use a measure, such as R2 or\n",
      "likelihood-ratio χ2 statistics, to determine whether the model fits the data.\n",
      "\n",
      "\n",
      "\n",
      "154 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "We can use such measures in a Bayesian setting as well, but the Bayesian\n",
      "approach offers a broader range of possibilities in addition to these, as we will\n",
      "discuss briefly here but more in depth in the remaining chapters of the book.\n",
      "\n",
      "6.4.1 Residual analysis\n",
      "\n",
      "One way we can evaluate model fit is to conduct residual analyses much\n",
      "as we would if we were conducting a classical analysis. The only difference\n",
      "between a Bayesian and a classical analysis of residuals is that, whereas the\n",
      "classical approach produces a point estimate for parameters, and thus a point\n",
      "estimate for the residual for each observation, the Bayesian approach produces\n",
      "a distribution of the parameters and thus a distribution of residuals for each\n",
      "observation in the data set. The distribution of residuals provides us with\n",
      "more information with which to assess the model, because we can examine\n",
      "the distribution of errors for each observation: We can construct “tests” to\n",
      "determine whether the distribution of errors for each case is “significantly”\n",
      "different from 0. Cases for which the distribution of errors is far from 0 indicate\n",
      "that the observation is not fit well by the model. Thus, we do not have to rely\n",
      "on a simple examination of how different a case’s error is from the rest of the\n",
      "sample’s errors—we can concentrate on each case itself. Furthermore, we can\n",
      "construct sample-level tests to determine the probability that some proportion\n",
      "of the errors exceeds some value. For example, in a sample of size n = 100,\n",
      "with an MCMC sample of 1,000 draws from the posterior distribution for\n",
      "the parameters, we could compute, iteration by iteration, the proportion of\n",
      "errors that exceed some value q, collect these proportions into a distribution\n",
      "of 1,000 proportions, and evaluate this distribution. A model that produces\n",
      "a relatively high average proportion of errors exceeding the criterion q may\n",
      "then be deemed a poorly fitting model.\n",
      "\n",
      "Ultimately, Bayesian residual analysis provides greater flexibility in evalu-\n",
      "ating the fit of the model to the data than does a classical analysis. This fact\n",
      "will be even more evident when we discuss generalized linear models (GLMs).\n",
      "In GLMs, the classical approach is limited to using ad hoc tests of residuals\n",
      "that are based on ordinary least squares (OLS) regression residual analysis;\n",
      "however, these tests are not well suited to models in which the outcome is not\n",
      "measured at the interval level. The data-augmentation/latent data approach\n",
      "used by Bayesians, on the other hand, allows us to compute “latent residuals”\n",
      "that are continuously distributed, and thus allows us to follow the same format\n",
      "for residual analyses as OLS regression without stretching assumptions (see\n",
      "Johnson and Albert 1999 for an in-depth exposition of this topic in ordinal\n",
      "data models).\n",
      "\n",
      "Because residual analyses are very similar, ultimately, to using poste-\n",
      "rior predictive simulation, and because they are primarily only useful in a\n",
      "regression-model setting, which we have yet to discuss, I do not discuss resid-\n",
      "ual analysis further at this point. We will discuss this topic more in depth in\n",
      "the remaining chapters when we develop regression models.\n",
      "\n",
      "\n",
      "\n",
      "6.4 Evaluating model fit 155\n",
      "\n",
      "6.4.2 Posterior predictive distributions\n",
      "\n",
      "One of the best and most flexible approaches to examining model fit is the\n",
      "use of posterior predictive distributions. The posterior predictive distribution\n",
      "for a model is the distribution of future observations that could arise from\n",
      "the model under consideration. The posterior predictive distribution takes\n",
      "into account both (1) parametric uncertainty and (2) sampling uncertainty\n",
      "from the original model. Parametric uncertainty is captured via the posterior\n",
      "distribution for the parameters, a sample of which is the result of simulation\n",
      "using MCMC methods. Sampling uncertainty is captured via the specification\n",
      "of the sampling density for the data. Recall that the posterior density for\n",
      "the parameters is a product of the prior distribution for parameters and the\n",
      "likelihood function (sampling density for the data):\n",
      "\n",
      "p(θ | data) ∝ p(data | θ)p(θ).\n",
      "\n",
      "Once this posterior density is obtained, future observations should be expected\n",
      "to arise from the sampling distribution for the data in the original model; the\n",
      "parameters for this sampling distribution, however, are no longer weighted\n",
      "based on the prior from the original model, but rather they are weighted by\n",
      "the posterior distribution for the parameters. Formally, if we use yrep to denote\n",
      "future observations, the probability density for future observations, given the\n",
      "observed sample data is:\n",
      "\n",
      "p(yrep | y) =\n",
      "∫\n",
      "\n",
      "p(yrep | θ)p(y | θ)p(θ)dθ. (6.2)\n",
      "\n",
      "The latter two terms on the right side of this equation (not counting the dθ)\n",
      "constitute the posterior distribution of the parameters. The first term on the\n",
      "right side is the probability density for a future observation that could be\n",
      "drawn from its sampling distribution, which, of course, is governed by the\n",
      "parameter θ.\n",
      "\n",
      "If a model fits the current data well—that is, we have adequately captured\n",
      "the data-generation process—future data simulated from the model should\n",
      "look much like the current data. Thus, we can simulate data from the pos-\n",
      "terior predictive distribution, compare it with the observed data, and, if the\n",
      "simulated data are similar to the observed data, we may conclude the model\n",
      "fits well. In order to determine whether the simulated and observed data are\n",
      "similar, we can conduct formal tests using Bayesian p-values. If we define T (y)\n",
      "to be a function of the data (a test statistic) and T (yrep) to be the same func-\n",
      "tion applied to the replicated data, then we can compute a Bayesian p-value\n",
      "as:\n",
      "\n",
      "p-value = p(T (yrep) ≥ T (y)|y). (6.3)\n",
      "\n",
      "In English, the p-value is the proportion of replicated future data sets whose\n",
      "function values T (yrep) exceed that of the function T (y) applied to the original\n",
      "\n",
      "\n",
      "\n",
      "156 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "data (Rubin 1984). For example, T (y) could be T (y) = max(y), that is, the\n",
      "maximum observed y in the sample. If we generated 1,000 replicated data\n",
      "sets of size n, where n was the original sample size, and the maximum yrep\n",
      "\n",
      "value exceeded the maximum observed y value in 3,500 of the replicated data\n",
      "sets, then the p-value would be .35. In that case, the replicated data appear\n",
      "consistent with the observed data.\n",
      "\n",
      "The interpretation of tests based on the posterior predictive distribution\n",
      "is straightforward. Such tests represent the probability that a future observa-\n",
      "tion would exceed the observed data, given the model. An extreme p-value,\n",
      "therefore, implies poor model fit. In addition to constructing tests that are\n",
      "solely functions of data, we can also compute “discrepancy statistics” that are\n",
      "functions of both data and parameters (see Gelman, Meng, and Stern 1996,\n",
      "and Rubin 1984).\n",
      "\n",
      "There is no limit to the types and numbers of posterior predictive tests\n",
      "that can be performed to evaluate model fit. In general, posterior predictive\n",
      "tests allow for much greater flexibility in testing particular features of a model\n",
      "than classical tests provide, and I provide some specific examples in several\n",
      "chapters in the second part of the book. Lynch and Western (2004) provide\n",
      "some examples for models, the fits of which are not easily evaluable using\n",
      "classical methods.\n",
      "\n",
      "Implementation of posterior predictive simulation is relatively simple,\n",
      "given an MCMC-generated sample of size J from the posterior distribution\n",
      "for the parameters in a model (θ1 . . . θJ), and can often be incorporated as\n",
      "part of the MCMC algorithm itself. For each value of θ simulated from the\n",
      "posterior, we generate a new observation from the sampling distribution for\n",
      "the data, using that parameter value, for every original observation in the\n",
      "sample.\n",
      "\n",
      "As an example, consider the planar density and bivariate normal models\n",
      "for the free speech and political participation data. I have saved 500 samples\n",
      "from the posterior distribution for the parameters for both of these models\n",
      "(say θ1P . . . θ\n",
      "\n",
      "500\n",
      "P and θ\n",
      "\n",
      "1\n",
      "BV N . . . θ\n",
      "\n",
      "500\n",
      "BV N ), respectively. For each member of these\n",
      "\n",
      "parameter sets, I have generated a new sample of size n = 1, 377 (the original\n",
      "sample size of the data) from the sampling density for the data. For the\n",
      "planar density model, this means sampling 500 samples of size n = 1, 377\n",
      "observations from the planar density; for the bivariate normal density, this\n",
      "means sampling 500 samples of size n = 1, 377 observations from a bivariate\n",
      "normal distribution.\n",
      "\n",
      "In these two models, the observations are exchangeable—there is no infor-\n",
      "mation to distinguish one observation from another. Thus, rather than exam-\n",
      "ining the fit of the model for individual cases (as can be done in regression\n",
      "models), our posterior predictive tests must remain at the sample level.\n",
      "\n",
      "I chose a variety of criteria by which to evaluate the fit of each model and,\n",
      "ultimately, to compare the two models, including the following: (1) the ratio\n",
      "of the median to the mean for both variables (political participation and free\n",
      "speech), (2) the number of observations in the lowest category of each variable\n",
      "\n",
      "\n",
      "\n",
      "6.4 Evaluating model fit 157\n",
      "\n",
      "[i.e., (x, y) = (0, 0)], (3) the number of observations in the highest category\n",
      "of each variable [i.e., (x, y) = (5, 5)], (4) the number of observations in the\n",
      "highest category of one variable but the lowest in the other [i.e., (x, y) = (5, 0)],\n",
      "and (5) vice versa [i.e., (x, y) = (0, 5)]. Given that the original data were\n",
      "discrete, the continuous values simulated from the bivariate normal and planar\n",
      "distributions were rounded to the nearest integer.2 These particular measures\n",
      "were chosen to evaluate whether the models managed to capture the essential\n",
      "shape of the data. The ratio of the mean to the median gives us a sense of\n",
      "whether the models were able to capture the skew of the data, and the four\n",
      "measures of the number of observations at each corner of the data give us\n",
      "a sense of whether each model is actually replicating the shape of the data\n",
      "distribution.\n",
      "\n",
      "Figure 6.16 shows the posterior predictive distributions for the ratio of the\n",
      "mean to the median for both variables (x and y) and for both models. The\n",
      "vertical reference line in each plot is the value of this ratio in the original\n",
      "data; the histograms are for the posterior predictive distributions. As the\n",
      "figure shows, neither model was successful at capturing the ratio of the mean\n",
      "to median (and hence the skew) for x (the political participation item). This\n",
      "value was 1.18 in the original sample. In the bivariate normal distribution\n",
      "model, the posterior predictive distribution for this measure was centered\n",
      "over 1 (as expected; there is no skew in a normal distribution). In the planar\n",
      "distribution model, the posterior predictive distribution is centered around\n",
      "1.04, which is somewhat closer to the original sample’s value than is the result\n",
      "for the normal distribution model.\n",
      "\n",
      "For the y variable (the free speech item), the ratio in the sample was 1.02.\n",
      "The planar density overestimates the ratio, as evidenced by the posterior\n",
      "predictive distribution being centered around 1.13. However, the posterior\n",
      "predictive distribution for the bivariate normal distribution is consistent with\n",
      "the original sample’s value. The Bayesian p-value for this test is .26: Only\n",
      "26% of the posterior predictive samples have a mean/median ratio that is\n",
      "more extreme than the observed sample ratio.\n",
      "\n",
      "Table 6.1 presents the results of all five tests. The results show that,\n",
      "although in the original data there were 361 observations in the (0,0) cell,\n",
      "the posterior predictive distributions for the two models predicted far fewer\n",
      "(95 and 27 for the bivariate normal and planar densities, respectively). Both\n",
      "Bayesian p-values are 0, which indicates that neither model fits the data well\n",
      "at this end of the distribution. At the other extreme end of the distribution\n",
      "(the 5,5 cell), the original data consisted of 5 observations. The posterior pre-\n",
      "dictive distributions for the two models predicted .07 and 1.6 observations,\n",
      "with p-values of 0 and .01, respectively. Both models appear to underestimate\n",
      "\n",
      "2 This approach is not entirely satisfactory—as the results show, this approach\n",
      "reduces the number of observations in the most extreme cells, which have a smaller\n",
      "range of values that can be rounded to them. For example, values from 0 to .499\n",
      "are rounded to 0, but values from .499 to 1.499 are rounded to 1.\n",
      "\n",
      "\n",
      "\n",
      "158 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "0.8 0.9 1.0 1.1 1.2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Mean(x)/Median(x) (BVN model)\n",
      "\n",
      "P\n",
      "P\n",
      "\n",
      "D\n",
      " f\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0.8 0.9 1.0 1.1 1.2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Mean(x)/Median(x) (Planar model)\n",
      "\n",
      "P\n",
      "P\n",
      "\n",
      "D\n",
      " f\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0.8 0.9 1.0 1.1 1.2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Mean(y)/Median(y) (BVN model)\n",
      "\n",
      "P\n",
      "P\n",
      "\n",
      "D\n",
      " f\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0.8 0.9 1.0 1.1 1.2\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Mean(y)/Median(y) (Planar model)\n",
      "\n",
      "P\n",
      "P\n",
      "\n",
      "D\n",
      " f\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 6.16. Posterior predictive distributions for the ratio of the mean to the me-\n",
      "dian in the bivariate normal distribution and planar distribution models: Vertical\n",
      "reference line is the observed value in the original data.\n",
      "\n",
      "the number of observations in this cell, but the planar model underestimates\n",
      "less. The final two tests suggest that the bivariate normal distribution con-\n",
      "sistently underestimates the counts in the tails of the distribution, but the\n",
      "planar distribution does only slightly better.\n",
      "\n",
      "We could conduct a similar test on each cell of the bivariate table, but\n",
      "as a simple summary, I computed the correlation between the observed and\n",
      "the posterior predictive distribution cell counts for both models. Figure 6.17\n",
      "shows the distributions of these correlations. The mean correlation between\n",
      "observed and predictive cell counts for the planar distribution model was .32,\n",
      "\n",
      "\n",
      "\n",
      "6.5 Formal comparison and combining models 159\n",
      "\n",
      "which is a low to moderate correlation, while the mean correlation for the\n",
      "bivariate normal distribution model was .75.\n",
      "\n",
      "Table 6.1. Posterior predictive tests for bivariate normal and planar distribution\n",
      "models.\n",
      "\n",
      "Bivariate Normal Planar\n",
      "Sample\n",
      "\n",
      "Test Value µPPD p-value µPPD p-value\n",
      "\n",
      "Mean(x)/Median(x) 1.18 1.00 0.0 1.04 0.0\n",
      "Mean(y)/Median(y) 1.02 0.99 0.26 1.13 0.\n",
      "# of (0,0) Observations 361.0 94.7 0.0 26.5 0.0\n",
      "# of (5,5) Observations 5.0 0.07 0.0 1.6 0.01\n",
      "# of (5,0) Observations 11.0 0.17 0.0 20.3 0.03\n",
      "# of (0,5) Observations 2.0 0.1 0.01 7.0 0.05\n",
      "\n",
      "Note: Data are from the 2000 GSS special topic module on freedom (variables are\n",
      "expunpop and partpol).\n",
      "\n",
      "Overall, the results of the posterior predictive simulation indicate that\n",
      "neither model fits these data particularly well. The normal distribution con-\n",
      "sistently underpredicts the number of observations in the tails of the distri-\n",
      "bution, and the planar distribution fails to match the overall shape of the\n",
      "data distribution. The results suggest that, of the two models, the bivariate\n",
      "normal should be preferred. However, this result is not particularly surprising.\n",
      "The planar density model involved three parameters and did not allow for a\n",
      "relationship between the two variables. The bivariate normal model, on the\n",
      "other hand, had five parameters, including one that captures the relationship\n",
      "between the two variables.\n",
      "\n",
      "6.5 Formal comparison and combining models\n",
      "\n",
      "6.5.1 Bayes factors\n",
      "\n",
      "We are often interested in comparing two or more models to determine which\n",
      "is “best.” Sometimes, choosing the best model may be our strategy for de-\n",
      "termining which of two competing theories or hypotheses provides a better\n",
      "explanation for the data at hand. Occasionally, the models we would like to\n",
      "compare are nested; that is, one model is a special case of another. In such\n",
      "cases, the classical approach using maximum likelihood methods provides a\n",
      "prescription for testing for “significant” differences between two models. More\n",
      "often, however, we need to compare models that are not nested. An informal,\n",
      "\n",
      "\n",
      "\n",
      "160 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Correlation between Observed & PPD Cell Counts\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "Bivariate Normal\n",
      "Planar\n",
      "\n",
      "Fig. 6.17. Correlations between observed cell counts and posterior predictive dis-\n",
      "tribution cell counts for the bivariate normal and planar distribution models.\n",
      "\n",
      "yet flexible and multifaceted approach to comparing models is the use of\n",
      "posterior predictive simulation as described in the previous section. That is,\n",
      "we could establish a number of criteria (e.g., test statistics) along which to\n",
      "compare models and choose the model that bests meets the criteria. Indeed,\n",
      "ultimately, model comparison rests on comparing how well two models fit the\n",
      "same data, and residual analysis and posterior predictive simulation tell us\n",
      "how well a model fits the data.\n",
      "\n",
      "The Bayesian approach to statistics also offers more formal means for\n",
      "comparing models. The use of Bayes factors is one tool for doing so (see Kass\n",
      "and Raftery 1995, and Raftery 1995, for more extensive discussion of model\n",
      "selection and the Bayes Factor in general, but also see Gelman and Rubin\n",
      "1995).\n",
      "\n",
      "Suppose we have two models M1 and M2 with parameters θ1 and θ2 that we\n",
      "would like to compare. Ultimately, what we would like to know is the posterior\n",
      "probability of each model, given the data we have observed—P (M | data). A\n",
      "comparison of the two models, given the posterior probabilities for each would\n",
      "then be straightforward:\n",
      "\n",
      "\n",
      "\n",
      "6.5 Formal comparison and combining models 161\n",
      "\n",
      "Posterior Odds =\n",
      "p(M1 | data)\n",
      "p(M2 | data)\n",
      "\n",
      ".\n",
      "\n",
      "In this equation, a value for the posterior odds that is greater than 1 indicates\n",
      "that model 1 is favored; a value less than 1 indicates that model 2 is favored;\n",
      "and a value equal to 1 indicates that neither model is preferable.\n",
      "\n",
      "The posterior probability for a model given the data can be computed using\n",
      "Bayes’ rule: Both the numerator and the denominator of the posterior odds can\n",
      "be broken down so that p(M,data) = p(data | M)p(M). [Or, expressed as a\n",
      "conditional: p(M | data) ∝ p(data | M)p(M).] The latter half of the resulting\n",
      "ratio—p(M1)/p(M2)—is called the “prior odds” for each model. Generally, we\n",
      "specify equal prior probabilities for the models, which means that this ratio\n",
      "is 1 and can be ignored.\n",
      "\n",
      "The first part of the ratio—p(data,M1)/p(data,M2)—is the ratio of the\n",
      "marginal likelihoods for the data, once the parametric uncertainty in each\n",
      "model is integrated out [i.e., p(y) =\n",
      "\n",
      "∫\n",
      "p(data,M)dM ] and is called the Bayes\n",
      "\n",
      "factor. Put another way, the marginal likelihood for the data is the integral\n",
      "of the posterior density over the parameters:\n",
      "\n",
      "p(y | Mi) =\n",
      "∫\n",
      "\n",
      "θi∈Si\n",
      "p(y | θi,Mi)p(θi | Mi)dθi. (6.4)\n",
      "\n",
      "This integration is a significant limitation to the use of Bayes factors; the\n",
      "integration is difficult. The integration essentially produces the normalizing\n",
      "constant that we generally cannot easily compute and are able to avoid with\n",
      "MCMC estimation. Although Raftery (1995) shows an approximation to the\n",
      "integral (the Bayesian Information Criterion—BIC) that can be fairly eas-\n",
      "ily computed using standard output from the maximum likelihood results in\n",
      "most software packages, it may not be easily computed from the results of\n",
      "an MCMC algorithm. Furthermore, the Bayes factor is quite sensitive to the\n",
      "choice of priors used for parameters in each model, and so it should be used\n",
      "with caution. I do not discuss the Bayes factor in this book for these rea-\n",
      "sons. Instead, I prefer the flexibility and ease of posterior predictive checks\n",
      "for selecting models, and I focus on them.\n",
      "\n",
      "6.5.2 Bayesian model averaging\n",
      "\n",
      "A fairly recent development in Bayesian statistics has been the emergence of\n",
      "Bayesian model averaging, seemingly as a response to (or extension of) the\n",
      "model selection approach encouraged by the use of Bayes factors. Whereas\n",
      "Bayes factors can be used to select the “best” model from a set of models, the\n",
      "model averaging approach essentially combines all models in a class of models\n",
      "to generate inference about a parameter. For example, in an OLS regression\n",
      "model with J predictor variables, the Bayes factor approach would be to se-\n",
      "lect the best model (i.e., the best combination of predictors) and then use the\n",
      "\n",
      "\n",
      "\n",
      "162 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "results of that model for inference. Bayesian model averaging, on the other\n",
      "hand, averages over model uncertainty—the fact that we are ultimately un-\n",
      "certain which model is, in fact, best—by assigning prior weights to all models\n",
      "in a class of models and producing a marginal posterior distribution for the\n",
      "parameter that is basically a weighted combination of the posterior for the\n",
      "parameter under all models (Hoeting et al. 1999). Formally:\n",
      "\n",
      "p(θ | y) =\n",
      "J∑\n",
      "\n",
      "j=1\n",
      "\n",
      "p(θ | Mj , y)p(Mj | y). (6.5)\n",
      "\n",
      "The first term on the right-hand side of Equation 6.5 is the posterior distri-\n",
      "bution for the parameter under each specific model, and the second term is\n",
      "the posterior probability for each model itself. The posterior probability for\n",
      "each model is the numerator of the posterior odds presented in the previous\n",
      "section on Bayes factors; hence, it incorporates a prior for each model under\n",
      "consideration with its marginal likelihood, given the data. Thus, the posterior\n",
      "distribution for a parameter under BMA is ultimately just a weighted mixture\n",
      "of the posterior distribution for the parameter under all models.\n",
      "\n",
      "Model averaging, although appealing, is difficult. First, it requires the\n",
      "construction of a huge number of prior distributions—one for each model\n",
      "in the class being examined. So, for example, in the OLS regression model\n",
      "mentioned above with J covariates, we would need 2J priors. Of course, we\n",
      "could simply be agnostic and give equal weight to all models, but doing so\n",
      "may lead us, in the posterior, to favor models that are very similar to one\n",
      "another. Second, model averaging is potentially incredibly computationally\n",
      "intensive. With J covariates, the model space consists of 2J models that must\n",
      "be estimated. Although there are ways to reduce the model space to a more\n",
      "manageable number of models (e.g., Occam’s window; see Hoeting et al. 1999),\n",
      "doing so makes the model-averaged posterior distribution for the parameter\n",
      "inexact. An alternative is to use MCMC sampling to marginalize (average)\n",
      "over all of the models. However, this approach may be too computationally\n",
      "intensive.\n",
      "\n",
      "I do not provide examples of BMA in this book, for a couple of reasons.\n",
      "First, this book is meant to be an introduction to Bayesian statistics and\n",
      "MCMC estimation, and BMA is a fairly complex process. As stated above,\n",
      "choosing appropriate priors for all models in a class can be quite difficult, and\n",
      "BMA is computationally incredibly intensive. Some of the models discussed in\n",
      "the second part of this book, for example, may take on the order of hours to run\n",
      "for a single model. Attempting to perform BMA with 2J possible models may\n",
      "be unfeasible without substantial programming expertise and access to very\n",
      "powerful computer systems (e.g., like a Beowulf cluster). Second, and perhaps\n",
      "the more important reason I do not discuss BMA in this book, is that it seems\n",
      "in some ways contradictory to the basic approach to research taken by social\n",
      "scientists. That is, social science research typically begins with a theory and a\n",
      "set of hypotheses that guide us in the selection of variables and in the overall\n",
      "\n",
      "\n",
      "\n",
      "6.7 Exercises 163\n",
      "\n",
      "design of a model. The logic of BMA seems to run counter to this approach:\n",
      "BMA allows the data to tell us whether particular variables are important. Of\n",
      "course, we can incorporate our prior expectations for which variables/models\n",
      "are most important in a class via our prior specifications for the models, but\n",
      "if we do so, why not simply specify the model we are interested in testing in\n",
      "the first place! This strategy is akin to conducting BMA and assigning a prior\n",
      "probability of 0 for all models in the class of models we are evaluating except\n",
      "for the one that our theory leads us to believe is the appropriate one—and\n",
      "giving it a prior probability of 1.\n",
      "\n",
      "6.6 Conclusions\n",
      "\n",
      "In this chapter we have discussed (1) assessing the performance of MCMC\n",
      "algorithms and (2) assessing model fit. The assessment of MCMC algorithm\n",
      "performance is an important process, because, if the model has not converged\n",
      "and mixed well, the results cannot be used for making inference about model\n",
      "parameters. As we discussed, algorithm assessment requires a multifaceted\n",
      "approach, because no single approach constitutes a sufficient test for deter-\n",
      "mining whether the routine has converged and mixed well. Indeed, developing\n",
      "better ways of assessing performance, as well as developing better performing\n",
      "algorithms for specific problems has been a key area of research in MCMC\n",
      "methods over the last decade. In this chapter, we have covered the most com-\n",
      "monly used methods of assessing performance, and we have discussed several\n",
      "strategies for improving performance if it is found to be lacking.\n",
      "\n",
      "Once we have determined that our MCMC algorithm sampled thoroughly\n",
      "from the posterior distribution of interest, our next step should be to assess\n",
      "whether the model in fact fits the data well. In this chapter, we discussed\n",
      "several methods for assessing model fit and comparing models, but the primary\n",
      "focus was on posterior predictive checks. I limited the in-depth discussion to\n",
      "this approach, because posterior predictive simulation is easy to perform, and\n",
      "it is a highly flexible approach to assessing how well the model fits any feature\n",
      "of the data we are concerned about.\n",
      "\n",
      "Our coverage of these two topics has been relatively basic and limited\n",
      "largely to contrived examples. In the remaining chapters, we will employ a\n",
      "variety of strategies for assessing MCMC performance and evaluating model\n",
      "fit using common social science models and real social science data.\n",
      "\n",
      "6.7 Exercises\n",
      "\n",
      "1. How are residual analysis and posterior predictive simulation similar to\n",
      "one another?\n",
      "\n",
      "2. How can posterior predictive simulation be considered a method to com-\n",
      "pare models and not simply a method to evaluate a single model?\n",
      "\n",
      "\n",
      "\n",
      "164 6 Evaluating Markov Chain Monte Carlo (MCMC) Algorithms and Model Fit\n",
      "\n",
      "3. Reconduct the bivariate normal distribution example from the last chap-\n",
      "ter, using an MH algorithm to estimate the parameters, but select ex-\n",
      "tremely poor starting values for them. Run the algorithm five times with\n",
      "different sets of such poor starting values and compute the scale reduction\n",
      "factor R. Also construct trace plots. Does the R calculation lead you to\n",
      "a different conclusion concerning where to consider the burn-in to have\n",
      "ended than the trace plots?\n",
      "\n",
      "4. Reconduct the final normal distribution example again with an MH al-\n",
      "gorithm. Use good starting values this time, but run the algorithm at\n",
      "least seven times using different widths for the proposal distribution for\n",
      "the means. Can you develop a rule (for this model) for determining the\n",
      "appropriate proposal density width/variance given a desired acceptance\n",
      "rate?\n",
      "\n",
      "5. Generate 100 observations from a normal distribution with a mean of 5\n",
      "and variance of 4, as in the example. Now square these values, assume\n",
      "they come from a normal distribution, and estimate the parameters for\n",
      "this normal distribution using either a Gibbs sampler or an MH algorithm.\n",
      "Next, use posterior predictive simulation to determine whether the model\n",
      "fits the data. Does it?\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "\n",
      "The Linear Regression Model\n",
      "\n",
      "The first six chapters of this book have developed statistical modeling from a\n",
      "mathematical view of probability, introduced the Bayesian approach to data\n",
      "analysis, shown how to sample from Bayesian posterior distributions in or-\n",
      "der to make statistical inference, and demonstrated the basics for evaluating\n",
      "MCMC algorithm performance, evaluating model fit, and comparing models.\n",
      "So far, the models have been relatively simple; yet statistical modeling in so-\n",
      "cial science research generally involves singling-out one or more variables as\n",
      "outcomes for which we would like to evaluate the influence of a set of theo-\n",
      "retically important predictors and controls. In short, most social science data\n",
      "analysis involves some form of regression modeling. The remaining chapters of\n",
      "this book are geared to showing how to perform such analyses in a Bayesian\n",
      "framework. The basic models themselves should be quite familiar, and so the\n",
      "new content will be the Bayesian implementation of them. In the process of\n",
      "demonstrating how a Bayesian might approach these models, I will show some\n",
      "of the advantages of a Bayesian approach, including the ease of handling miss-\n",
      "ing data, the ability to make statistical inference for functions of parameters\n",
      "in models, and the breadth of possible methods for evaluating model fit.\n",
      "\n",
      "7.1 Development of the linear regression model\n",
      "\n",
      "The linear regression model—often called the “ordinary least squares (OLS)\n",
      "Regression Model”—is the fundamental model of all social scientific research.\n",
      "Although over the last few decades it has been supplanted by more compli-\n",
      "cated models, especially generalized linear models and models that can handle\n",
      "serial correlation between errors (e.g., fixed and random effects models), the\n",
      "basic assumption that an outcome variable y (or a function thereof) can be\n",
      "expressed as a linear combination of predictor variables and some stochastic\n",
      "error is the foundation of virtually all parametric models in social science\n",
      "today.\n",
      "\n",
      "\n",
      "\n",
      "166 7 The Linear Regression Model\n",
      "\n",
      "Because the linear regression model is the first model that is discussed\n",
      "in most graduate programs, I do not spend much time developing the model\n",
      "and theory (see Fox 1997 or Neter et al. 1996 for detailed discussion of the\n",
      "linear model). Instead, I primarily focus on demonstrating several MCMC\n",
      "approaches that may be used to estimate the parameters of the model.\n",
      "\n",
      "The OLS regression model is generally represented in one of two ways, one\n",
      "involving a direct specification of a distribution for the outcome variable y and\n",
      "the other involving a specification of a distribution for the error e. Under the\n",
      "classical—and typical social science—specification, we assume that yi is equal\n",
      "to a linear combination of a set of predictors, XTi β plus error ei, and that the\n",
      "error term is normally distributed with a mean of 0 and some variance, σ2e .\n",
      "In matrix form for an entire sample:\n",
      "\n",
      "Y = Xβ + e, (7.1)\n",
      "e ∼ N(0, σ2eIn), (7.2)\n",
      "\n",
      "where In is an n-dimensional identity matrix. Often, this term is omitted in\n",
      "the specification, but the distribution for the vector is technically multivariate\n",
      "normal with a 0 mean vector and an n × n covariance matrix. The diagonal\n",
      "elements of this matrix are all equal (representing the homoscedasticity of\n",
      "the errors assumption), and the off-diagonal elements of this matrix are 0\n",
      "(representing the independence of errors assumption).\n",
      "\n",
      "The normality assumption for the error term is not necessary for OLS\n",
      "estimation, but it is necessary for maximum likelihood estimation and for\n",
      "classical statistical tests on the parameters (the t-tests). Under the maximum\n",
      "likelihood approach, a normal error assumption yields the following likelihood\n",
      "function:\n",
      "\n",
      "L(β, σ2e |X, Y ) =\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "(2πσ2e)\n",
      "−1/2 exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2e\n",
      "\n",
      "(yi −XTi β)\n",
      "2\n",
      "\n",
      "}\n",
      "(7.3)\n",
      "\n",
      "= (2πσ2e)\n",
      "−n/2 exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2e\n",
      "\n",
      "(Y −Xβ)T (Y −Xβ)\n",
      "}\n",
      "\n",
      ". (7.4)\n",
      "\n",
      "The maximum likelihood solution to find the best estimates of β and σ2e (β̂\n",
      "and σ̂2e , respectively) and their standard errors can be found by taking the\n",
      "first and second derivatives of the log of this likelihood function following the\n",
      "steps discussed in Chapter 2. After derivation, we find:\n",
      "\n",
      "β̂ = (XT X)−1(XT Y ), (7.5)\n",
      "\n",
      "σ̂2e =\n",
      "1\n",
      "n\n",
      "\n",
      "eT e, (7.6)\n",
      "\n",
      "where e is the vector of errors obtained under β̂. The estimated standard errors\n",
      "for the parameters can be found by square-rooting the diagonal elements of\n",
      "\n",
      "\n",
      "\n",
      "7.1 Development of the linear regression model 167\n",
      "\n",
      "the asymptotic covariance matrix of the parameters, and the standard error\n",
      "for the error variance can be found as well1:\n",
      "\n",
      "ACOV(β̂) = σ̂2e(X\n",
      "T X)−1, (7.7)\n",
      "\n",
      "SE(σ̂2e) =\n",
      "(\n",
      "\n",
      "2σ̂2e\n",
      "n\n",
      "\n",
      ")1/2\n",
      ". (7.8)\n",
      "\n",
      "Rather than specifying a normal distribution on the error term, a Bayesian\n",
      "specification typically begins with a normality assumption on y | x (often with\n",
      "the conditioning suppressed): yi ∼ N(XTi β , σ\n",
      "\n",
      "2\n",
      "e). This specification yields the\n",
      "\n",
      "same likelihood function as the classical solution. What remains to make the\n",
      "model fully Bayesian is the specification of a prior for β and σ2e , and this is\n",
      "often done by specifying independent priors for each parameter. An improper\n",
      "uniform prior over the real line is often specified for the regression param-\n",
      "eters, while the common reference prior for the normal distribution model\n",
      "(as discussed in Chapter 3)—1/σ2e—is often specified for the error variance\n",
      "parameter. This yields a posterior distribution that appears as:\n",
      "\n",
      "P (β , σ2e |X, Y ) ∝ (σ\n",
      "2\n",
      "e)\n",
      "−(n/2+1) exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2e\n",
      "\n",
      "(Y −Xβ)T (Y −Xβ)\n",
      "}\n",
      "\n",
      ", (7.9)\n",
      "\n",
      "after simplification. Note that this posterior differs from the likelihood func-\n",
      "tion only in the leading exponent. The absolute value of the exponent for σ2e\n",
      "is increased from n/2 to n/2+1, which is an asymptotically irrelevant modifi-\n",
      "cation of the likelihood function. This result once again highlights that, with\n",
      "large samples, the prior may matter very little in affecting posterior inference.\n",
      "\n",
      "Given a posterior distribution, the goal of the Bayesian approach is to\n",
      "produce more than simply a point estimate for the parameter and its standard\n",
      "error, as we have discussed in the previous chapters. Thus, I now discuss\n",
      "several strategies for sampling from this posterior distribution. Although this\n",
      "model has been well studied, and the full conditionals are well known for the\n",
      "parameters, I will develop an MH algorithm along with two different Gibbs\n",
      "samplers.\n",
      "\n",
      "1 This standard error is almost never used, nor even reported in classical regression\n",
      "analysis, in part because normal theory tests based on it (e.g., t-tests like for the\n",
      "regression coefficients) are inappropriate. Variances are inverse gamma distributed\n",
      "and hence nonnegative. So, a typical t-test evaluating the statistical significance of\n",
      "a variance parameter is simply an unreasonable test. Unfortunately, in structural\n",
      "equation modeling, a setting in which standard errors of variance parameters are\n",
      "commonly reported, users often report the results of such t-tests and perpetuate\n",
      "the myth of testing the hypothesis that a variance is less than 0. This practice is\n",
      "especially common in modern latent growth modeling.\n",
      "\n",
      "\n",
      "\n",
      "168 7 The Linear Regression Model\n",
      "\n",
      "7.2 Sampling from the posterior distribution for the\n",
      "model parameters\n",
      "\n",
      "7.2.1 Sampling with an MH algorithm\n",
      "\n",
      "A variety of options exists for constructing an MH algorithm for the OLS\n",
      "regression model. Here, I develop a random walk metropolis algorithm in\n",
      "which each parameter is updated sequentially. The construction of an MH\n",
      "algorithm requires only that we be able to compute the unnormalized posterior\n",
      "density function (i.e., Equation 7.9); we do not need to derive any conditional\n",
      "distributions. Computing the unnormalized posterior density in this case is\n",
      "straightforward, and hence I skip directly to the R algorithm:\n",
      "\n",
      "#R program for MH sampling of parameters in linear regression\n",
      "\n",
      "#number of iterations\n",
      "\n",
      "m=200000\n",
      "\n",
      "#read in data, establish x and y matrices\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,2:10])\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,11])\n",
      "\n",
      "#establish parameter vectors, proposal scales and acceptance rates\n",
      "\n",
      "s2=matrix(1,m); b=matrix(0,m,9)\n",
      "\n",
      "bscale=sqrt(diag(vcov(lm(y~x-1))))*.5\n",
      "\n",
      "s2scale=sqrt(var(residuals(lm(y~x-1))*(2313-1)/(2313-9)))*.5\n",
      "\n",
      "accrate=matrix(0,m,9); s2accrate=matrix(0,m)\n",
      "\n",
      "#unnormalized posterior distribution function\n",
      "\n",
      "post<-function(x,y,b,s2)\n",
      "\n",
      "{return((-1157.5*log(s2)+(-.5/s2)*(t(y- x%*%b)%*%(y- x%*%b))))}\n",
      "\n",
      "#Begin MH Sampling\n",
      "\n",
      "for(i in 2:m){\n",
      "\n",
      "#temporarily set ‘new’ values of b\n",
      "\n",
      "b[i,]=b[i-1,]\n",
      "\n",
      "#update regression parameters\n",
      "\n",
      "for(j in 1:9){\n",
      "\n",
      "#generate candidate and assume it will be accepted...\n",
      "\n",
      "b[i,j]=b[i-1,j]+rnorm(1,mean=0, sd=bscale[j]); acc=1\n",
      "\n",
      "#...until it is evaluated for rejection\n",
      "\n",
      "if((post(x,y,b[i,],s2[i-1]) - post(x,y,b[i-1,],s2[i-1]))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{b[i,j]=b[i-1,j]; acc=0}\n",
      "\n",
      "accrate[i,j]=(accrate[i-1,j]*(i-1)+acc)/i\n",
      "\n",
      "}\n",
      "\n",
      "#update s2. generate candidate and assume accepted\n",
      "\n",
      "\n",
      "\n",
      "7.2 Sampling from the posterior distribution for the model parameters 169\n",
      "\n",
      "s2[i]=s2[i-1]+rnorm(1,mean=0, sd=s2scale); acc=1\n",
      "\n",
      "#...until it is evaluated for rejection\n",
      "\n",
      "if(s2[i]<0 ||\n",
      "\n",
      "(post(x,y,b[i,],s2[i]) - post(x,y,b[i,],s2[i-1]))\n",
      "\n",
      "<log(runif(1,min=0,max=1)))\n",
      "\n",
      "{s2[i]=s2[i-1]; acc=0}\n",
      "\n",
      "s2accrate[i]=(s2accrate[i-1]*(i-1)+acc)/i\n",
      "\n",
      "#write output to file and screen\n",
      "\n",
      "write(c(b[i,],s2[i],accrate[i,],s2accrate[i]),\n",
      "\n",
      "file=\"c:\\\\ols_examp.out\", append=T, ncol=20)\n",
      "\n",
      "if(i%%10==0){print(c(i,b[i,1],s2[i],accrate[i,1],s2accrate[i]))}\n",
      "\n",
      "}\n",
      "\n",
      "The program is fairly straightforward, and the comments within it clar-\n",
      "ify what each section does, so only a few comments are in order. First, the\n",
      "proposal densities for all parameters are normal with a mean of 0 and some\n",
      "standard deviation. This standard deviation is determined in the second sec-\n",
      "tion using the ML estimated standard errors of the parameters multiplied by\n",
      "a fraction (.5) in order to produce a reasonable acceptance rate (here, about\n",
      "35% for each parameter). If this model were not a standard model, I would\n",
      "have needed to establish some other method for obtaining the scale of the\n",
      "proposal densities, possibly through experimentation and/or approximation\n",
      "of the standard errors via ML estimation of similar models.\n",
      "\n",
      "Second, I created a function to evaluate the log posterior density, as I\n",
      "have done in previous programs. The log posterior is used, again, to prevent\n",
      "possible underflow problems that arise from attempting to exponentiate large\n",
      "negative numbers. Using the log posterior necessitates comparing the ratio\n",
      "R—which is now a subtraction—with the log of a U(0, 1) random draw.\n",
      "\n",
      "Third, the slope parameters in the program are updated sequentially, but\n",
      "at the beginning of each iteration, I set the current values of each parameter\n",
      "to the previous values first. I do this, because it allows me to send the poste-\n",
      "rior density function the current vector for the parameters without having to\n",
      "determine which sampled value of the parameter to send it (i.e., the current\n",
      "or previous, depending on which parameter is currently being updated, which\n",
      "have yet to be updated, and which have already been updated).\n",
      "\n",
      "Fourth, the program writes all parameters and acceptance rates to a file\n",
      "at each iteration. Thus, it is really unnecessary to store all parameters in a\n",
      "vector of length 200,000. Instead, we could use two distinct variables for each\n",
      "parameter (e.g., currentb and previousb). I demonstrate this approach in\n",
      "programs in subsequent chapters.\n",
      "\n",
      "7.2.2 Sampling the model parameters using Gibbs sampling\n",
      "\n",
      "The MH algorithm presented in the previous section is quite long and cum-\n",
      "bersome. Fortunately, the conditional posterior distributions for both the re-\n",
      "\n",
      "\n",
      "\n",
      "170 7 The Linear Regression Model\n",
      "\n",
      "gression parameters and the error variance parameter are well known, and so\n",
      "Gibbs sampling provides a more efficient alternative.\n",
      "\n",
      "The Full conditionals method\n",
      "\n",
      "There are at least two general ways to develop a Gibbs sampler for the linear\n",
      "regression model. The first method involves determining the full conditionals\n",
      "for (1) the regression parameter vector and (2) the error variance parameter.\n",
      "The full conditional posterior distribution for the error variance parameter is\n",
      "straightforward to derive from Equation 7.9. With β known/fixed, the condi-\n",
      "tional posterior for σ2 is:\n",
      "\n",
      "p(σ2|β, X, Y ) ∝ (σ2)−(n/2+1) exp\n",
      "{\n",
      "−\n",
      "\n",
      "eT e\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ", (7.10)\n",
      "\n",
      "where eT e is the sum of the square error terms under the given value for β.\n",
      "This conditional posterior is easily seen to be an inverse gamma distribution\n",
      "with parameters α = n/2 and β = eT e/2.\n",
      "\n",
      "The conditional distribution for β is slightly more difficult to derive, be-\n",
      "cause its derivation involves matrix algebra. However, the process is identical\n",
      "in concept to the approach we used in the univariate normal distribution\n",
      "example in Chapter 3. With σ2 fixed, we can focus exclusively on the expo-\n",
      "nential:\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2\n",
      "\n",
      "(Y −Xβ)T (Y −Xβ)\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "First, we can distribute the transpose in the first term in the numerator:\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2\n",
      "\n",
      "(Y T − βT XT )(Y −Xβ)\n",
      "}\n",
      "\n",
      "and then expand the multiplication:\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2\n",
      "\n",
      "[Y T Y − Y T Xβ − βT XT Y + βT XT Xβ]\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "The first term is constant with respect to β, and so it can be removed as a mul-\n",
      "tiplicative proportionality constant as we have done before (e.g., in Chapter 4).\n",
      "The middle two terms are identical to one another—one is just a transposed\n",
      "version of the other, but both are 1× 1; thus, one can be transposed, and the\n",
      "two may be grouped. After rearranging terms, we obtain:\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2\n",
      "\n",
      "[βT XT Xβ − 2βT XT Y ]\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "7.2 Sampling from the posterior distribution for the model parameters 171\n",
      "\n",
      "If we now multiply the numerator and denominator through by (XT X)−1\n",
      "\n",
      "appropriately,2 we get:\n",
      "\n",
      "exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2σ2(XT X)−1\n",
      "\n",
      "[βT β − 2βT (XT X)−1(XT Y )]\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      "At this point, we can complete the square in β, or we can simply recognize\n",
      "that doing so will yield a distribution for β that is normal with a mean equal\n",
      "to (XT X)−1(XT Y ) and a variance of σ2e(X\n",
      "\n",
      "T X)−1.\n",
      "With the conditionals derived, we can implement Gibbs sampling from\n",
      "\n",
      "the full conditionals by (1) establishing starting values for the parameters, (2)\n",
      "sampling β from its multivariate normal distribution with σ2e fixed, and (3)\n",
      "sampling σ2e from its inverse gamma distribution with β (and hence e) fixed.\n",
      "The following is an R program that implements this process:\n",
      "\n",
      "#R program for Gibbs sampling from full conditionals in OLS example\n",
      "\n",
      "#number of iterations\n",
      "\n",
      "m=5000\n",
      "\n",
      "#read only observations with complete information, n=2313\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,2:10])\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,11])\n",
      "\n",
      "#establish parameter vectors and constant quantities\n",
      "\n",
      "s2=matrix(1,m); b=matrix(0,m,9)\n",
      "\n",
      "xtxi=solve(t(x)%*%x)\n",
      "\n",
      "pars=coefficients(lm(y~x-1))\n",
      "\n",
      "#Gibbs sampling begins\n",
      "\n",
      "for(i in 2:m){\n",
      "\n",
      "#simulate beta from its multivariate normal conditional\n",
      "\n",
      "b[i,]=pars+t(rnorm(9,mean=0,sd=1))%*%chol(s2[i-1]*xtxi)\n",
      "\n",
      "#simulate sigma from its inverse gamma distribution\n",
      "\n",
      "s2[i]=1/rgamma(1,2313/2,.5*t(y-x%*%(b[i,]))%*%(y-x%*%(b[i,])))\n",
      "\n",
      "#write output to file and screen\n",
      "\n",
      "write(c(b[i,],s2[i]),file=\"c:\\\\ols_examp.out\", append=T, ncol=10)\n",
      "\n",
      "if(i%%50==0){print(c(i,b[i,1],s2[i]))}\n",
      "\n",
      "}\n",
      "\n",
      "This program is remarkably shorter than the MH algorithm and requires\n",
      "very little explanation. Before the Gibbs sampling begins, I compute the\n",
      "\n",
      "2 Technically, we cannot “divide” by (XT X)−1. Instead, we can multiply the nu-\n",
      "merator through by (XT X)(XT X)−1 and just distribute the inverse term. Then,\n",
      "the inverse variance of the distribution for β is (XT X)/σ2e . Inverting this quantity\n",
      "gives us the variance.\n",
      "\n",
      "\n",
      "\n",
      "172 7 The Linear Regression Model\n",
      "\n",
      "(XT X)−1 matrix and the OLS estimates (XT X)−1(XT Y ), both of which are\n",
      "used repeatedly and are unchanging. Once the Gibbs sampling loop begins,\n",
      "the entire vector of regression parameters is updated via a draw from the mul-\n",
      "tivariate normal distribution with the appropriate covariance matrix (which\n",
      "is conditional on the previous value of σ2e). This is conducted as discussed\n",
      "in previous chapters: We multiply a vector of independent N(0, 1) random\n",
      "draws by the Cholesky decomposition of the variance/covariance matrix of\n",
      "the parameters.\n",
      "\n",
      "Once the regression parameters have been updated, the error variance\n",
      "parameter is updated with a draw from the inverse gamma distribution. As\n",
      "discussed in previous chapters, R does not have an inverse gamma random\n",
      "number generator, and so a draw from the gamma distribution is obtained\n",
      "and then inverted.\n",
      "\n",
      "The Composition method\n",
      "\n",
      "A second, more efficient approach to Gibbs sampling in the linear regression\n",
      "model is to decompose the posterior distribution as the conditional distribu-\n",
      "tion for the regression parameters, given the error variance parameter, multi-\n",
      "plied by the marginal distribution for the error variance parameter:\n",
      "\n",
      "p(β , σ2e |X, Y ) = p(β|σ\n",
      "2\n",
      "e , X, Y )p(σ\n",
      "\n",
      "2\n",
      "e |X, Y ).\n",
      "\n",
      "This approach is the regression analog to the univariate normal distribu-\n",
      "tion example discussed in Chapter 4. Under this decomposition, the marginal\n",
      "distribution for σ2e is inverse gamma, and a sequence of draws from the ap-\n",
      "propriate inverse gamma distribution can be generated first. Then, given each\n",
      "sampled value of σ2e , the conditional distribution p(β|σ2e) is normal. Thus,\n",
      "once the sequence of draws for σ2e is obtained, one can simulate a sequence of\n",
      "draws for β from the appropriate normal distribution for each value of σ2e .\n",
      "\n",
      "The conditional distribution for β is the same as in the previous Gibbs\n",
      "sampling approach: normal with mean equal to the least squares solution\n",
      "(XT X)−1(XT Y ) and variance σ2e(X\n",
      "\n",
      "T X)−1. Thus, given a fixed value for σ2e ,\n",
      "we can simulate β directly from a normal distribution.\n",
      "\n",
      "The marginal distribution for σ2e can be derived by integrating the poste-\n",
      "rior density over the regression parameter vector [i.e., p(σ2) =\n",
      "\n",
      "∫\n",
      "p(β, σ2)dβ]\n",
      "\n",
      "and is shown in Gelman et al. (1995) to be a scaled inverse-chi-square distri-\n",
      "bution with parameters n − k and (1/(n − k))(Y − Xβ̂)T (Y − Xβ̂), where\n",
      "n is the sample size, k is the number of parameters in the β vector, and β̂\n",
      "is the least squares solution for β. The scaled inverse-chi-square distribution\n",
      "(with parameters v and s2) is a special case of the inverse gamma distribution\n",
      "with parameters α = v/2 and β = (v/2)s2. Thus, we can draw σ2e from its\n",
      "marginal distribution using an inverse gamma distribution with parameters\n",
      "α = (n − k)/2 and β = (1/2)eT e, where e is the vector of errors computed\n",
      "from the least squares solution.\n",
      "\n",
      "\n",
      "\n",
      "7.2 Sampling from the posterior distribution for the model parameters 173\n",
      "\n",
      "Below is an R program that performs Gibbs sampling using this approach:\n",
      "\n",
      "#R program for Gibbs sampling using composition method in OLS\n",
      "\n",
      "#number of iterations\n",
      "\n",
      "m=100000\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,2:10])\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\ols_examp.dat\")[1:2313,11:14])\n",
      "\n",
      "#establish parameter vectors and constant quantities\n",
      "\n",
      "s2=matrix(1,m); b=matrix(0,m,9)\n",
      "\n",
      "xtxi=solve(t(x)%*%x)\n",
      "\n",
      "pars=coefficients(lm(y[,1] ~ x-1))\n",
      "\n",
      "#simulate sigma from its inverse gamma marginal\n",
      "\n",
      "s2=1/rgamma(m,(2313-9)/2,.5*t(residuals(lm(y[,1] ~ x-1)))%*%\n",
      "\n",
      "residuals(lm(y[,1] ~ x-1)))\n",
      "\n",
      "#simulate beta vector from appropriate mvn\n",
      "\n",
      "for(i in 1:m)\n",
      "\n",
      "{\n",
      "\n",
      "b[i,]=pars+t(rnorm(9,mean=0,sd=1))%*%chol(s2[i]*xtxi)\n",
      "\n",
      "#write output to file and screen\n",
      "\n",
      "write(c(b[i,],s2[i]),\n",
      "\n",
      "file=\"c:\\\\ols_examp.out\", append=T, ncolumns=10)\n",
      "\n",
      "if(i%%50==0){print(c(i,b[i,1],s2[i]))}\n",
      "\n",
      "}\n",
      "\n",
      "This algorithm differs very little from the previous Gibbs sampler, but it is\n",
      "faster and more efficient, because σ2e is drawn all at once. Because every draw\n",
      "of σ2e is directly from its marginal posterior distribution, there is no burn-in\n",
      "to discard.\n",
      "\n",
      "One comment is in order concerning the two Gibbs samplers. They will\n",
      "both yield the same results, despite that one is sampling from the marginal\n",
      "distribution for σ2e , and the other is sampling from its conditional distribution\n",
      "(given β). This fact highlights a key feature of MCMC methods: They are a\n",
      "means of stochastic integration. Sampling values of σ2 in proportion to their\n",
      "probability conditioning on β is equivalent to analytically integrating β out of\n",
      "the joint distribution for σ2e and β to obtain the marginal distribution for σ\n",
      "\n",
      "2\n",
      "e .\n",
      "\n",
      "Regardless of the approach taken, although the conditional distribution for\n",
      "the regression vector is multivariate normal, the marginal distribution after\n",
      "integrating over σ2e is multivariate t. This is precisely why I said in Chapter 2\n",
      "that we often do not need to use the t and multivariate t distributions directly.\n",
      "\n",
      "\n",
      "\n",
      "174 7 The Linear Regression Model\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than\n",
      "others?\n",
      "\n",
      "Conventional wisdom holds that Southern (US) culture is very different from\n",
      "Northeastern, Western, and even Midwestern culture. This is perhaps one\n",
      "reason why so many regression models in social science, when they include\n",
      "region as a control variable, include only an indicator variable for “South.”\n",
      "Three important ways that the South is considered different from other regions\n",
      "in the US are (1) the pace of life is assumed to be slower than especially that\n",
      "of the Northeast and West, (2) people are assumed to be friendlier and more\n",
      "compassionate than persons from other regions, and (3) people are assumed\n",
      "to be poorer, less sophisticated, and perhaps even less intelligent than people\n",
      "from other regions.3\n",
      "\n",
      "In 2002 and 2004, the General Social Survey (GSS) conducted a special\n",
      "topics module on “altruism,” which allows us to examine item 2, the assump-\n",
      "tion that people in the South are more compassionate. For this example, I\n",
      "examine four outcome variables: (1) a summed scale of seven items that as-\n",
      "sesses individuals’ feelings of empathy, (2) a single item measuring tolerance\n",
      "of others, (3) a summed scale of four items assessing selfless attitude, and (4)\n",
      "a summed scale of actual altruistic behavior.\n",
      "\n",
      "I use age, sex, race, education, and family income as control variables.\n",
      "I measure region using a series of three dummy variables constructed from\n",
      "the region the respondent lived in at age 16 and the region the respondent\n",
      "currently lives in: continuous resident of the South, South in-migrant, South\n",
      "out-migrant.4 I choose this measure, because, although the culture of the\n",
      "South may be indigenous to that region, individuals internalize culture over\n",
      "time. Thus, individuals who move into a region do not immediately adopt the\n",
      "cultural practices of the region, and individuals who move out of a region do\n",
      "not immediately shed their previous cultural identity.\n",
      "\n",
      "The original sample size for the two years in which this topic module was\n",
      "used was n = 2, 712. Sixteen persons (.6%) were missing on either age or edu-\n",
      "cation; they were deleted. An additional 257 persons (9.5%) were missing on\n",
      "income only, and an additional 127 persons (4.7%) were missing on an out-\n",
      "\n",
      "3 These assumptions probably stem largely from the agricultural history of the\n",
      "South, where/when time was measured by daylight, rather than by a clock,\n",
      "most workers were physical (agricultural) laborers lacking in formal education\n",
      "beyond elementary school, and the closed nature of rural communities produced\n",
      "“gemeinschaft-like” relations that made empathy and compassion more viable\n",
      "than in urban communities in which individuals were less similar to one another\n",
      "(i.e., where “gesellschaft-like” relations predominated) and thus less able to em-\n",
      "pathize with the plight of others. My goal here is not to develop a sociological\n",
      "theory of South/non-South differences. See Durkheim (1984) for the foundation\n",
      "of such sociological theory.\n",
      "\n",
      "4 Of course, this measure is not perfect. Individuals may have moved many times;\n",
      "this measure only captures residence at two points in time.\n",
      "\n",
      "\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than others? 175\n",
      "\n",
      "come variable and possibly income. For the initial analyses, I use individuals\n",
      "with complete information only (85.8% of the original sample, not including\n",
      "those missing on age or education). In subsequent analyses, I will show how\n",
      "to incorporate the missing data in a Bayesian framework. Table 7.1 presents\n",
      "descriptive statistics for the variables in the models.\n",
      "\n",
      "Table 7.1. Descriptive statistics for variables in OLS regression example (2002 and\n",
      "2004 GSS data, n = 2, 696).\n",
      "\n",
      "Variable Mean(s.d.) or % Range % Missing Cronbach’s α\n",
      "\n",
      "Predictors\n",
      "Age 46.3(17.3) [16, 89] 0.0% NA\n",
      "Male 47.1% [0, 1] 0.0% NA\n",
      "White 80.1% [0, 1] 0.0% NA\n",
      "Yrs. Schooling 13.5(3.0) [0, 20] 0.0% NA\n",
      "Income ($1,000s) 49.8(31.6) [.5, 110] 11.2% NA\n",
      "Continuous South 28.0% [0, 1] 0.0% NA\n",
      "South Out-Migrant 3.9% [0, 1] 0.0% NA\n",
      "South In-Migrant 8.6% [0, 1] 0.0% NA\n",
      "\n",
      "Outcomes\n",
      "Empathy 21.0(4.8) [0, 28] 2.1% .73\n",
      "Tolerance 2.9(1.3) [0, 5] 1.7% NA\n",
      "Selflessness 10.1(2.4) [0, 16] 1.3% .55\n",
      "Altruistic Acts 13.3(6.5) [0, 50] 2.6% .71\n",
      "\n",
      "7.3.1 Results and comparison of the algorithms\n",
      "\n",
      "I ran each of these algorithms for at least 20,000 iterations. Figure 7.1 shows\n",
      "the scale reduction factor R̂ discussed in Chapter 6 computed for each regres-\n",
      "sion parameter from each of the three different MCMC algorithms. As the\n",
      "figure shows, the MH and Gibbs samplers converge very rapidly toward each\n",
      "other (and presumably on the posterior distribution).\n",
      "\n",
      "Trace plots suggested that all three algorithms converged within 1,000\n",
      "iterations. For example, Figure 7.2 shows trace plots for the error variance\n",
      "parameter for all three algorithms. As the plot shows, the three algorithms\n",
      "produce indistinguishable traces after only a few iterations.\n",
      "\n",
      "Based on these similarities, which of these three algorithms is preferrable to\n",
      "the others? One of the Gibbs samplers is probably the best approach because\n",
      "of the efficiency of Gibbs sampling over MH sampling. However, the answer to\n",
      "this question is case-specific and depends on a couple of considerations. First,\n",
      "\n",
      "\n",
      "\n",
      "176 7 The Linear Regression Model\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "6\n",
      "8\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "1\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".6\n",
      "\n",
      "2\n",
      ".2\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "2\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "2\n",
      "6\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "3\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "3\n",
      ".0\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "4\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".4\n",
      "\n",
      "1\n",
      ".8\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "5\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".4\n",
      "\n",
      "1\n",
      ".8\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "6\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "3\n",
      ".0\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "7\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".3\n",
      "\n",
      "1\n",
      ".6\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "8\n",
      ")\n",
      "\n",
      "0 5000 15000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "1\n",
      ".2\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "Iteration\n",
      "\n",
      "R\n",
      "(b\n",
      "\n",
      "9\n",
      ")\n",
      "\n",
      "Fig. 7.1. Scale reduction factors by iteration for all regression parameters.\n",
      "\n",
      "notice that as the programs became shorter and more efficient, they required\n",
      "more mathematical investment. The MH algorithm required us only to know\n",
      "the posterior density up to a normalizing constant. The full conditionals Gibbs\n",
      "sampling algorithm required us to derive the conditional distributions for both\n",
      "parameters. The composition method required us to find the full conditional\n",
      "distribution only for β, but it also required us to integrate the posterior density\n",
      "over β to obtain the marginal distribution for σ2e . Thus, when deciding which\n",
      "algorithm to use, a key consideration may be the difficulty of deriving the\n",
      "conditionals and/or marginal distributions. Second, if the data are such that\n",
      "it is difficult to obtain reasonable acceptance rates on all parameters using an\n",
      "MH algorithm, a Gibbs sampler may be preferred. In the example presented\n",
      "here, all three algorithms yielded comparable results, and so one of the Gibbs\n",
      "samplers is best, given its rapid convergence and mixing.\n",
      "\n",
      "\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than others? 177\n",
      "\n",
      "0 500 1000 1500 2000\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "3\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "σ\n",
      "e2\n",
      "\n",
      "MH\n",
      "Gibbs−−full conditional for σe\n",
      "\n",
      "2\n",
      "\n",
      "Gibbs−−marginal for σe\n",
      "2\n",
      "\n",
      "Fig. 7.2. Trace plot of error variance parameter in three different MCMC algo-\n",
      "rithms.\n",
      "\n",
      "I report the results from the full-conditionals Gibbs sampler. After discard-\n",
      "ing the first half of the samples, and keeping every 10th iteration to reduce\n",
      "autocorrelation (which is not really necessary in this problem), and repeating\n",
      "this process for each of the four outcomes, I computed the posterior means\n",
      "and standard deviations of all parameters using 1,000 samples. These results\n",
      "are displayed in Table 7.2. The results are consistent with the hypothesis that\n",
      "Southerners are nicer than others. Individuals who resided in the South at\n",
      "age 16 and at the present wave of the study have higher levels of empathy,\n",
      "evidence greater tolerance, are more selfless, and commit more altruistic acts\n",
      "than persons from or in other regions. There is some evidence, based on the\n",
      "sign and magnitude of most of the coefficients, that, if the culture of the\n",
      "South is nicer than that of other regions, the influence of the regional culture\n",
      "takes some time to fade, but perhaps not to adopt. Both emigrants from the\n",
      "South and immigrants to the South have higher scores on almost all outcome\n",
      "measures, net of the control variables in the model.\n",
      "\n",
      "Although I have reported the results with asterisks, as is common in clas-\n",
      "sical statistical research, there is no need to do this in a Bayesian setting.\n",
      "\n",
      "\n",
      "\n",
      "178 7 The Linear Regression Model\n",
      "\n",
      "Instead, we could directly discuss the probabilities that say, immigrants to\n",
      "the South are nicer than persons who have never lived in the South. For ex-\n",
      "ample, the probability that immigrants have greater empathy than persons\n",
      "who have never lived in the South is .821. This probability was obtained\n",
      "simply by computing the proportion of sampled regression parameter values\n",
      "that exceed 0. Thus, although this parameter was not considered “significant”\n",
      "from a classical standpoint, the Bayesian approach suggests that immigrants\n",
      "are quite likely to be nicer (by this measure) than persons who have never\n",
      "lived in the South.\n",
      "\n",
      "The bottom of the table reports interval estimates for the model R2. Given\n",
      "that we have complete distributions for the regression parameters and error\n",
      "variance parameter, and not simply point estimates, we can compute the R2\n",
      "\n",
      "implied by each sample of the parameters, thereby obtaining a sample from\n",
      "the posterior distribution for the model R2. Once this sample is sorted, we\n",
      "can then select the 2.5th and 97.5th percentile values as the end points for a\n",
      "95% interval estimate.\n",
      "\n",
      "Table 7.2. Results of linear regression of measures of “niceness” on three measures\n",
      "of region.\n",
      "\n",
      "Outcome\n",
      "\n",
      "Variable Empathy Tolerance Selflessness Altruistic Acts\n",
      "\n",
      "Intercept 19.62(.57)*** 3.16(.15)*** 8.71(.30)*** 9.17(.76)***\n",
      "Age 0.018(.006)** −0.008(.002)*** 0.01(.003)*** −0.03(.01)***\n",
      "Male −2.42(.19)*** −0.35(.05)*** −0.87(.10)*** 0.32(.26)\n",
      "White 0.55(.26)* 0.09(.07)# 0.17(.13)# 0.01(.33)\n",
      "Yrs. Schooling 0.06(.04)* 0.004(.01) 0.07(.02)*** 0.32(.05)***\n",
      "Income ($1,000s) 0.003(.003) 0.0006(.0009) 0.001(.002) 0.02(.005)***\n",
      "Continuous South 0.65(.21)** 0.22(.06)*** 0.25(.11)* 0.41(.30)#\n",
      "South Out-Migrant 0.39(.50) 0.11(.14) −0.33(.25)# 0.01(.70)\n",
      "South In-Migrant 0.34(.35) 0.24(.10)** 0.17(.17) −0.01(.45)p\n",
      "\n",
      "σ2e 4.61 1.22 2.31 6.23\n",
      "R2 .07[.066,.072] .03[.029,.036] .05[.047,.054] .055[.051,.057]\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The Bayesian p-values are based\n",
      "on one-sided tail probabilities that the parameter exceeds 0 truncated to the classical\n",
      "cut-points of #p < .1, *p < .05, **p < .01, ***p < .001. The R2 reported are based\n",
      "on the posterior mean estimates for the parameters; a 95% interval estimate is also\n",
      "reported.\n",
      "\n",
      "7.3.2 Model evaluation\n",
      "\n",
      "Although I modeled the tolerance outcome variable as if it were a continuous\n",
      "measure, in fact it is a single ordinal item with five categories. Thus, we should\n",
      "evaluate the fit of the model and verify that it fits the data. The R2 presented\n",
      "in the table is quite low (3%), which suggests poor fit, but R2 reveals only one\n",
      "\n",
      "\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than others? 179\n",
      "\n",
      "aspect of how well the model fits the data in this problem. Some remaining\n",
      "questions include: (1) How well does the model fit various aspects of the\n",
      "data? and (2) Are there particularly poorly fitted cases that contribute to\n",
      "the model’s poor fit? As discussed in earlier chapters, a Bayesian approach\n",
      "allows us many more options to address these questions than the classical\n",
      "approach, and it allows us to take into account both sampling and parametric\n",
      "uncertainty in doing so.\n",
      "\n",
      "In order to begin to address these questions, I used posterior predictive\n",
      "simulation. For each person in the sample, I generated 1,000 posterior predic-\n",
      "tive cases with the same covariate structure, using every sampled value of β\n",
      "and σ2e . This approach requires two steps:\n",
      "\n",
      "1. Compute ŷji = x\n",
      "T\n",
      "i β\n",
      "\n",
      "j , ∀ individuals, i, and all sampled values of the re-\n",
      "gression coefficients βj .\n",
      "\n",
      "2. Simulate a predictive score yji = ŷ\n",
      "j\n",
      "i + N(0, σ\n",
      "\n",
      "2(j)\n",
      "e ), for each case.\n",
      "\n",
      "If you visualize the original data set, with 2,313 rows representing the\n",
      "original sample members with complete data and 10 columns representing the\n",
      "9 predictors plus the outcome variable, these predictive cases can be viewed as\n",
      "expanding the column space of the data array by an additional 1,000 columns.\n",
      "For any row, i, each new column (j) represents a future observation with\n",
      "outcome yj as a function of the jth sampled value of β and σ2e , and the\n",
      "covariate profile of the ith person. Thus, each new complete column can be\n",
      "considered a replicated sample, whereas the new 1,000 elements of each row\n",
      "can be considered replications of individual i under different parameter values.\n",
      "This means we can examine how well a model fits various features of the\n",
      "original sample (like the ratio of the mean of y to its median) by examining\n",
      "the column-wise collection of these values in repeated samples. Plus, we can\n",
      "examine how well a model fits particular cases by examining the row-wise\n",
      "distributions for each observation in the sample.\n",
      "\n",
      "For this particular example, I considered three posterior predictive dis-\n",
      "tribution features of the first type—sample level features. First, I examined\n",
      "the extent to which the distribution of replicated values of y matched the\n",
      "distribution of the original y. The top plot in Figure 7.3 shows this result.\n",
      "In general, the distribution of replicated data appears to match that of the\n",
      "observed data fairly well. Second, I computed the ratio of the mean of y to\n",
      "its median. The second plot in the figure shows that the distribution of the\n",
      "ratio of the mean to the median in the replicated samples is centered over 1\n",
      "(as it should be, given that the error distribution is assumed normal), while\n",
      "the observed ratio was .95. As the plot shows, the Bayesian p-value associated\n",
      "with a test of this statistic (the mean/median ratio) would be 0 (or 1), which\n",
      "indicates very poor fit of the model to the data. Third I computed the range of\n",
      "y from each posterior predictive sample. The third plot in the figure reinforces\n",
      "that the model has poor fit. This plot shows the distribution of the range of\n",
      "values in the replicated data with a vertical reference line for the range in the\n",
      "\n",
      "\n",
      "\n",
      "180 7 The Linear Regression Model\n",
      "\n",
      "observed data. Again, the posterior predictive distribution does not overlap\n",
      "the observed value, which indicates poor fit.\n",
      "\n",
      "−2 0 2 4 6 8 10\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      "\n",
      "y (−−−); yrep (solid line)\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0.95 1.00 1.05\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "\n",
      "yrep Mean/Median\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "2 4 6 8 10 12\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "yrep Range\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 7.3. Posterior predictive distributions: (1) The distribution of all replicated\n",
      "samples and the distribution of the original data; (2) the distribution of the ratio\n",
      "of the sample mean of y to the median, with the observed ratio superimposed;\n",
      "and (3) the distribution of the ranges of predicted values with the observed range\n",
      "superimposed.\n",
      "\n",
      "In addition to these tests of the overall fit of the model, as I said above,\n",
      "we can examine how well a model fits any particular case by examining the\n",
      "row-wise collection of new observations and comparing it with the observed\n",
      "value of y for each individual. One way to identify potential outliers is to\n",
      "compute the proportion of future observations for individual i that would\n",
      "have a higher (or lower) value on the response variable. Observed cases in\n",
      "\n",
      "\n",
      "\n",
      "7.3 Example: Are people in the South “nicer” than others? 181\n",
      "\n",
      "which the posterior predictive distribution is extreme (either a high or low\n",
      "proportion of posterior predictive cases exceed the observed value) indicate\n",
      "the model tends to poorly fit the outcome for that case. For instance, person\n",
      "1,452 had an observed value of 5 for the outcome variable, but .8% of the\n",
      "posterior predictive cases for this individual had a predicted value of y that\n",
      "exceeded this value (and 99.2% of the posterior predictive cases were smaller\n",
      "than this value), which suggests that this case is not fit well by the model.\n",
      "Similarly, person 1,997 had the largest proportion of future observations with\n",
      "a predicted tolerance level exceeding her observed tolerance (100.0%). She was\n",
      "a 31-year-old nonwhite female with 12 years of schooling and a family income\n",
      "of $4,500 who is a continuous resident of the South. Her expected tolerance\n",
      "was much higher than what was observed. Figure 7.4 shows these two extreme\n",
      "cases. As the figure shows, the posterior predictive distributions for these two\n",
      "persons are not centered over the true values of y for them, which indicates\n",
      "poor fit of these cases.\n",
      "\n",
      "−2 0 2 4 6 8\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      "\n",
      "y1452\n",
      "rep\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−2 0 2 4 6 8\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      "\n",
      "y1997\n",
      "rep\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 7.4. Posterior predictive distributions (solid lines) and observed values (dashed\n",
      "vertical lines) for persons 1,452 and 1,997.\n",
      "\n",
      "\n",
      "\n",
      "182 7 The Linear Regression Model\n",
      "\n",
      "To determine whether such persons are outliers or whether the model\n",
      "simply is not fitting particular covariates well, we can examine collections\n",
      "of possible outliers and determine whether they share similar characteristics.\n",
      "For example, in these data, although there is only one person (1,452) whose\n",
      "tolerance is substantially underpredicted, there are 31 observations for whom\n",
      "the proportion of replicated outcomes exceeding the observed outcome is at\n",
      "least 99%. If these individuals are substantially similar to one another on\n",
      "one or more covariates, then the implication would be that the model does\n",
      "not fit those covariates well. Figure 7.5 shows the sample distribution of age,\n",
      "education, and income with the distribution of these variables for the 31 po-\n",
      "tential outlying cases superimposed. The figure shows quite clearly that these\n",
      "individuals do not appear to differ from the overall sample on these three\n",
      "characteristics. Thus, it is reasonable to conclude that these cases are prob-\n",
      "ably simply outliers, at least in the context of the current model. We should\n",
      "probably consider including additional covariates that may help explain their\n",
      "low tolerance levels.\n",
      "\n",
      "All in all, the posterior predictive checks suggest that the OLS regression\n",
      "model does not fit the tolerance item well, and so perhaps an alternative model\n",
      "should be considered for this item. Specifically, we may consider treating this\n",
      "outcome as an ordinal measure and model it using a generalized linear model\n",
      "as discussed in the next chapter. If we were to estimate a new model, we\n",
      "could apply similar tests and decide which model fits better either formally\n",
      "or informally. For example, formally, we could begin with a test of the pro-\n",
      "portion of values of R2 for the new model exceeded the maximum value of\n",
      "R2 for the first model. This proportion would be a measure of the probability\n",
      "that the new model fit the data better than the original model and would\n",
      "therefore serve as a basis for deciding which model to prefer. Note that this\n",
      "type of comparison does not require that the models be nested, because, from\n",
      "a Bayesian perspective, any two probability distributions can be compared if\n",
      "it makes sense to compare them.\n",
      "\n",
      "7.4 Incorporating missing data\n",
      "\n",
      "In the previous sections I discussed the results of models in which the cases\n",
      "with missing data were simply listwise deleted, leaving us with 85.8% of the\n",
      "original sample (14.2% missing). Conventional wisdom in the social sciences\n",
      "claims that this amount of missing data is unacceptable, and so some alterna-\n",
      "tive to listwise deletion of observations with missing data should be considered.\n",
      "The Bayesian paradigm using MCMC simulation offers some relatively simple,\n",
      "yet appropriate methods for handling missing data.\n",
      "\n",
      "7.4.1 Types of missingness\n",
      "\n",
      "Before determining how to handle missing data, we should decide whether\n",
      "the missing data mechanism—that is, the process that generated the missing\n",
      "\n",
      "\n",
      "\n",
      "7.4 Incorporating missing data 183\n",
      "\n",
      "20 40 60 80 100\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Age\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0 5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "\n",
      "Years of Schooling\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy Whole Sample\n",
      "Possible Outliers\n",
      "\n",
      "−20 0 20 40 60 80 100 120\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "Income\n",
      "\n",
      "fr\n",
      "e\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 7.5. Distribution of age, education, and income for entire sample and for 31\n",
      "potential outliers identified from posterior predictive simulation (reference lines at\n",
      "means).\n",
      "\n",
      "data—is ignorable or nonignorable. Little and Rubin (1987) developed a now\n",
      "classic schema for understanding the implications of missing data:\n",
      "\n",
      "• Data missing on Y are observed at random (OAR) if missingness on Y is\n",
      "not a function of X. Phrased another way, if X determines missingness on\n",
      "Y , the data are not OAR.\n",
      "\n",
      "• Data missing on Y are missing at random (MAR) if missingness on Y is\n",
      "not a function of Y . Phrased another way, if Y determines missingness on\n",
      "Y , the data are not MAR.\n",
      "\n",
      "• Data are missing completely at random (MCAR) if missingness on Y is\n",
      "unrelated to X or Y . In other words MCAR = OAR + MAR.\n",
      "\n",
      "\n",
      "\n",
      "184 7 The Linear Regression Model\n",
      "\n",
      "If the data are MCAR or at least MAR, then the missing data mechanism is\n",
      "considered “ignorable.” Otherwise, the missing data mechanism is considered\n",
      "“nonignorable.”\n",
      "\n",
      "To make these ideas concrete, suppose we are examining the effect of ed-\n",
      "ucation on income. If missingness on income is a function of education (e.g.,\n",
      "highly educated individuals do not report their incomes), then the data are\n",
      "not OAR. If missingness on income is a function of income (e.g., persons with\n",
      "high income do not report their income), on the other hand, then the missing\n",
      "data are not MAR, and the missing data mechanism is nonignorable.\n",
      "\n",
      "Why are missing data that are MAR but not OAR ignorable? When miss-\n",
      "ing data are not OAR, the fundamental pattern between the predictors and\n",
      "the outcome is not altered. Instead, the result is much the same as simply\n",
      "having a sample in which some values of x are not represented. Figure 7.6\n",
      "illustrates. I simulated two sets of 10,000 N(0, 1) observations. The first set\n",
      "I considered to be the X variable; the second set I considered to be error\n",
      "(e). I then constructed a vector Y = 5 + 3X + 10e, which implies that Y\n",
      "has a linear relationship with X as regression modeling assumes. The up-\n",
      "per left figure shows a subset of these 10,000 observations along with the\n",
      "regression line through the points. Based on the simulation parameters, the\n",
      "regression coefficients should be 5 and 3, and the R2 for this model should be\n",
      "R2 = 1−[var(10e)/(var(5+3X+10e)] = 1−(100/109) = .083. This particular\n",
      "sample yielded 4.98 and 3.06 for the coefficients, with an R2 of .081.\n",
      "\n",
      "The upper right plot shows a subset of the data for which cases with\n",
      "values of x < 0 have been eliminated from the data, meaning the y data are\n",
      "not OAR. As the figure shows, the new regresssion line is almost identical to\n",
      "the original (indeed, it cannot be distinguished from the original line), with\n",
      "coefficients equal to 4.99 and 3.07. The R2 was substantially less, however, at\n",
      ".032, reflecting the added uncertainty that results from the loss of information\n",
      "on the left half of the distribution. The lower left plot, in contrast, shows the\n",
      "results when observations with values of y < 5 are eliminated, meaning that\n",
      "the data are not MAR. In this case, the regression coefficients are substantially\n",
      "affected (at 13.1 and 1.3; R2 = .04). The bias occurs because the distribution\n",
      "for y has been systematically truncated, distorting the relationship between\n",
      "x and y.\n",
      "\n",
      "There are a number of ways in which we may encounter missing data in\n",
      "social science research, not all of which produce missing data that is nonignor-\n",
      "able. For example, in a panel study, some individuals may not be followed up\n",
      "by design. Similarly, some individuals may be lost-at-follow-up because they\n",
      "moved between waves of the study for reasons unrelated to the outcome vari-\n",
      "able of interest. In these cases, the missing data mechanism is ignorable, and\n",
      "listwise deletion of the observations with missing data introduces no biases\n",
      "whatsoever. In most cases, however, it is unclear whether the missing data\n",
      "mechanism is ignorable. Item nonresponse—a common source of missingness\n",
      "in social surveys—may be random, or it may be attributable to the fact that\n",
      "the respondent would have selected a value of the response variable that would\n",
      "\n",
      "\n",
      "\n",
      "7.4 Incorporating missing data 185\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      "x\n",
      "\n",
      "y ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      "x (y missing if x<0)\n",
      "\n",
      "y\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "4\n",
      "0\n",
      "\n",
      "x (y missing if y<5)\n",
      "\n",
      "y\n",
      "\n",
      "●\n",
      "\n",
      "Design  : β0=5.00   β1=3.00\n",
      "\n",
      "1st Case: β0=4.98   β1=3.06\n",
      "\n",
      "2nd Case: β0=4.99   β1=3.07\n",
      "\n",
      "3rd Case: β0=13.13   β1=1.30\n",
      "\n",
      "Fig. 7.6. Examples of types of missingness: No missing, missing are not OAR, and\n",
      "missing are not MAR.\n",
      "\n",
      "have been extreme and potentially undesirable. For instance, consider a highly\n",
      "prejudiced white male respondent answering questions regarding prejudicial\n",
      "attitudes asked by a black female interviewer. In cases like this one, in which\n",
      "the missing response is due to the value of the response, the missing data\n",
      "mechanism is nonignorable.\n",
      "\n",
      "How do we determine whether the missing data mechanism is ignorable?\n",
      "Although it is common practice in social science literature to report whether\n",
      "various x variables predict missingness on another x or y, formally this ap-\n",
      "proach only tells us whether the data are OAR. There is no definitive way\n",
      "to determine whether data are MAR, because, of course, the data we need\n",
      "\n",
      "\n",
      "\n",
      "186 7 The Linear Regression Model\n",
      "\n",
      "to make this determination are missing. Thus, ultimately, we must rely on\n",
      "some sort of prior, out-of-sample, knowledge to decide whether the missing\n",
      "data mechanism is ignorable or nonignorable. For example, in the GSS data,\n",
      "there were 302 persons missing on income. Suppose income had been our out-\n",
      "come variable, and we estimated a probit model with education predicting\n",
      "missingness on the income item. The result of that analysis (not presented\n",
      "here) showed that education was strongly related to missingness on income,\n",
      "such that more educated persons were less likely to be missing on income\n",
      "(conversely, less educated persons are more likely to be missing on income).\n",
      "This result simply tells us that income is not OAR. However, if we have prior\n",
      "(out-of-sample) information indicating that more educated individuals tend\n",
      "to have higher incomes, then we might infer that the missing data on income\n",
      "is not MAR: Lower income predicts missingness on income.\n",
      "\n",
      "If we decide that the missing data mechanism is ignorable, then the only\n",
      "cost of listwise deletion—that is, deletion of an entire case when one or more\n",
      "variable is missing—is the reduction in sample size and the corresponding\n",
      "inefficiency in estimating standard errors (or posterior standard deviations)\n",
      "of parameters. Indeed, if we attempt to perform some sort of imputation in\n",
      "this setting, we are more likely to produce biases or underestimate uncertainty\n",
      "in the parameters (see Allison 2003).\n",
      "\n",
      "If, on the other hand, the missing data mechanism is nonignorable, list-\n",
      "wise deletion will produce biased parameter estimates, but so will most forms\n",
      "of simple imputation or modeling strategies that fail to accurately capture\n",
      "the process that generates the missing data. For example, so-called “full-\n",
      "information maximum likelihood” (FIML) estimation (sometimes also called\n",
      "“direct maximum likelihood estimation”), estimation based on the EM algo-\n",
      "rithm, multiple imputation methods, and other popular contemporary meth-\n",
      "ods do not resolve the nonignorability of the missing data mechanism. Instead,\n",
      "in their typical application, these approaches simply assume the missing data\n",
      "follow the same distribution (or pattern) as the observed data. Therefore,\n",
      "these methods are ultimately of little use when the missing data are not\n",
      "MAR, despite their growth in popularity.\n",
      "\n",
      "7.4.2 A generic Bayesian approach when data are MAR: The\n",
      "“niceness” example revisited\n",
      "\n",
      "The Bayesian paradigm (coupled with MCMC methods) offers a simple ap-\n",
      "proach to handling missing data when the missing data mechanism is ignor-\n",
      "able, one that is generally easier to implement than (1) switching software\n",
      "or estimators when the package you are using does not have a missing data\n",
      "method (e.g., switching from one structural equation modeling package to an-\n",
      "other that can handle missing data) or (2) performing multistep procedures\n",
      "to compute parameters and adjust standard errors for missing data models\n",
      "(e.g., using a secondary package to adjust standard errors when performing\n",
      "multiple imputation).\n",
      "\n",
      "\n",
      "\n",
      "7.4 Incorporating missing data 187\n",
      "\n",
      "As an example demonstrating the ease with which the Gibbs sampler can\n",
      "handle missing data, I return to the first outcome (empathy) in the previous\n",
      "example. Recall that the original sample size was 2,696, but that a number\n",
      "of persons were missing on either income or one of the outcome variables,\n",
      "leaving us with 2,313 persons with complete information. We will continue to\n",
      "ignore persons missing on income for the moment, but suppose we wished to\n",
      "include persons missing on the empathy item. There were 34 such persons in\n",
      "the sample. Focusing on this outcome only allows us to incorporate individuals\n",
      "who were missing on the other outcomes, and so our sample size is 2,394.\n",
      "\n",
      "Let Y be the observed outcome data, and let Z be the missing outcome\n",
      "data. In the OLS regression model without missing data, the posterior dis-\n",
      "tribution we sought included the regression parameter vector β and the error\n",
      "variance parameter σ2e . However, when we have missing data, the missing\n",
      "data can also be treated as unknown, and hence, the posterior distribution\n",
      "may involve Z as well. Thus (suppressing dependence on X), our posterior\n",
      "becomes:\n",
      "\n",
      "p(β, σ2e , Z|Y ) ∝ p(Y |β, σ\n",
      "2\n",
      "e , Z)p(β, σ\n",
      "\n",
      "2\n",
      "e , Z).\n",
      "\n",
      "The latter term in this posterior is a joint prior probability for the missing\n",
      "data, the regression parameters, and the error variance, and it can be decom-\n",
      "posed as:\n",
      "\n",
      "p(β, σ2e , Z) ∝ p(Z|β, σ\n",
      "2\n",
      "e)p(β, σ\n",
      "\n",
      "2\n",
      "e).\n",
      "\n",
      "Furthermore, just as the observed cases are assumed to be independent, we\n",
      "may also assume that the observed and missing cases are independent, and\n",
      "so the first term in the posterior reduces to p(Y |β, σ2e). The full posterior\n",
      "distribution, then, is:\n",
      "\n",
      "p(β, σ2e , Z|Y ) ∝ p(Y |β, σ\n",
      "2\n",
      "e)p(Z|β, σ\n",
      "\n",
      "2\n",
      "e)p(β, σ\n",
      "\n",
      "2\n",
      "e),\n",
      "\n",
      "where the latter term is simply the joint prior for the regression parameters\n",
      "and error variance. Let’s assume the same reference prior as we did in the\n",
      "example with complete data (1/σ2e).\n",
      "\n",
      "A Gibbs sampler for sampling from this posterior distribution simply in-\n",
      "volves sequentially sampling from the conditional posterior distributions for\n",
      "β, σ2e , and Z. The conditional posterior distribution for β|Z, σ2e , Y is straight-\n",
      "forward, given values for Z. Indeed, assume we have values for Z, so that\n",
      "Y ∗ = [Y,Z] is a set of complete data. In that case:\n",
      "\n",
      "(β| Y ∗, σ2e) ∼ N((X\n",
      "T X)−1(XT Y ∗), σ2e(X\n",
      "\n",
      "T X)−1),\n",
      "\n",
      "which is a result identical to that we obtained previously, only now we have\n",
      "incorporated the missing data.\n",
      "\n",
      "Similarly, given Y ∗, the conditional posterior for the error variance is:\n",
      "\n",
      "\n",
      "\n",
      "188 7 The Linear Regression Model\n",
      "\n",
      "(σ2e | Y\n",
      "∗, β) ∼ IG(α = n∗/2, β = e∗T e∗/2),\n",
      "\n",
      "where n∗ is the sample size, and e∗T e∗ is the error sum of squares, after\n",
      "including the missing data (e∗ = Y ∗ −Xβ).\n",
      "\n",
      "What remains is the specification of the conditional posterior for the miss-\n",
      "ing data Z, so that we can form Y ∗. Given that Y and Z are independent,\n",
      "that each of the Z are independent, and given that we have assumed the same\n",
      "model for Z as for Y (that the missing data mechanism is ignorable; the data\n",
      "are MAR), we have no information regarding the posterior distribution for\n",
      "each Z beyond what is known about β and σ2e from the observed data Y .\n",
      "Therefore, the posterior distribution for each Z is simply normal with a mean\n",
      "of XTi β and variance σ\n",
      "\n",
      "2\n",
      "e .\n",
      "\n",
      "All in all, then, incorporating the missing data involves the addition of\n",
      "only a couple of lines of programming code to our original (full conditional)\n",
      "Gibbs sampler. Below is some replacement code for the “meat” of the Gibbs\n",
      "sampler. The original variable, y, is the outcome vector with missing data\n",
      "coded 999. First, ystar is set equal to the value for y, but for each person\n",
      "with missing data, y is replaced with a normal random variable with a mean\n",
      "of XTi β and a variance of σ\n",
      "\n",
      "2\n",
      "e , using current values of those parameters.\n",
      "\n",
      "The only two additional changes include: (1) Given that the outcome vec-\n",
      "tor changes at each iteration due to the simulation of the missing data, the\n",
      "mean for the conditional posterior for β must be computed at every itera-\n",
      "tion (i.e., this is the line: b[i,]<-coefficients(lm(ystar x-1))...); and\n",
      "(2) The second parameter of the inverse gamma distribution for σ2e uses the\n",
      "complete data after y∗ has been updated.\n",
      "\n",
      "...\n",
      "\n",
      "for(i in 2:m){\n",
      "\n",
      "#simulate missing data\n",
      "\n",
      "ystar=y\n",
      "\n",
      "ystar[y==999]=rnorm(length(ystar[y==999]),\n",
      "\n",
      "mean=x[y==999,]%*%(b[i-1,]), sd=sqrt(s2[i-1]))\n",
      "\n",
      "#simulate beta from its multivariate normal conditional\n",
      "\n",
      "b[i,]<-coefficients(lm(ystar ~ x-1))+\n",
      "\n",
      "t(rnorm(9,mean=0,sd=1))%*%chol(s2[i-1]*xtxi)\n",
      "\n",
      "#simulate sigma from its inverse gamma distribution\n",
      "\n",
      "s2[i]<-1/rgamma(1,length(y)/2,\n",
      "\n",
      ".5*t(ystar-x%*%(b[i,]))%*%(ystar-x%*%(b[i,])))\n",
      "\n",
      "...\n",
      "\n",
      "I reran the Gibbs sampler with these modifications and obtained virtually\n",
      "identical results to those obtained from listwise deletion. In fact, a classical\n",
      "regression of the original parameter means and posterior standard deviations\n",
      "on the parameter means and standard deviations obtained using the modified\n",
      "\n",
      "\n",
      "\n",
      "7.4 Incorporating missing data 189\n",
      "\n",
      "algorithm yielded an intercept of .02 (s.e. = .02) and slope of .997 (s.e. =\n",
      ".002) (R2 = .999) suggesting nearly one-to-one correspondence between sets of\n",
      "estimates. This result is not surprising, given that we only had 34 observations\n",
      "for whom missing data were present, and we assumed that missing cases were\n",
      "similar to observed cases (i.e., they followed the same model).\n",
      "\n",
      "Returning to the current example, let’s now also include the observations\n",
      "missing on income. In the original data, 302 persons were missing on income.\n",
      "If we include individuals who are missing on income and missing on the em-\n",
      "pathy outcome variable, we have the complete sample of 2,696. Incorporating\n",
      "missingness on income is perhaps even more straightforward than incorpo-\n",
      "rating missingness on the outcome, because income is an exogenous variable\n",
      "in the model. Thus, a simple approach to handling missingness on income is\n",
      "to assume that the missing income data are MAR. Under this assumption,\n",
      "we can simply add a step to the Gibbs sampler in which the missing values\n",
      "of income are replaced with simulated values. With no conditional model for\n",
      "income, and under the MAR assumption, it is reasonable to simulate values\n",
      "for income based on the distribution of income among observed cases. If we\n",
      "assume that income is normally distributed (which may not necessarily be a\n",
      "good assumption), then we can simulate values of income for those missing on\n",
      "this variable using the mean and standard deviation of income among those\n",
      "observed on income. This means we only need to make the following changes\n",
      "to the Gibbs sampler:\n",
      "\n",
      "...\n",
      "\n",
      "for(i in 2:m){\n",
      "\n",
      "xstar=x\n",
      "\n",
      "xstar[x[,6]==999,6]=rnorm(length(xstar[x[,6]==999,6]),\n",
      "\n",
      "mean=mean(x[x[,6]!=999,6]),sd=sd(x[x[,6]!=999,6]))\n",
      "\n",
      "ystar=y\n",
      "\n",
      "ystar[y==999]=rnorm(length(ystar[y==999]),\n",
      "\n",
      "mean=x[y==999,]%*%(b[i-1,]), sd=sqrt(s2[i-1]))\n",
      "\n",
      "#simulate beta from its multivariate normal conditional\n",
      "\n",
      "b[i,]=coefficients(lm(y ~ x-1)) +\n",
      "\n",
      "t(rnorm(9,mean=0,sd=1))%*%chol(s2[i-1]*solve(t(xstar)%*%xstar))\n",
      "\n",
      "...\n",
      "\n",
      "Notice that this change mirrors the change we made to incorporate missing\n",
      "data on y. The key difference is that the missing values for y were replaced\n",
      "by a simulated value using the regression-predicted value as the mean and\n",
      "the error variance as the variance, whereas the missing values for income\n",
      "were simulated using simply the mean and variance of the observed values of\n",
      "income. Notice also that we now must compute (XT X)−1 at every iteration,\n",
      "because the X matrix changes each time the missing X are simulated.\n",
      "\n",
      "\n",
      "\n",
      "190 7 The Linear Regression Model\n",
      "\n",
      "The parameter estimates obtained under this approach do not differ sub-\n",
      "stantially from those obtained using listwise deletion nor using the previous\n",
      "change to include the missing data on the outcome variable, with the excep-\n",
      "tion of the effect of being a South out-migrant (see Exercises). However, the\n",
      "posterior standard deviations are smaller, as a result of the increased sample\n",
      "size, coupled with the assumption that the missing follow the same model as\n",
      "the nonmissing (i.e., they are MAR).\n",
      "\n",
      "Table 7.3. Results of regression of empathy on region with and without missing\n",
      "data: Missing data assumed to be MAR.\n",
      "\n",
      "Model (What Missing Is Incorporated?)\n",
      "Variable No Missing Y only Y & X Y & X\n",
      "\n",
      "(Mean) (Regression)\n",
      "\n",
      "Intercept 19.58(.58) 19.56(.58) 19.42(.54) 19.43(.55)\n",
      "Age 0.018(.006) 0.018(.006) 0.020(.005) 0.02(.005)\n",
      "Male −2.43(.20) −2.44(.19) −2.43(.18) −2.43(.19)\n",
      "White 0.52(.25) 0.52(.25) 0.60(.24) 0.60(.24)\n",
      "Yrs. Schooling 0.07(.04) 0.07(.04) 0.07(.03) 0.06(.04)\n",
      "Income ($1,000s) 0.004(.003) 0.004(.003) 0.003(.003) 0.003(.003)\n",
      "Continuous South 0.69(.23) 0.69(.22) 0.75(.21) 0.76(.21)\n",
      "South Out-Migrant 0.19(.50) 0.18(.50) 0.38(.49) 0.39(.48)\n",
      "South In-Migrant 0.29(.35) 0.27(.35) 0.25(.33) 0.25(.33)√\n",
      "\n",
      "σ2e 4.62(.07) 4.63(.07) 4.65(.06) 4.65(.06)\n",
      "n 2,360 2,394 2,696 2,696\n",
      "\n",
      "Note: The first column shows the results when only cases with complete information\n",
      "are used. The second column shows the results when persons missing on Y are\n",
      "included in the analysis (see text). The third column presents the results when\n",
      "missing income values are replaced stochastically using simulated values from a\n",
      "normal distribution with a mean and variance equal to those for the observed cases.\n",
      "The fourth column presents the results when missing income values are replaced\n",
      "stochastically with predicted values and the error variance from a regression model\n",
      "predicting income for observed cases using all other predictor variables.\n",
      "\n",
      "Table 7.3 shows the parameter estimates and posterior standard deviations\n",
      "for the various models. The final column in the table shows the results of one\n",
      "additional approach to handling missing data on the income item: regression-\n",
      "based simulation. Under the previous approach, we simply simulated missing\n",
      "income from a distribution that was common to all observations, regardless of\n",
      "the values of the covariates for the missing cases. An alternative approach is to\n",
      "estimate the regression of income on all other predictors and simulate a value\n",
      "of income from a normal distribution with a mean equal to the regression-\n",
      "\n",
      "\n",
      "\n",
      "7.5 Conclusions 191\n",
      "\n",
      "based predicted score for income and a variance equal to the residual variance.5\n",
      "\n",
      "I leave the modification of the Gibbs sampler to handle this approach as an\n",
      "exercise. This set of results is virtually identical to those obtained using the\n",
      "previous approach for handling missingness on income.\n",
      "\n",
      "Overall, the results reveal more similarities than differences between pa-\n",
      "rameter estimates, but the latter two approaches show a substantial reduction\n",
      "in the posterior standard deviations of the parameters. The reason is that the\n",
      "sample size is increased by retaining observations that would have been deleted\n",
      "due to having missing data. Is this reduction in the standard error legitimate;\n",
      "that is, should we see smaller posterior standard deviations despite the fact\n",
      "that our models recognize the uncertainty the missing data present? Yes—in\n",
      "fact, the posterior standard deviations are reduced proportional to the gain in\n",
      "the square root of the sample size obtained by incorporating the observations\n",
      "with missing data, just as the Central Limit Theorem suggests. However, it is\n",
      "important to recognize that, if we had taken the common approach of simply\n",
      "imputing a single fixed value for the missing data (either using regression im-\n",
      "putation or mean imputation), the uncertainty presented by the missing data\n",
      "would have been ignored, and the posterior standard deviations would have\n",
      "been even smaller (and inappropriately so).\n",
      "\n",
      "Does the fact that the parameter estimates are consistent across these\n",
      "models reassure us of the robustness of the results to missing data? The answer\n",
      "to this question is “absolutely not,” although it is tempting to believe that such\n",
      "consistency of results demonstrates robustness. Each of the approaches that\n",
      "we have taken has assumed that the missing data are MAR (but possibly not\n",
      "OAR). So long as that assumption is maintained in our missing data strategy,\n",
      "the results will be consistently similar.\n",
      "\n",
      "In order to obtain different results, we would need to make adjustments to\n",
      "the model that compensate for possible violations of the MAR assumption. Of\n",
      "course, in that case, we have no way to know whether the results we obtain at\n",
      "that point are realistic or not: That determination rests on our prior knowledge\n",
      "regarding the missing data generation mechanism. In brief, there is no free\n",
      "lunch when it comes to violations of the MAR assumption. If the data are\n",
      "not MAR, we cannot know from the data at hand how to obtain unbiased\n",
      "parameter estimates.\n",
      "\n",
      "7.5 Conclusions\n",
      "\n",
      "In this chapter, we demonstrated the Bayesian approach to the multiple re-\n",
      "gression model using a Metropolis-Hastings sampling algorithm and two Gibbs\n",
      "samplers. In this process, I used several of the techniques discussed in the pre-\n",
      "vious chapter for assessing how well the algorithms performed, as well as for\n",
      "\n",
      "5 Allison (2003) argues that y should also be included in imputing the missing\n",
      "values of x, but I have used only other x variables in this example.\n",
      "\n",
      "\n",
      "\n",
      "192 7 The Linear Regression Model\n",
      "\n",
      "assessing how well the models fit the data. Finally, I showed how Gibbs sam-\n",
      "pling can be adapted easily to handle missing data that are MAR. In the next\n",
      "chapter, we will consider some basic generalized linear models that are useful\n",
      "when the outcome variable is not continuous.\n",
      "\n",
      "7.6 Exercises\n",
      "\n",
      "1. Derive the maximum likelihood estimate for the regression vector (β) in\n",
      "the linear regression model (i.e., Equation 7.5).\n",
      "\n",
      "2. Develop an MH algorithm for the linear regression model that samples all\n",
      "regression parameters simultaneously. How difficult is it to find a proposal\n",
      "density that yields a reasonable acceptance rate? What are some possible\n",
      "strategies for finding a good proposal density?\n",
      "\n",
      "3. How can we work with the original posterior density (not the logged ver-\n",
      "sion) without worrying about overflow/underflow problems. Hint: Con-\n",
      "sider computing the ratio case by case. Write the R code and test it.\n",
      "\n",
      "4. Develop a Gibbs sampler that handles missingness on income via regression-\n",
      "based simulation of income on all other covariates.\n",
      "\n",
      "5. We discovered, under the various missing data approaches that assumed\n",
      "the data to be MAR, that the only coefficients that changed substantially\n",
      "were some of the region coefficients. Why did they change?\n",
      "\n",
      "6. Using your own data, treat the outcome variable as missing for every\n",
      "tenth case and rerun the Gibbs sampler that handles missingness on the\n",
      "outcome. Repeat the process with every fifth case treated as missing.\n",
      "Finally, repeat the process with every other case missing (50% missing).\n",
      "What happens to the posterior means and standard deviations of the\n",
      "parameters?\n",
      "\n",
      "\n",
      "\n",
      "8\n",
      "\n",
      "Generalized Linear Models\n",
      "\n",
      "In the last chapter, we discussed the linear regression model. A requirement\n",
      "for using that model is that the outcome variable must be continuous and\n",
      "normally distributed. In social science research, having a continuous outcome\n",
      "variable is rare. More often, we tend to have dichotomous, ordinal, or nom-\n",
      "inal outcomes. In these cases, the linear regression model discussed in the\n",
      "previous chapter is inappropriate for several reasons. First, heteroscedasticity\n",
      "and nonnormal errors are guaranteed when the outcome is not continuous,\n",
      "nullifying the justification for statistical tests on the parameters (see Long\n",
      "1997). Second, the linear model will often predict values that are impossible.\n",
      "For example, although the tolerance item in the last chapter had a range of 5,\n",
      "nothing in the model prevents predicted values from falling outside the [0,5]\n",
      "interval.1 Third, the functional form specified by the linear model will often\n",
      "be incorrect. For example, we should doubt that increases in a covariate will\n",
      "yield the same returns on the dependent variable at the extremes as would\n",
      "be obtained toward the middle. Although we did not explore this issue in the\n",
      "previous chapter, it seems clear that the effect of education on tolerance, say,\n",
      "would be greatest in the middle of the education distribution than at either\n",
      "end. Adding a few years of education beyond a college degree or adding a few\n",
      "years but maintaining a grade level below 8 will probably not be as influential\n",
      "as transitioning from 11 to 12 years of education or from 12 to 16 years in\n",
      "influencing an individual’s tolerance.\n",
      "\n",
      "Generalized linear models (GLMs) provide a way to handle these problems.\n",
      "Recall that the basic OLS regression model can be expressed as:\n",
      "\n",
      "µi = X\n",
      "T\n",
      "i β,\n",
      "\n",
      "1 As it turns out, no combination of covariates will actually produce predicted\n",
      "values outside the allowable range, but this is because the model fits poorly—the\n",
      "covariates as specified do not have much influence on the outcome.\n",
      "\n",
      "\n",
      "\n",
      "194 8 Generalized Linear Models\n",
      "\n",
      "where µi = E(yi). Observe that there is no error term in this specification:\n",
      "This is because the expected value of yi is simply the linear predictor. Gener-\n",
      "alized linear models can be expressed as:\n",
      "\n",
      "F (µi) = X\n",
      "T\n",
      "i β,\n",
      "\n",
      "µi = E(yi).\n",
      "\n",
      "That is, some function F of the expected value of yi is equal to the linear\n",
      "predictor with which we are already familiar. This function is called a “link”\n",
      "function, because it links the nonlinear/noncontinuous variable yi with the\n",
      "linear predictor XTi β. The most common GLMs used in the social sciences\n",
      "and their link functions can be found in Table 8.1. As the table shows, when\n",
      "the link function is simply F (µ) = µ, we have the linear regression model;\n",
      "when the link function is F (µ) = ln (µ/(1− µ)), we have the logistic regression\n",
      "model; and so on.\n",
      "\n",
      "Table 8.1. Link functions and corrresponding generalized linear models (individual\n",
      "subscripts omitted).\n",
      "\n",
      "Link Function (F (µ) is ?) Model\n",
      "\n",
      "µ Linear Regression\n",
      "\n",
      "ln\n",
      "“\n",
      "\n",
      "µ\n",
      "1−µ\n",
      "\n",
      "”\n",
      "Logistic Regresssion\n",
      "\n",
      "Φ−1(µ) Probit Regression\n",
      "\n",
      "ln(µ) Poisson Regression\n",
      "\n",
      "ln(− ln(1− µ)) Complementary Log–Log Regression\n",
      "\n",
      "An alternative way of expressing the link function is in terms of a trans-\n",
      "formation of XTi β, rather than a transformation of µi. In that case, we could\n",
      "write the logistic regression model, for example, as:\n",
      "\n",
      "µi = G(X\n",
      "T\n",
      "i β) ≡\n",
      "\n",
      "eX\n",
      "T\n",
      "i β\n",
      "\n",
      "1 + eX\n",
      "T\n",
      "i\n",
      "\n",
      "β\n",
      ",\n",
      "\n",
      "where G = F−1. In the dichotomous logit and probit models, E(yi) = p(yi =\n",
      "1), and so the model can be written in terms of probabilities:\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 195\n",
      "\n",
      "p(yi = 1) = G(X\n",
      "T\n",
      "i β).\n",
      "\n",
      "In this notation, G is the link function.\n",
      "For a logistic regression model, G is the cumulative standard logistic dis-\n",
      "\n",
      "tribution function, whereas for a probit model, G is the cumulative standard\n",
      "normal distribution function.\n",
      "\n",
      "8.1 The dichotomous probit model\n",
      "\n",
      "One of the first generalized linear regression models to achieve popularity in\n",
      "the social sciences was the logistic regression model for dichotomous outcome\n",
      "variables (Hosmer and Lemeshow 1989). Dichotomous outcomes are prevalent\n",
      "in social science research, but prior to the 1980s, the computing power required\n",
      "to estimate the parameters of generalized linear regression models exceeded\n",
      "capacity. As a result, many social science researchers estimated parameters\n",
      "using OLS regression, which, as we discussed above, is an undesirable approach\n",
      "for several reasons. With the development of the iteratively reweighted least\n",
      "squares algorithm, coupled with an increase in computing power, the use of\n",
      "logistic regression rapidly replaced OLS regression as the preferred method\n",
      "for handling dichotomous outcomes. The logistic regression model obtained\n",
      "popularity primarily because of the relative ease of interpretation of the model\n",
      "parameters, as I show below. Nonetheless, in this chapter, my examples involve\n",
      "only the dichotomous probit model and its extension to the ordinal probit\n",
      "models.\n",
      "\n",
      "The choice of the probit model over the logistic regression model or other\n",
      "models (e.g., the complementary log-log model) is largely for convenience: As\n",
      "we will see, the probit model, from a probability standpoint is often easier to\n",
      "work with, because it involves use of the normal distribution. Nonetheless, the\n",
      "extensions to logistic and complementary log-log models are straightforward.\n",
      "For a classical approach to these models, I recommend Long (1997) and Powers\n",
      "and Xie (2000). For an in-depth Bayesian exposition to dichotomous and\n",
      "ordinal outcome models, I recommend Johnson and Albert (1999).\n",
      "\n",
      "8.1.1 Model development and parameter interpretation\n",
      "\n",
      "If our outcome variable in a regression model is dichotomous (0,1), then an ap-\n",
      "propriate likelihood function for the data is based on the binomial or Bernoulli\n",
      "distribution. The likelihood function can be expressed as:\n",
      "\n",
      "L(P |y) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "p\n",
      "yi\n",
      "i (1− pi)\n",
      "\n",
      "1−yi , (8.1)\n",
      "\n",
      "where yi is the observed dichotomous outcome for person i. For a person with\n",
      "a “1” on the outcome, the contribution to the likelihood reduces to the first\n",
      "\n",
      "\n",
      "\n",
      "196 8 Generalized Linear Models\n",
      "\n",
      "term; for a person with a 0, the contribution to the likelihood reduces to the\n",
      "second term.\n",
      "\n",
      "As written, this likelihood function has no regression parameters, but we\n",
      "would like to link pi—the probability of a “1” response—to the linear combi-\n",
      "nation XTi β. As discussed at the beginning of the chapter, this is problematic\n",
      "because an identity link [i.e., pi = XTi β] can predict illegitimate values for\n",
      "pi. Cumulative distribution functions constitutes a class of functions that can\n",
      "map the predictor from the entire real line onto the interval [0, 1], because\n",
      "cdfs are naturally constrained to produce values that fall in this range (refer\n",
      "to Chapter 2). In the probit model case, we allow pi = Φ(XTi β), where Φ(.) is\n",
      "\n",
      "the cumulative normal distribution function (i.e.,\n",
      "∫XTi β\n",
      "−∞ N(0, 1)). Regardless\n",
      "\n",
      "of the value of XTi β, pi will fall in the acceptable range. To obtain a logistic\n",
      "regression model, one would simply need to set pi = eX\n",
      "\n",
      "T\n",
      "i β/(1 + eX\n",
      "\n",
      "T\n",
      "i β) (the\n",
      "\n",
      "cumulative logistic distribution function).\n",
      "Another way to think about dichotomous probit and logistic regression\n",
      "\n",
      "models—one that is useful for setting up a Gibbs sampler—is in terms of\n",
      "latent variables. We could express the probit or logistic regression model as\n",
      "a linear model for a continuously distributed unobserved trait or propensity\n",
      "score, y∗i as:\n",
      "\n",
      "y∗i = X\n",
      "T\n",
      "i β + ei (8.2)\n",
      "\n",
      "using the link:\n",
      "\n",
      "yi =\n",
      "{\n",
      "\n",
      "1 iff y∗i > 0\n",
      "0 iff y∗i ≤ 0.\n",
      "\n",
      "(8.3)\n",
      "\n",
      "From this perspective, although y∗i is continuous, crude measurement lim-\n",
      "its our observation to the dichotomous individual response yi. If the individ-\n",
      "ual’s propensity is great enough (i.e., it falls above the threshold of 0), we\n",
      "observe a 1 on y; otherwise, we observe a 0.\n",
      "\n",
      "Given this set up, we need to rearrange the model somewhat to allow esti-\n",
      "mation of the regression parameters of interest. We can combine Equations 8.2\n",
      "and 8.3 such that:\n",
      "\n",
      "If yi =\n",
      "{\n",
      "\n",
      "1, ei > −XTi β\n",
      "0, ei < −XTi β.\n",
      "\n",
      "(8.4)\n",
      "\n",
      "If we assume a standard normal distribution for e, then we obtain the probit\n",
      "model:\n",
      "\n",
      "p(yi = 1) = p(ei > −XTi β) = p(ei < X\n",
      "T\n",
      "i β) =\n",
      "\n",
      "∫ XTi β\n",
      "−∞\n",
      "\n",
      "N(0, 1). (8.5)\n",
      "\n",
      "Observe that this is the same expression we placed into the likelihood\n",
      "function above in Equation 8.1. If we assume a logistic distribution for the\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 197\n",
      "\n",
      "error instead of a normal distribution, then we obtain the logistic regression\n",
      "model discussed above.\n",
      "\n",
      "To make the probit model fully Bayesian, we need to specify priors on the\n",
      "regression parameters in the model. For that purpose, we could use improper\n",
      "uniform priors over the real line, as we did in the previous chapter, we could use\n",
      "very vague normal priors [e.g., β ∼ N(0, 10000)], or we could use something\n",
      "called “conditional means priors.” Under this approach, we would specify\n",
      "prior expected probabilities for several combinations of covariates, invert these\n",
      "probabilities into the regression coefficient scale, and combine this information\n",
      "with the likelihood function to obtain the posterior (see Johnson and Albert\n",
      "1999 for examples). In the examples, I simply use uniform priors so that\n",
      "comparisons can be made with estimates obtained via maximum likelihood\n",
      "estimation.\n",
      "\n",
      "With priors established, and Φ(XTi β) substituted for pi, the posterior dis-\n",
      "tribution is:\n",
      "\n",
      "p(β|Y,X) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "Φ(XTi β)\n",
      "yi(1− Φ(XTi β))\n",
      "\n",
      "1−yi . (8.6)\n",
      "\n",
      "The interpretation of the parameters in this model—and all GLMs—is not\n",
      "as easy as the interpretation of parameters in the OLS regression model (see\n",
      "Liao 1994). Because the link function is nonlinear (an integral of a nonlinear\n",
      "function), the model itself is nonlinear in the probabilities, even though the\n",
      "predictor is linear. This nonlinearity complicates interpretation, because the\n",
      "effects of each variable is no longer independent of the effects of other variables.\n",
      "The probit model is linear, however, in z (standard normal) units. That is,\n",
      "given that Xβ implies a particular upper limit for the integral of the standard\n",
      "normal distribution, each β can be viewed in terms of its expected effect on\n",
      "the z score for a one-unit increase in its corresponding covariate. The logistic\n",
      "regression model, on the other hand, is linear in log-odds units. Recall that\n",
      "odds are computed as the ratio of p\n",
      "\n",
      "1−p . The logistic link function, then, is a\n",
      "log-odds function. The coefficients from the model can be interpreted in terms\n",
      "of their linear effect on the log-odds, but this is not much help. Instead, if we\n",
      "exponentiate the model, we obtain:\n",
      "\n",
      "exp\n",
      "(\n",
      "\n",
      "ln\n",
      "(\n",
      "\n",
      "pi\n",
      "1− pi\n",
      "\n",
      "))\n",
      "= exp\n",
      "\n",
      "(\n",
      "XTi β\n",
      "\n",
      ")\n",
      "= eβ0eβ1xi1 . . . eβkxik . (8.7)\n",
      "\n",
      "This result says that the odds are equal to the multiple of the exponenti-\n",
      "ated coefficients. Suppose we had an exponentiated coefficient of 2 for sex\n",
      "(measured as a dummy variable in which male = 1). This exponentiated coef-\n",
      "ficient would imply that the odds for a male responding “1” on the outcome\n",
      "variable are twice as great as the odds for a female responding “1,” net of\n",
      "the other variables in the model. This ease of interpretation has made the\n",
      "logistic regression model popular in the social sciences. The limitations, how-\n",
      "ever, include (1) that this interpretation tells us nothing about absolute risk,\n",
      "\n",
      "\n",
      "\n",
      "198 8 Generalized Linear Models\n",
      "\n",
      "only relative risk; and (2) the odds ratios are commonly interpreted as if they\n",
      "were ratios of probabilities.2 In fact, there is not one-to-one correspondence\n",
      "between absolute risk measured by a ratio of probabilities and relative risk\n",
      "measured by a ratio of odds. For example, if groups A and B have absolute\n",
      "risks of .1 and .01, respectively, the odds ratio representing the relative risk for\n",
      "group A versus B is the same as if the probabilities were .99 and .9: The odds\n",
      "ratio is 11, but certainly the absolute risks are substantially different. The\n",
      "ratio of probabilities conveys this information about the absolute risk. The\n",
      "ratio of probabilities is 10 under the first set of probabilities and is 1.1 under\n",
      "the second, which indicates a substantially larger difference in the absolute\n",
      "probabilities for the first pair than for the second pair.\n",
      "\n",
      "For both the logistic and the probit models, model probabilities can be\n",
      "obtained by (1) evaluating the linear predictor, and (2) applying the link\n",
      "function to obtain p.\n",
      "\n",
      "8.1.2 Sampling from the posterior distribution for the model\n",
      "parameters\n",
      "\n",
      "With the posterior distribution established in Equation 8.6, MCMC sampling\n",
      "relies either on using this posterior as written for an MH algorithm or on\n",
      "deriving the conditional distribution for β for Gibbs sampling. Developing an\n",
      "MH algorithm for this model is straightforward using the pnorm function in\n",
      "R (or its equivalent in other languages), and I leave doing so as an exercise\n",
      "(see Exercises).\n",
      "\n",
      "As written in Equation 8.6, the posterior cannot be simplified further, mak-\n",
      "ing the construction of a Gibbs sampler appear to be a daunting process. How-\n",
      "ever, as I mentioned above, the latent variable approach makes the process of\n",
      "Gibbs sampling simple. The unknown quantities in the model are the vector β\n",
      "and the vector Y ∗. We know from the initial specification that Y ∗ ∼ N(Xβ, 1),\n",
      "subject to the constraint that each y∗i > 0 iff yi = 1 and y\n",
      "\n",
      "∗\n",
      "i < 0 iff yi = 0.\n",
      "\n",
      "Together, these specifications imply that y∗i |yi, Xi, β ∼ TN(X\n",
      "T\n",
      "i β, 1), where\n",
      "\n",
      "TN is the truncated normal distribution. The point of truncation is 0, and the\n",
      "side of the distribution that is truncated is determined by yi: For individuals\n",
      "with y = 0, the distribution is truncated above 0; for individuals with y = 1,\n",
      "the distribution is truncated below 0 (see Albert and Chib 1993).\n",
      "\n",
      "What is the distribution for β? Because the vector Y ∗ is normally dis-\n",
      "tributed, given Y ∗, the conditional distribution for β is the same as for the OLS\n",
      "regression model (with σ2 = 1): β|X, Y ∗ ∼ N\n",
      "\n",
      "(\n",
      "(XT X)−1(XT Y ∗), (XT X)−1\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "Thus, a Gibbs sampler can be specified in four steps:\n",
      "\n",
      "1. Establish starting values for β.\n",
      "2. Simulate Y ∗|X, Y, β ∼ TN(Xβ, 1).\n",
      "2 Researchers commonly say that an odds ratio of 2 implies that one group is “twice\n",
      "\n",
      "as likely” as another to have a “1” on the outcome, but this language implies that\n",
      "the odds ratio is a ratio of two probabilities, and it is not.\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 199\n",
      "\n",
      "3. Simulate β|X, Y ∗, Y ∼ N\n",
      "(\n",
      "(XT X)−1(XT Y ∗), (XT X)−1\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "4. Return to step 2 (repeat).\n",
      "\n",
      "An R program for implementing this Gibbs sampler is as follows:\n",
      "\n",
      "#R program for dichotomous probit model\n",
      "\n",
      "#read the data\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\bookdead.dat\")[,3:9])\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\bookdead.dat\")[,10])\n",
      "\n",
      "#create variables, set values, and write out starting values\n",
      "\n",
      "b=matrix(0,7); vb=solve(t(x)%*%x); ch<-chol(vb)\n",
      "\n",
      "write(c(i,t(b)), file=\"c:\\\\dprob_gibbs.out\", append=T, ncol=8)\n",
      "\n",
      "#begin MCMC simulation\n",
      "\n",
      "for(i in 2:5000){\n",
      "\n",
      "#simulate latent data from truncated normal distributions\n",
      "\n",
      "u=as.matrix(runif(length(y),min=0,max=1))\n",
      "\n",
      "xb=as.matrix(x%*%b)\n",
      "\n",
      "ystar=qnorm(y*u + u*(-1)^y*pnorm(0,mean=xb,sd=x[,1]) +\n",
      "\n",
      "y*pnorm(0,mean=xb,sd=1), mean=xb, sd=x[,1])\n",
      "\n",
      "#simulate beta vector from appropriate mvn distribution\n",
      "\n",
      "b<-vb%*%(t(x)%*%ystar) + t((rnorm(7,mean=0,sd=1))%*%ch)\n",
      "\n",
      "write(c(i,t(b))), file=\"c:\\\\dprob_gibbs.out\", append=T, ncolumns=8)\n",
      "\n",
      "if(i%%10==0){print(c(i,t(b))),digits=2)}\n",
      "\n",
      "}\n",
      "\n",
      "This program is similar to the Gibbs samplers for the OLS regression model\n",
      "in the previous chapter, with four key differences. First, there is no simulation\n",
      "of σ2e in this program, because this parameter is fixed at 1 in the probit model\n",
      "(i.e., the error is assumed to be standard normal). Second, latent data (ystar)\n",
      "are simulated to replace the observed dichotomous y. Third, the regression\n",
      "parameter vector mean changes at every iteration. This change is attributable\n",
      "to the fact that the latent data, ystar, change at every iteration, and thus\n",
      "(XT X)−1(XT Y ∗) changes. Fourth, because I am saving the results to a file, I\n",
      "do not need to double-subscript b (the regression parameter vector) to reflect\n",
      "the iteration of the algorithm. Instead, the current value of the vector, after\n",
      "it is sampled, is written to the file.\n",
      "\n",
      "I have written this program to be quite compact—with many statements\n",
      "I have used separately in previous programs now being combined into a sin-\n",
      "gle line—but only one section of this program requires some discussion: the\n",
      "simulation of the latent data from truncated normal distributions.\n",
      "\n",
      "\n",
      "\n",
      "200 8 Generalized Linear Models\n",
      "\n",
      "8.1.3 Simulating from truncated normal distributions\n",
      "\n",
      "Simulation from truncated normal distributions can be quite simple but inef-\n",
      "ficient, or it can be very fast but mathematically challenging. In this and the\n",
      "following sections, I discuss three ways to perform such simulation: a naive\n",
      "but simple way, a rejection sampling approach, and an inversion sampling ap-\n",
      "proach. Ultimately, I use the inversion sampling approach, but I have often\n",
      "used the naive approach. I describe the rejection sampling approach largely\n",
      "to motivate thinking about sampling from uncommon distributions in a more\n",
      "practical setting than in previous chapters.\n",
      "\n",
      "The naive approach\n",
      "\n",
      "A “naive” way of simulating from truncated normal distributions, as Robert\n",
      "and Casella (1999) call it, is to repeatedly simulate draws from a full normal\n",
      "distribution with the specified mean until a draw is obtained in the appropriate\n",
      "part of the distribution. This approach is very simple to implement but has\n",
      "one serious drawback: It is extremely inefficient. If an individual is an outlier,\n",
      "that is, his/her XTi β is large (small) but s/he reported a 0 (1), most of the\n",
      "mass of the normal distribution from which we would be simulating possible\n",
      "values will be centered well away from the region from which the latent score\n",
      "must be drawn. This fact implies that it may take hundreds or even thousands\n",
      "of simulated scores before obtaining just one that falls in the right region.\n",
      "For a general example, see Figure 8.1. The figure shows a complete normal\n",
      "distribution with a mean of 1.645 and a standard deviation of 1. If a person\n",
      "with XTi β = 1.645 happened to respond “0” on the outcome variable, his/her\n",
      "latent score would need to be simulated to the left of the point of truncation\n",
      "(0), which is 1.645 standard deviations away from the mean. It is well known\n",
      "that only 5% of the mass of a normal density lies to the left of 1.645 standard\n",
      "deviations from the mean, and so it would take an expected 20 draws before\n",
      "we would sample this person’s latent score at that particular iteration of the\n",
      "algorithm (5 out of every 100 draws should come from that region). Simulating\n",
      "an expected 20 draws to obtain one valid value is highly inefficient, considering\n",
      "that we may need to draw from a tail region for many cases (e.g., if the model\n",
      "does not fit the data well and/or has many outliers), and we need to do this\n",
      "for thousands of iterations. If one has chosen particularly poor starting values\n",
      "for the parameters, an algorithm may not even begin to simulate parameters,\n",
      "because it may find itself stuck indefinitely attempting to simulate latent\n",
      "scores at the first iteration!\n",
      "\n",
      "In all fairness, this naive sampling scheme often works well and may be fast\n",
      "enough despite its inefficiency if one has access to very powerful computing.\n",
      "However, it may not be satisfactory. Fortunately, alternative approaches to\n",
      "simulating values from truncated normal distributions can be constructed.\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 201\n",
      "\n",
      "Latent Distribution (Y*)\n",
      "\n",
      "y\n",
      "\n",
      "-4 -2 0 2 4 6 8\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "Not so easy to draw from here\n",
      "\n",
      "(Y=0)\n",
      "\n",
      "Easy to draw from here\n",
      "\n",
      "(Y=1)\n",
      "\n",
      "X(i)b=1.645\n",
      "\n",
      "Fig. 8.1. Depiction of simulation from a truncated normal distribution.\n",
      "\n",
      "A rejection sampling strategy\n",
      "\n",
      "Recall from Chapter 4 that rejection sampling involves sampling a value (z)\n",
      "from a known density (g) that—when multiplied by a constant m—envelops\n",
      "the unknown but desired density f (mathematically: m × g(z) > f(z), ∀z)\n",
      "and evaluating whether the value will be accepted. We can apply this strategy\n",
      "to the truncated normal distribution.\n",
      "\n",
      "Figure 8.2 provides a larger depiction of the region of the truncated normal\n",
      "distribution (from Figure 8.1) from which sampling may be desired. The first\n",
      "step in constructing a rejection sampling routine is to evaluate the height of\n",
      "the normal density at the point of truncation. This is easily done within R\n",
      "as: h=dnorm(0,mean=?,sd=1), where the mean (?) is set to the expected pre-\n",
      "dicted score (XTi β) for the person for whom the sampling is to be performed.\n",
      "We then sample a point z from a uniform density on the interval [−2, 0], giv-\n",
      "ing us a point in the x dimension to evaluate. Next we draw a U(0, 1) random\n",
      "variable u and multiply it by the height of the normal density, which, as the\n",
      "figure shows, is also the height of our scaled uniform proposal density. This\n",
      "gives us a point in the y dimension, at x = z, to evaluate. If u × h < f(z),\n",
      "\n",
      "\n",
      "\n",
      "202 8 Generalized Linear Models\n",
      "\n",
      "where f(z) is the height of the normal density at z, then we accept the draw\n",
      "as coming from the normal density. In other words, if the sampled value u falls\n",
      "below the normal curve, we accept it; otherwise we reject it and try again.\n",
      "\n",
      "Since 5% of the mass of a N(1.645, 1) density falls below 0, and the mass\n",
      "of our uniform proposal density is [f(0) = .103]× (−2) = .206, this rejection\n",
      "scheme will have an acceptance rate of approximately 25%: one out of four\n",
      "draws will be accepted. This represents a five-fold improvement over the naive\n",
      "sampling approach and may produce a remarkable increase in the speed of\n",
      "the algorithm. However, this sampling routine can be improved, and there\n",
      "is one issue to consider. First, it could be improved by using a triangular\n",
      "proposal density rather than a uniform (rectangular) density. Since the normal\n",
      "distribution is monotonic beyond 1 standard deviation from the mean, if we\n",
      "establish a set point for our left “end” of the normal distributions tail we\n",
      "can construct a triangular proposal and sample from it using the inversion\n",
      "method. I will not elaborate on this approach here, because there is an even\n",
      "faster way to generate draws from the tail region with a 100% acceptance rate\n",
      "that we will discuss in the next section (but see Exercises).\n",
      "\n",
      "Second, with the construction of a bounded uniform proposal density, we\n",
      "have truncated the tail of the already truncated normal distribution: We will\n",
      "not draw samples from beyond x = −2. For all practical purposes, this is not\n",
      "much of a problem, because there is only a mass of .0001 we are excluding\n",
      "from a normal with mean 1.645 by imposing this boundary (the area under the\n",
      "normal curve up to −3.645 standard deviations from the mean). Nonetheless,\n",
      "the boundary can be expanded as far as one likes, but doing so will reduce\n",
      "the acceptance rate of the rejection sampling strategy because the mass of\n",
      "the proposal rectangle will expand faster than the area under the normal den-\n",
      "sity. Thus, we must balance two considerations, including (1) how large the\n",
      "mean must be before we decide to bypass the naive sampling approach and\n",
      "turn to the rejection sampling approach, (2) how much error we will allow in\n",
      "the rejection sampling routine. Those two considerations can be balanced by\n",
      "deciding a limit on the error, and then finding a mean for the normal distri-\n",
      "bution at which the acceptance rates of the naive sampler and the rejection\n",
      "sampler coincide. For example, we know that, at a mean of 1.645, the naive\n",
      "sampler will only have an acceptance rate of 5% when sampling from the tail.\n",
      "However, as we just discused, the rejection sampler will have an acceptance\n",
      "rate of about 25% (actually less if we discount the error region we are not\n",
      "sampling from). At a mean of 0, the naive sampler will have an acceptance\n",
      "rate of 50%, and the rejection sampler will have an acceptance rate of 59.8%,\n",
      "which indicates we would always be better off using the rejection sampler!\n",
      "But, the error is quite large in these cases—with a mean of 0 and a proposal\n",
      "width of 2, we would be missing approximately 5% of the left-hand side of\n",
      "the distribution under the rejection sampler. Thus, if we decide to reduce our\n",
      "limit on the error by using a uniform density with a width of three units,\n",
      "the acceptance rates of the rejection sampler and the naive sampler intersect\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 203\n",
      "\n",
      "(with the rejection samplers rate being better) for distributions with a mean\n",
      "around .6.\n",
      "\n",
      "Tail region of normal distribution, Mean=1.645, S.D.=1\n",
      "\n",
      "f(\n",
      "x)\n",
      "\n",
      "-6 -4 -2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      "Point of Truncation\n",
      "\n",
      "Ht. of density at truncation\n",
      "\n",
      "Scaled uniform density for rejection sampling\n",
      "\n",
      "25% acceptance rate\n",
      "\n",
      "w/ rejection sampling\n",
      "\n",
      "Fig. 8.2. Depiction of rejection sampling from the tail region of a truncated normal\n",
      "distribution.\n",
      "\n",
      "A better strategy: Inversion sampling\n",
      "\n",
      "For sampling from univariate truncated normal distributions, there is no in-\n",
      "herent reason to use the rejection sampling approach we just discussed for\n",
      "simulating. The inversion method, when the truncated region of the normal\n",
      "distribution is renormalized, is much faster and has an acceptance rate of\n",
      "100%. We discussed the inversion method of sampling in Chapter 4. The\n",
      "inversion method involves (1) drawing a U(0, 1) random variable that repre-\n",
      "sents the area under the distribution of interest, and (2) using the inverse-\n",
      "distribution function to find the value x from the distribution that yields that\n",
      "area below it. For example, if we were attempting to draw samples from a\n",
      "standard normal distribution and we drew a .5 from the uniform distribution,\n",
      "\n",
      "\n",
      "\n",
      "204 8 Generalized Linear Models\n",
      "\n",
      "the inversion method would produce x = 0 as our sample from the N(0, 1)\n",
      "distribution. If we drew a .025 from the uniform distribution, the inversion\n",
      "method would produce x = −1.96, etc.\n",
      "\n",
      "The inversion method can be adapted to sample from truncated nor-\n",
      "mal distributions. I will first show how to simulate from the left-hand side\n",
      "of the distribution. In the first step, we must “renormalize” the truncated\n",
      "region; that is, we must multiply the density by a constant so that the\n",
      "area of the truncated region integrates to 1. This renormalization step in-\n",
      "volves simply determining the area of the normal distribution that falls in\n",
      "the truncated region and hence just involves using the distribution function\n",
      "Φµ,σ(T ) =\n",
      "\n",
      "∫ T\n",
      "−∞N(µ, σ), with a given mean µ, standard deviation σ, and trun-\n",
      "\n",
      "cation point T . For the probit model, we have already stated the mean is the\n",
      "expected value for the given observation/case, the standard deviation is 1, and\n",
      "the truncation point is x = 0. Thus, in R we use: A=pnorm(0,xb,1), where 0\n",
      "is the truncation point, xb is the mean, and 1 is the standard deviation. Our\n",
      "constant for renormalizing is then 1/A.\n",
      "\n",
      "In the next step, we draw a U(0, 1) random variable u, just as in the\n",
      "original inversion routine. Finally, we multiply u by A, and we then apply\n",
      "the inverse-normal distribution function to this draw to obtain the draw from\n",
      "the truncated region. Why do we multiply u by A? In the original inversion\n",
      "approach, we let u = Φ(x), where u is our uniform draw and x is our sampled\n",
      "value of interest. In order to find x, we take Φ−1(u) = Φ−1(Φ(x)) = x. In\n",
      "the truncated distribution, however, our area is determined by the integral\n",
      "from −∞ to T . Φ must then be rescaled to produce an area no bigger than\n",
      "A, and so we use: u = (1/A)Φ(x). To solve for x, we move A to the left side\n",
      "of this equation: uA = Φ(x). Since u can be no bigger than 1, uA can be no\n",
      "bigger than A. So, Φ(x) will be limited between 0 and A and hence Φ−1 will\n",
      "be limited to the interval [−∞, T ].\n",
      "\n",
      "Sampling from the right-hand side of the normal distribution is only\n",
      "slightly more tedious. We can do this in one of two ways: (1) use the fact\n",
      "that the normal distribution is symmetric and reverse the sign of the mean\n",
      "and then reverse the sign of x once it is computed using Φ−1, or (2) keep the\n",
      "mean as is and simply add the appropriate constant before applying Φ−1 to\n",
      "keep x from falling below the truncation point. I discuss the latter approach\n",
      "here, because it is easier to understand, and because it is easier to use in the\n",
      "ordinal probit model to be discussed later.\n",
      "\n",
      "As before, when sampling from the right-hand side of a truncated normal\n",
      "distribution, we first need to renormalize the area to the right of the trun-\n",
      "cation point. Given that the area under the normal curve totals 1, this can\n",
      "be computed easily as: B = 1 − Φ(T ). Now, in this case, we need x to be\n",
      "bounded between T and ∞. We know from before that uA provides the area\n",
      "for which x = T , and so, we need Φ to yield a minimum value of uA. We also\n",
      "need Φ to yield a maximum value of 1. For Φ to yield A at a minimum, we\n",
      "therefore need Φ−1(A + C), where C must range from 0 to 1 − A (or B) to\n",
      "ensure that x falls in the appropriate region. Thus, we let C = uB, and we\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 205\n",
      "\n",
      "have x = Φ−1(A + uB) as our solution. When u is 0, the function returns T ;\n",
      "when u = 1, the function returns ∞, because A + B = 1.\n",
      "\n",
      "Another way to perform inversion sampling, rather than renormalizing\n",
      "the appropriate area (left or right side of the distribution), is to set the\n",
      "minimum and maximum values of the uniform random number function\n",
      "to the appropriate values. For example, if we are sampling from the left\n",
      "side of a normal distribution, we could set the minimum value for the uni-\n",
      "form density to Φ(−∞) = 0 and the maximum to Φ(0) for the normal\n",
      "density with mean xb and standard deviation 1 [e.g., in R, we would use:\n",
      "u=runif(1,min=0,max=pnorm(0,mean=xb,sd=1)]. This approach is mathe-\n",
      "matically equivalent to the previous one. In the first one, we leave the uniform\n",
      "density as U(0, 1) but multiply it to rescale its maximum value; in the sec-\n",
      "ond, we simply resize the uniform density. Under either approach, we would\n",
      "need to apply the inverse normal distribution function (qnorm in R) to u to\n",
      "obtain our latent y∗. Both approaches involve the same number of “steps,”\n",
      "but one approach may be easier to implement than another, depending on the\n",
      "programming language being used.\n",
      "\n",
      "Returning to the Gibbs sampler presented earlier, the truncated normal\n",
      "sampling code was:\n",
      "\n",
      "u=as.matrix(runif(length(y),min=0,max=1))\n",
      "xb=as.matrix(x%*%b)\n",
      "\n",
      "ystar=qnorm(y*u + u*(-1)^y*pnorm(0,mean=xb,sd=1) +\n",
      "y*pnorm(0,mean=xb,sd=1), mean=xb, sd=1)\n",
      "\n",
      "The first line simply generates an n-length vector of U(0, 1) random numbers.\n",
      "The second line generates an n-length vector of predicted values/expected\n",
      "means. The remaining code performs all the truncated normal simulation\n",
      "steps discussed above, but simultaneously handles (1) all cases in one step,\n",
      "relying on R’s ability to handle operations to entire vectors at once, and (2)\n",
      "simulation for y = 0 and y = 1. Feature (1) is straightforward: y, u, and xb\n",
      "are all vectors, and so ystar also is a vector. The second feature is a little\n",
      "more difficult to see. Suppose, for an individual, y = 0. In that case, the first\n",
      "argument within the qnorm function reduces to:\n",
      "\n",
      "u*pnorm(0,mean=xb,sd=1)\n",
      "\n",
      "This is the appropriate argument for cases in which y = 0, as shown above.\n",
      "In the event y = 1, the first argument reduces to:\n",
      "\n",
      "u - u*pnorm(0,mean=xb,sd=1) + pnorm(0,mean=xb,sd=1)\n",
      "\n",
      "Rearranging terms and factoring u reveals that this is, indeed, the correct\n",
      "argument for cases in which y = 1.\n",
      "\n",
      "An alternative argument, based on the second strategy listed above—\n",
      "adjusting the minimum and maximum arguments to the uniform random\n",
      "draw—can be implemented as:\n",
      "\n",
      "\n",
      "\n",
      "206 8 Generalized Linear Models\n",
      "\n",
      "ystar=qnorm(mean=xb,sd=1,\n",
      "runif(length(y),\n",
      "\n",
      "min=(y*pnorm(0,xb,1)), max=(pnorm(0,xb,1)^(1-y))))\n",
      "\n",
      "Notice here that when y = 0, the min=0 and the max=pnorm(0,xb,1); when\n",
      "y = 1, min=pnorm(0,xb,1) and max=1.(Also notice that I have shuffled\n",
      "around the arguments to the qnorm function—the order of arguments is ir-\n",
      "relevant to R for most functions).\n",
      "\n",
      "8.1.4 Dichotomous probit model example: Black–white differences\n",
      "in mortality\n",
      "\n",
      "Racial differences in mortality—especially black-white differences—are an im-\n",
      "portant concern for mortality demographers and health researchers. It is well\n",
      "known that blacks have higher levels of mortality than whites. A common\n",
      "goal in the literature has been to try to understand why this health advan-\n",
      "tage for whites exists (see Markides and Black 1996). Interestingly, this white\n",
      "advantage, however, does not exist across the entire age range. Instead, at ad-\n",
      "vanced ages (usually over 75 years of age), a crossover is observed such black\n",
      "mortality rates fall below those of whites. Although a relatively large demo-\n",
      "graphic literature has emerged over the last few decades suggesting possible\n",
      "explanations for the crossover (e.g., see Elo and Preston 1994; Lynch, Brown\n",
      "and Harmsen 2003; Preston et al. 1996), very few studies have attempted to\n",
      "quantify uncertainty in the age at which the crossover occurs, largely because\n",
      "the crossover age is a quantity for which the standard error is difficult to de-\n",
      "rive using classical methods. The example of the dichotomous probit model\n",
      "(and ordinal probit model) I use here seeks to quantify this uncertainty.\n",
      "\n",
      "A standard approach to modeling racial differences in mortality is to use\n",
      "dichotomous probit or logistic regression models, often in a discrete-time\n",
      "format, when individual-level longitudinal data are used. In a discrete-time\n",
      "specification, each person contributes more than one observation to the fi-\n",
      "nal dataset—one record for every year observed (Allison 1984; Hosmer and\n",
      "Lemeshow 1999; Yamaguchi 1991). I construct such a data set here, using data\n",
      "from the National Health and Nutrition Examination Survey (NHANES) and\n",
      "its follow-ups, the National Epidemiologic Follow-up Surveys (NHEFS). The\n",
      "NHANES was a cross-sectional survey designed to assess the nation’s health\n",
      "using a sample of roughly 34,000 individuals ages 24-77 beginning in 1971. Of\n",
      "these, roughly 7,000 sample members were administered a physical exam by\n",
      "a physician, as well as a health care supplemental survey measuring self-rated\n",
      "health, and were then followed-up on at least three occasions: in 1982, 1987,\n",
      "and 1992. Using individuals who (1) survived to 1975 (the last year of the\n",
      "baseline study), (2) were at least 40 years of age in 1975, (3) had no missing\n",
      "data on variables used in the analyses, and (4) were either white or black,\n",
      "I created an analytic sample of 3,201 persons, of which 685 died during the\n",
      "observation period. The resulting person-year file consisted of 55,173 records,\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 207\n",
      "\n",
      "with an average person-year contribution of 14.43 years by those who died (18\n",
      "person-years were contributed by those who survived through 1992).\n",
      "\n",
      "The predictor variables used in the analyses include age, sex, race (black\n",
      "or white), southern residence at baseline, years of schooling, and an inter-\n",
      "action between age and race. Age is the only time-varying covariate: It is\n",
      "incremented up until the point of censoring (if the respondent survived the\n",
      "observation period) or until death for each person-year record contributed by\n",
      "an individual.\n",
      "\n",
      "The outcome variable is an indicator for whether the individual died in\n",
      "a particular year. For individuals who survived the observation period, this\n",
      "variable takes a 0 for every person-year record contributed by the individual\n",
      "(18 records). For individuals who died during the observation period, this\n",
      "variable takes a 0 for every person-year record contributed until the year of\n",
      "death and takes a 1 for the person-year record for the year in which the\n",
      "respondent died. Thus, out of 55,173 person-records, only 685 records have a\n",
      "value of 1 on this variable, representing 1.2% of all records. Table 8.2 presents\n",
      "descriptive statistics for the original 3201 individuals in the baseline sample.\n",
      "\n",
      "Table 8.2. Descriptive statistics for NHANES/NHEFS data used in dichotomous\n",
      "probit model example (baseline n = 3, 201).\n",
      "\n",
      "Variable Mean(s.d.) or % Range\n",
      "\n",
      "Predictors\n",
      "Age 55.9(9.1) [40,77]\n",
      "Female 56.6% [0 or 1]\n",
      "Black 10.8% [0 or 1]\n",
      "Yrs. Schooling 11.0(3.3) [0,17]\n",
      "South (in ’71) 31.7% [0 or 1]\n",
      "Died 21.4% [0 or 1]\n",
      "\n",
      "Why use a Bayesian approach to this model when we could simply use\n",
      "a standard maximum likelihood approach? There are several reasons. First,\n",
      "we could incorporate prior information, although I have simply chosen not to\n",
      "here. Second, as we have discussed in previous chapters, the Bayesian approach\n",
      "offers a broader array of model diagnostics, especially in a GLM setting, than\n",
      "traditional approaches to estimation. Under a traditional maximum likelihood\n",
      "(ML) approach, we may obtain two measures of model fit—a pseudo-R2 and\n",
      "a model chi-square—and residual diagnostic tests that are derived from OLS\n",
      "regression residual diagnostic tests. We can obtain these same measures of\n",
      "model fit here (distributions of them, actually) using the Bayesian approach,\n",
      "but we can also obtain better residual diagnostic tests that do not rest on\n",
      "\n",
      "\n",
      "\n",
      "208 8 Generalized Linear Models\n",
      "\n",
      "assumptions that the dichotomous data cannot meet (see Johnson and Albert\n",
      "1999 for more discussion).\n",
      "\n",
      "Assessing model fit is important in a discrete time setting. Something that\n",
      "is often overlooked in discrete time analyses is that the dilution of the original\n",
      "data via construction of a person-year file reduces the “success” proportion in\n",
      "the outcome variable. For example, here, 21% of the respondents died over the\n",
      "observation period (685/3201), yet deaths represent only 1.2% of the records\n",
      "in the person-year data. Logit and probit models tend to fare best when the\n",
      "“success” proportion is close to .5, and thus we should carefully consider model\n",
      "fit when using discrete time models.3\n",
      "\n",
      "A third reason for taking a Bayesian approach is that, although the black-\n",
      "white crossover is an important phenomenon to mortality demographers at-\n",
      "tempting to understand racial heterogeneity and inequality, the ML approach\n",
      "offers no simple approach to testing hypotheses about it. The Bayesian ap-\n",
      "proach, on the other hand, does. The crossover age can be computed by setting\n",
      "the regression equations equal for blacks and whites and solving for age to find\n",
      "the point of intersection. With only age, race, and an interaction between them\n",
      "in the model, the model is:\n",
      "\n",
      "y∗ = b0 + b1Age + b2Black + b3Age× Black. (8.8)\n",
      "\n",
      "For blacks, Black = 1, and so the right-hand side reduces to (b0 + b2) + (b1 +\n",
      "b3)Age. For whites, Black = 0, and so the right-hand side reduces to b0+b1Age.\n",
      "Setting these equal, we find:\n",
      "\n",
      "b2 + b3Age = 0, (8.9)\n",
      "\n",
      "and so the age at which the crossover occurs is −b2/b3. This estimate can\n",
      "be found with the ML approach to estimation, but there is no simple way to\n",
      "quantify our uncertainty in it using the estimated standard errors of the con-\n",
      "tributing parameters—the product of normally distributed random variables\n",
      "has no known distribution, and so the standard error of a product of normal\n",
      "variables is not defined. In the Bayesian setting, however, we can construct\n",
      "this crossover age for every sampled value of the contributing parameters\n",
      "produced by the Gibbs sampler and thereby obtain a distribution for this\n",
      "quantity. Doing so allows us to answer important questions like: What is the\n",
      "probability that the crossover age lies outside reasonable bounds on the life\n",
      "span?\n",
      "\n",
      "I ran the Gibbs sampler presented in Section 8.1.2 several times, using\n",
      "different starting values for the parameters for each run. Figure 8.3 shows a\n",
      "trace plot of the intercept parameter, along with a plot of the autocorrelation\n",
      "3 One issue that is problematic when the success proportion is extreme (either high\n",
      "\n",
      "or low) is that a model that simply predicts success or failure for every observation\n",
      "will tend to predict the observed outcome very well. In this particular example, a\n",
      "model that simply predicts that no one died would seem to explain 98.8% of the\n",
      "observed responses! See Hosmer and Lemeshow for more discussion of this issue.\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 209\n",
      "\n",
      "function (ACF). The trace plot (and R statistic) shows rapid convergence, but\n",
      "the ACF plot shows a high level of autocorrelation. Ultimately, I ran the final\n",
      "algorithm for 25,000 iterations, eliminated the first 1,000 as the burn-in, and\n",
      "thinned the chain to every 24th sampled value to reduce autocorrelation. This\n",
      "process left me with 1,000 posterior samples for the regression coefficients. The\n",
      "latter plot shows the ACF after thinning the chain.\n",
      "\n",
      "0 500 1000 1500 2000 2500\n",
      "\n",
      "−\n",
      "6\n",
      "\n",
      "−\n",
      "4\n",
      "\n",
      "−\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "b\n",
      "0\n",
      "\n",
      "0 5 10 15 20 25 30\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "0 5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag (after thinning to every 24th value)\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "Fig. 8.3. Trace plot and autocorrelation function (ACF) plot for intercept parame-\n",
      "ter in dichotomous probit model example (upper plots show samples thinned to every\n",
      "\n",
      "10th sample; lower plot shows the ACF after thinning to every 24th post-burn-in\n",
      "sample).\n",
      "\n",
      "\n",
      "\n",
      "210 8 Generalized Linear Models\n",
      "\n",
      "Figure 8.4 shows sampled latent propensities (y∗) for four person-records.\n",
      "The first, second, and third graphs show the distributions for three person-\n",
      "years in which the respondent died. The first is the latent propensity distri-\n",
      "bution for an 84-year-old black male not from the South with 11 years of\n",
      "schooling. The second is the latent propensity distribution for a 62-year-old\n",
      "white male not from the South with 16 years of schooling. The third is the\n",
      "distribution of latent propensities for an 87-year-old white female not from the\n",
      "South with 17 years of education. Finally, the fourth is the distribution for a\n",
      "93-year-old black southern male with no formal education. This person-record\n",
      "represents a case in which the respondent did not die, and thus, although the\n",
      "first three distributions are to the right of 0, the fourth distribution is to the\n",
      "left of 0. Notice that the distributions for the first and third cases are very\n",
      "similar, while the distribution for the second case is quite different. The reason\n",
      "for the dissimilarity is that the second case is for a person who was substan-\n",
      "tially younger than the others—62 years of age—and the age-dependence of\n",
      "mortality is strong. Thus, the latent trait distribution tends to be clustered\n",
      "close to 0.\n",
      "\n",
      "These latent distributions can be used in multiple ways to assess model fit.\n",
      "First, I retained the variance of the complete sample of Y ∗ at each iteration.\n",
      "Because we know that the error distribution is N(0, 1), we can construct a\n",
      "distribution for R2 as4:\n",
      "\n",
      "R2 = 1−\n",
      "var(N(0, 1))\n",
      "\n",
      "var(Y ∗)\n",
      ". (8.10)\n",
      "\n",
      "The results of performing this calculation for each of the 1,000 retained values\n",
      "for var(Y ∗) produced a mean estimated R2 of .192 and a 95% interval estimate\n",
      "of [.17, .22]. These results suggest the model fits the data rather well by social\n",
      "science standards.\n",
      "\n",
      "In addition to this type of global test of model fit, we can also conduct\n",
      "residual analyses. How do we calculate a residual in a dichotomous probit\n",
      "model? For each individual in the sample, we only observe a 0 or 1 for the\n",
      "outcome variable. However, the predicted scores are in z units (or in log-odds\n",
      "units if we are using logistic regression). Thus, the standard approaches to\n",
      "residual analysis that have been well studied for linear regression model are\n",
      "generally poorly suited to studying residuals in GLMs.\n",
      "\n",
      "The Bayesian approach offers a solution to this dilemma. With the use of\n",
      "latent data (Y ∗), we can construct latent residuals (e∗ = Y ∗ −Xβ) and use\n",
      "whatever linear regression-based residual diagnostics we prefer. These latent\n",
      "residuals can be computed for every sampled value of β and Y ∗; in which case\n",
      "we will have a distribution of latent residuals for each individual, or they can\n",
      "be computed using the mean of the Y ∗ for each observation and the mean of\n",
      "β.\n",
      "\n",
      "4 There are numerous methods for computing pseudo-R2 measures in GLMs; this\n",
      "is simply one approach. See Long and Freese 2003\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 211\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "Latent Traits for Person 1\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "Latent Traits for Person 2\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "Latent Traits for Person 4\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "Latent Traits for Person 3\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 8.4. Sampled latent health scores for four sample members from the probit\n",
      "model example.\n",
      "\n",
      "Figure 8.5 shows the distributions of latent scores obtained during the run\n",
      "of the Gibbs sampler as well as the latent distributions of predicted scores com-\n",
      "puted from the results of the Gibbs sampler. Figure 8.6 shows the distribution\n",
      "of latent residuals—computed as the difference between these distributions—\n",
      "for the four cases discussed above. Of the four latent residual distributions,\n",
      "only one overlaps 0, which suggests adequate fit of the case. The other cases\n",
      "have residuals that are large and positive, which indicates that their latent\n",
      "propensities are substantially larger than the model predicts, as shown in Fig-\n",
      "ure 8.5. In other words, the model predicts that these individuals should not\n",
      "\n",
      "\n",
      "\n",
      "212 8 Generalized Linear Models\n",
      "\n",
      "have died, but in fact they did, which leads to values for Y ∗ that are positive\n",
      "(but predicted values that are negative).\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "Person 1\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "1\n",
      "\n",
      "0\n",
      "\n",
      "Person 2\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "Person 3\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "3\n",
      ".0\n",
      "\n",
      "Person 4\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 8.5. Model-predicted traits and actual latent traits for four observations in\n",
      "probit model (solid line = model-predicted latent traits from sample values of β;\n",
      "dashed line = latent traits simulated from the Gibbs sampler).\n",
      "\n",
      "I selected these cases for this particular reason: As I discussed above, in\n",
      "a model with a proportion of “successes” that is substantially far from .5, a\n",
      "model that predicts that all cases are either successes or failures will appear,\n",
      "on its face, to fit the data well. In this particular model, we have very few\n",
      "deaths, and so the model tends to predict that no one dies. In order to be\n",
      "convinced that the model fits well, we might consider investigating the latent\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 213\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "1\n",
      ".2\n",
      "\n",
      "Latent Residual, Person 1\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "Latent Residual, Person 2\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "1\n",
      ".2\n",
      "\n",
      "Latent Residual, Person 3\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "Latent Residual, Person 4\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 8.6. Latent residuals for four observations in probit model (reference line at\n",
      "0).\n",
      "\n",
      "error distribution for all cases or construct summary measures of them. For\n",
      "this particular model, doing so suggested adequate overall fit of the model.\n",
      "\n",
      "Table 8.3 shows the parameter estimates derived from the Gibbs sampler.\n",
      "The results show that age has a strong, positive effect on mortality risk, as does\n",
      "sex, with females evidencing substantially lower mortality risk than males.\n",
      "Region of residence and years of schooling have effects that are in the expected\n",
      "direction, but the effects would not be “significant” by classical standards.\n",
      "Under the Bayesian interpretation, the probability that these parameters are\n",
      "greater than (or less than, in the case of education) 0 is not less than .05.\n",
      "\n",
      "From the sampled values for the race parameter and the age-by-race inter-\n",
      "action, we can construct a distribution for the crossover age. This distribution\n",
      "is decidedly non-normal with a number of extreme values. The minimum and\n",
      "\n",
      "\n",
      "\n",
      "214 8 Generalized Linear Models\n",
      "\n",
      "Table 8.3. Gibbs sampling results for dichotomous probit model predicting mor-\n",
      "tality.\n",
      "\n",
      "Variable Parameter\n",
      "\n",
      "Intercept −5.37(.15)***\n",
      "Age 0.047(.002)***\n",
      "Female −0.24(.03)***\n",
      "Black 0.81(.36)*\n",
      "Yrs. Schooling −0.003(.005)\n",
      "South (in ’71) 0.03(.04)\n",
      "Age × Black −0.0099(.005)*\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The p-values are based on one-\n",
      "sided tail probabilities that the parameter exceeds 0 truncated to the classical cut-\n",
      "points of #p < .1, *p < .05, **p < .01, **p < .001.\n",
      "\n",
      "maximum values for the crossover age, for example, were −13, 226.38 and\n",
      "393.33, respectively. These extreme values are a product of the fact that we\n",
      "have uncertainty in both of the contributing parameters. Figure 8.7 shows the\n",
      "bivariate distribution for these contributing parameters, with horizontal and\n",
      "vertical reference lines at 0 dividing the distribution into four “cases.” In Case\n",
      "1, the main effect parameter is negative, and the interaction effect parameter\n",
      "is positive. This combination produces crossover ages that are positive. How-\n",
      "ever, under this scenario blacks begin life advantaged over whites and slowly\n",
      "see this advantage erode. In Case 2, the main and interaction effect parameters\n",
      "are both positive, which produces crossover ages that are negative. Through-\n",
      "out the observed life span, the black-white gap expands. In Case 3, the main\n",
      "and interaction effect parameters are both negative, producing crossover ages\n",
      "that also occur outside the life span (negative). In these cases, blacks evi-\n",
      "dence lower mortality than whites throughout life with the gap expanding\n",
      "across age. Finally, in Case 4, the main effect is positive and the interaction\n",
      "effect is negative, producing crossover ages that are positive and generally\n",
      "consistent with expectation. That is, the white advantage erodes across age.\n",
      "\n",
      "As the figure shows, the vast majority of the sampled parameter values—\n",
      "97.3% of them—fall under scenario 4. Overall, 1.6% of the sample parameter\n",
      "values fall under scenario 1, while 1.1% of the sample parameter values fall\n",
      "under scenario 2, and none fall under scenario 3. Thus, from a Bayesian view,\n",
      "there is a large probability that a crossover exists, and that the age pattern\n",
      "of mortality is consistent with theory—that is, blacks are disadvantaged in\n",
      "early life, but the white advantage decreases across age. In contrast, there is\n",
      "0 probability that blacks actually experience a large and growing advantage\n",
      "in mortality across age (cases 3), and very low probabilities that whites expe-\n",
      "rience a growing advantage across age (case 2) or that blacks begin life with\n",
      "an advantage that declines across age (case 1).\n",
      "\n",
      "\n",
      "\n",
      "8.1 The dichotomous probit model 215\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "● ●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "−1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 2.0\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "2\n",
      "\n",
      "5\n",
      "−\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      "5\n",
      "\n",
      "Black Parameter\n",
      "\n",
      "A\n",
      "g\n",
      "\n",
      "e\n",
      "−\n",
      "\n",
      "b\n",
      "y−\n",
      "\n",
      "B\n",
      "la\n",
      "\n",
      "ck\n",
      " P\n",
      "\n",
      "a\n",
      "ra\n",
      "\n",
      "m\n",
      "e\n",
      "\n",
      "te\n",
      "r (+) crossover ages\n",
      "\n",
      "(Case 4)\n",
      "\n",
      "(Case 2)\n",
      "\n",
      "(Case 3)\n",
      "\n",
      "(Case 1)\n",
      "\n",
      "Fig. 8.7. Distribution of black and age-by-black parameters (Cases 2 and 3 = both\n",
      "values are positive or negative, respectively; Cases 1 and 4 = values are of opposite\n",
      "signs. Case 4 is the typically-seen/expected pattern).\n",
      "\n",
      "We can consider the crossover in more detail by summarizing its distribu-\n",
      "tion, rather than considering the contributing parameters. Figure 8.8 shows a\n",
      "trace plot as well as a histogram of the distribution for crossover ages between\n",
      "0 years of age and 200 years of age. The trace plot shows a seemingly stationary\n",
      "distribution of crossover ages, with periodically very high or very low sampled\n",
      "values. The histogram shows several summary statistics for the crossover, in-\n",
      "cluding the mean age based on all sampled values of the crossover age, the\n",
      "median crossover age based on all sampled values, the mean age based on only\n",
      "values falling between ages 0 and 200, and the typical maximum likelihood-\n",
      "estimated crossover age. This latter age is found by taking the maximum\n",
      "likelihood estimates for the contributing parameters, substituting them into\n",
      "Equation 8.9, and solving for age.\n",
      "\n",
      "In general, the various measures of the crossover age vary substantially.\n",
      "The posterior mean when all sampled values for the crossover age are con-\n",
      "sidered is 69.04, but the posterior mean when only those values falling in the\n",
      "interval [0,200] are considered was 84.1. The median age, when all values are\n",
      "\n",
      "\n",
      "\n",
      "216 8 Generalized Linear Models\n",
      "\n",
      "0 200 400 600 800 1000\n",
      "\n",
      "−\n",
      "4\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "0\n",
      "4\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      "Iteration\n",
      "\n",
      "C\n",
      "ro\n",
      "\n",
      "ss\n",
      "o\n",
      "\n",
      "ve\n",
      "r \n",
      "\n",
      "A\n",
      "g\n",
      "\n",
      "e\n",
      "\n",
      "0 50 100 150 200\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "3\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "6\n",
      "\n",
      "Crossover Age (X)\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy Mean\n",
      "Median\n",
      "Mean, 0<X<200\n",
      "ML estimate\n",
      "\n",
      "Fig. 8.8. Distribution of black-white crossover ages for ages > 0 and < 200 (various\n",
      "summary measures superimposed as reference lines).\n",
      "\n",
      "considered, was 81.6, which is very close to the maximum likelihood estimate\n",
      "of 81.9. The posterior mean estimates, both with and without the constraints\n",
      "on the age range are more extreme than the median and maximum likelihood\n",
      "estimates. This extremeness is attributable to the influence the extreme posi-\n",
      "tive and negative values exert on the mean, bringing us to the question posed\n",
      "at the beginning of the section: What is the probability that the crossover\n",
      "falls outside the human “life span,” making it an irrelevant quantity/issue?\n",
      "Obviously, 0 is the lower bound on the life span. Thus far, the oldest living\n",
      "human lived to age 122, and so 122 may be a reasonable upper bound. Based\n",
      "on our results, the probability that the crossover age falls in this range is .966.\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 217\n",
      "\n",
      "The probability that the crossover age is negative is .011; the probability that\n",
      "the crossover age is greater than 122 is .023. These results suggest that the\n",
      "crossover most likely occurs during the life span.\n",
      "\n",
      "As an alternative to using 122 as the upper boundary, we may consider\n",
      "life expectancy at birth to be a more reasonable upper bound, given that it\n",
      "represents (in theory) an age around which most deaths cluster in a popu-\n",
      "lation.5 Life expectancy at birth is approximately 78. The probability that\n",
      "the crossover occurs before this age (but after age 0!) is: .258, a relatively\n",
      "small probability. However, research has argued that the crossover is an arti-\n",
      "fact of within-population heterogeneity in mortality rates (Lynch, Brown, and\n",
      "Harmsen 2003), and thus, we may not expect the crossover to occur until after\n",
      "half the population has died. Approximately half of individuals born today\n",
      "can expect to live to age 80, and so we may consider the probability that the\n",
      "crossover age falls between 80 and 100, the age at which only approximately\n",
      "1-2% of the population remains alive. Thus, if the crossover occurs after this\n",
      "age, it has little substantive importance. The probability that the crossover\n",
      "age falls between 80 and 100 is .506.\n",
      "\n",
      "In addition to these specific probabilities, we may consider constructing\n",
      "probability intervals for the crossover age. A 95% probability interval for the\n",
      "crossover age is [57.1,120.2], and a 90% interval is [72.1,108.0]. All of these\n",
      "results taken together provide a much more detailed summary of the crossover\n",
      "age than the maximum likelihood approach can offer. Substantively, from\n",
      "these results, we might conclude that the crossover exists and that it occurs\n",
      "within the normal life span of individuals within a population.6\n",
      "\n",
      "8.2 The ordinal probit model\n",
      "\n",
      "The dichotomous probit model is easily generalizable in the event our outcome\n",
      "variable is ordinal rather than dichotomous. In the event the outcome variable\n",
      "is ordinal with numerous categories (e.g., more than five), we may consider an\n",
      "OLS regression model. However, by definition ordinal variables have rankable\n",
      "outcome categories with unequal spacing between categories, which makes\n",
      "OLS regression theoretically inappropriate. For example, in health research,\n",
      "a four-category self-rated health measure—with categories Excellent, Good,\n",
      "Fair, and Poor—is a commonly used outcome variable. Although the order of\n",
      "the categories reflects an unmistakable progression from best health to worst\n",
      "5 Technically, it is the expected value for the age of death distribution, but in most\n",
      "\n",
      "modern societies, deaths increasingly cluster around this point.\n",
      "6 In all fairness, a key reason the crossover is assumed not to exist is that it is an\n",
      "\n",
      "artifact of age misreporting among older persons (see Elo and Preston 1994). Age\n",
      "misreporting is less problematic in a longitudinal study in which most individuals\n",
      "are observed at a younger age than we typically expect to find such misreporting.\n",
      "Nonetheless, we cannot easily control for such measurement error with the model\n",
      "we have used.\n",
      "\n",
      "\n",
      "\n",
      "218 8 Generalized Linear Models\n",
      "\n",
      "health, the interval between, say, Excellent and Good is not necessarily com-\n",
      "parable with the interval between Fair and Poor. Thus, OLS regression, which\n",
      "requires an interval level outcome, is an inappropriate model. Nonetheless, as\n",
      "the number of categories increases—and thus the interval width between any\n",
      "two categories shrinks—the difference between interval widths may become\n",
      "negligible and potentialize the legitimate use of OLS regression. We will con-\n",
      "sider this possibility in the example.\n",
      "\n",
      "8.2.1 Model development and parameter interpretation\n",
      "\n",
      "Whereas the likelihood function for the dichotomous probit model relied on\n",
      "the binomial distribution, the likelihood function for the ordinal probit model\n",
      "relies on the multinomial distribution. As we discussed in Chapter 2, the\n",
      "multinomial distribution is simply an extension of the binomial distribution\n",
      "for the case in which there are multiple p parameters being modeled rather\n",
      "than simply one. Thus, the likelihood function for the ordinal probit model\n",
      "is:\n",
      "\n",
      "L(P |Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "\n",
      " J∏\n",
      "\n",
      "j=1\n",
      "\n",
      "p\n",
      "I(yi=j)\n",
      "ij\n",
      "\n",
      "\n",
      " . (8.11)\n",
      "\n",
      "This representation may at first appear confusing, but it is a straightfor-\n",
      "ward extension of the binomial likelihood function in Equation 8.1. First, in\n",
      "the ordinal probit case, p is replaced by P to represent the fact that we are\n",
      "now dealing with more than simply the probability of an individual falling in\n",
      "one category versus its complement (e.g., Healthy versus Not): Now we have\n",
      "multiple possible categories (e.g., Excellent health, Good health, Fair health,\n",
      "Poor health). Second, we have two product symbols. The first constructs the\n",
      "joint sampling density for all individuals in the sample, just as the product\n",
      "in the binomial likelihood. The second is an extension to handle the fact that\n",
      "we now have J outcome categories rather than two. In the binomial likeli-\n",
      "hood function, a given individual’s contribution is pyii (1 − pi)\n",
      "\n",
      "1−yi , where pi\n",
      "represents the probability the individual registers a “1” (versus a 0) on the\n",
      "outcome. An individual who registers a “1” ends up only contributing the for-\n",
      "mer term, whereas an individual who registers a “0” ends up only contributing\n",
      "the latter term. In the multinomial version, the individual’s contribution is\n",
      "p\n",
      "\n",
      "I(yi=1)\n",
      "i1 p\n",
      "\n",
      "I(yi=2)\n",
      "i2 . . . p\n",
      "\n",
      "I(yi=J)\n",
      "iJ , where I(yi = j) is an indicator function indicat-\n",
      "\n",
      "ing that the individual’s response is in category j. Thus, the individual only\n",
      "ultimately contributes one term to the likelihood (all others drop, because the\n",
      "indicator function takes a value of 0).\n",
      "\n",
      "How do we incorporate the regression parameters (and X) into this model?\n",
      "As before, the pij correspond to integrals of the standard normal distribution.\n",
      "Consider the latent variable approach discussed above. If we assume once\n",
      "again that a latent variable y∗i underlies our observed ordinal measure, then\n",
      "we can expand the link equation presented earlier (see Equation 8.3) to:\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 219\n",
      "\n",
      "yi =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1 iff −∞ = τ1 ≤ y∗i < τ2\n",
      "2 iff τ2 ≤ y∗i < τ3\n",
      "...\n",
      "k iff τk ≤ y∗i < τk+1 = ∞.\n",
      "\n",
      "(8.12)\n",
      "\n",
      "Figure 8.9 presents a graphic depiction of the process outlined in the equa-\n",
      "tion. The latent distribution (y∗) is divided by thresholds (τ), with individuals’\n",
      "observed values of y determined by the location of their y∗ and the placement\n",
      "of the thresholds. For example, an individual whose value of y∗ is small enough\n",
      "to fall between τ1 and τ2 will respond “1” to the measured item.7\n",
      "\n",
      "As before, the link equation, given a distributional specification for the\n",
      "error term, implies an integral over the error distribution, but now the integral\n",
      "is bounded by thresholds that divide the latent y∗ into the observed ordinal\n",
      "“bins”:\n",
      "\n",
      "p(yi = j) = P (τj−1 −XTi β < ei < τj −X\n",
      "T\n",
      "i β) =\n",
      "\n",
      "∫ τj−XTi β\n",
      "τj−1−XTi β\n",
      "\n",
      "N(0, 1). (8.13)\n",
      "\n",
      "This result allows us to write the likelihood more succinctly as:\n",
      "\n",
      "L(β, τ |y) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "∫ τyi−XTi β\n",
      "τyi−1−X\n",
      "\n",
      "T\n",
      "i\n",
      "\n",
      "β\n",
      "\n",
      "N(0, 1) ≡\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "Φ(τyi −X\n",
      "T\n",
      "i β)− Φ(τyi−1 −X\n",
      "\n",
      "T\n",
      "i β).\n",
      "\n",
      "(8.14)\n",
      "To make the model fully Bayesian, we simply need to specify priors for the\n",
      "regression parameters and thresholds. Once again, I opt to use improper uni-\n",
      "form priors for all parameters, and so the posterior distribution is proportional\n",
      "to the likelihood.\n",
      "\n",
      "The interpretation of parameters from this model poses the same difficul-\n",
      "ties as described in the previous section. The metric for the coefficients in\n",
      "the ordinal probit model, just as in the dichotomous probit model, is the z\n",
      "scale, and thus the interpretation of the effect of the parameters is in terms of\n",
      "shifting the z score for an individual up or down β units for each unit increase\n",
      "in the corresponding variable. Alternative interpretations in terms of deter-\n",
      "mining the probability an individual falls in a particular category—or in a\n",
      "particular category or higher—can be derived (see Johnson and Albert 1999),\n",
      "although interest generally centers simply on the sign, relative magnitude, and\n",
      "statistical significance of the coefficient.\n",
      "\n",
      "As with the dichotomous logistic regression model, an ordinal logistic re-\n",
      "gression model coefficient can be exponentiated to reflect the change in the\n",
      "\n",
      "7 Although many texts begin their threshold subscripts at 0, I begin the threshold\n",
      "numbering at 1 for ease of translation into R, which does not allow 0 subscripts.\n",
      "\n",
      "\n",
      "\n",
      "220 8 Generalized Linear Models\n",
      "\n",
      "Latent Distribution (Y*)\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      " o\n",
      "\n",
      "f \n",
      "Y\n",
      "\n",
      "*\n",
      "\n",
      "τ1=− ∞\n",
      "τ2 τ3 τ4 τ5\n",
      "\n",
      "τ6=∞\n",
      "\n",
      "y=1\n",
      "\n",
      "y=2\n",
      "\n",
      "y=3\n",
      "\n",
      "y=4\n",
      "\n",
      "y=5\n",
      "\n",
      "Fig. 8.9. Depiction of latent distribution for Y ∗ and Y with thresholds superim-\n",
      "posed.\n",
      "\n",
      "relative odds for being in one category versus the next lower category asso-\n",
      "ciated with a one-unit increase in the covariate corresponding to the expo-\n",
      "nentiated coefficient (see Long 1997). Because this odds ratio applies to each\n",
      "category relative to the one immediately below it, the model is sometimes\n",
      "called a “proportional odds model.”\n",
      "\n",
      "8.2.2 Sampling from the posterior distribution for the parameters\n",
      "\n",
      "For an ordinal probit model, the algorithm for the dichotomous probit model\n",
      "only needs (1) to be extended to handle estimation of the thresholds that\n",
      "bound the categories and (2) to be adapted to simulate from doubly trun-\n",
      "cated normal distributions. Recall from the posterior density shown above\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 221\n",
      "\n",
      "that the parameters of the model include not only the regression coefficients,\n",
      "but also the category thresholds that “slice up” the latent distribution into\n",
      "the observed ordinal bins. In the dichotomous probit model, we fixed the sole\n",
      "threshold at 0. This constraint is necessary for the intercept to be identified;\n",
      "if the threshold is allowed to vary, the intercept can simply change without\n",
      "affecting the likelihood. In the ordinal probit model, we need to constrain one\n",
      "threshold parameter, but we can estimate the remainder of them (but see\n",
      "Johnson and Albert, 1999, who discuss using a proper prior on the thresh-\n",
      "olds to allow estimation of all of them). Thus, our Gibbs sampling algorithm\n",
      "must be extended to include simulating the thresholds from their conditional\n",
      "posterior distribution. Assuming uniform priors for the thresholds, the con-\n",
      "ditional distribution for threshold j is a uniform distribution on the interval\n",
      "between the maximum latent propensity (Y ∗) drawn for an individual in cat-\n",
      "egory j − 1 and the minimum latent propensity drawn for an individual in\n",
      "category j. Thus, the algorithm is extended as:\n",
      "\n",
      "1. Establish starting values for all parameters. Now the parameter vector\n",
      "also includes thresholds, which need some reasonable starting values. A\n",
      "simple approach to obtaining starting values for threshold j is to use the\n",
      "inverse normal cumulative distribution function (CDF) for the proportion\n",
      "of observations in category j or below.\n",
      "\n",
      "2. Sample latent propensities (Y ∗) from truncated normal distributions.\n",
      "Whereas with the dichotomous probit in which the propensities were sam-\n",
      "pled from above 0 if the response was a 1 and below 0 if the response was\n",
      "a 0, the truncation points in the ordinal probit model are the thresholds\n",
      "that bound the ordinal category in which the response falls.\n",
      "\n",
      "3. Sample threshold j(∀j) from uniform densities on the interval [max(y :\n",
      "y ∈ j − 1),min(y : y ∈ j)].\n",
      "\n",
      "4. Sample regression coefficients from their conditional distribution: β ∼\n",
      "(X ′X)−1(X ′Y ∗).\n",
      "\n",
      "5. Repeat until enough draws are obtained.\n",
      "\n",
      "Notice that the only two differences between this algorithm and the one\n",
      "for the dichotomous probit model are (1) the inclusion of the step in which\n",
      "the thresholds are sampled, and (2) the sampling of the latent propensities\n",
      "from doubly truncated normal distributions implied by the introduction of\n",
      "additional thresholds.\n",
      "\n",
      "Sampling from doubly-truncated normal distributions using the inversion\n",
      "method requires relatively little change from the inversion method discussed\n",
      "in the previous section. In the polytomous case, we need to renormalize the\n",
      "area between thresholds rather than simply computing the area from −∞ to\n",
      "0 or from 0 to ∞. Thus, determining the bounded area requires two calls to\n",
      "the function that computes Φ. To sample from a particular category k the\n",
      "appropriate renormalized density is:\n",
      "\n",
      "z[τk,τk+1] ∼ Φ\n",
      "−1 [u (Φ(τk+1)− Φ(τk)) + Φ(τk)] , (8.15)\n",
      "\n",
      "\n",
      "\n",
      "222 8 Generalized Linear Models\n",
      "\n",
      "where z[τk,τk+1] is the latent draw from category k, τk+1 is the upper threshold\n",
      "for category k, and u is a random draw from the U(0, 1) distribution. Since u\n",
      "can range from 0 to 1, z will range from τk to τk+1.\n",
      "\n",
      "An even easier method for simulating from doubly truncated normal dis-\n",
      "tributions in R is to set the minimum and maximum values for the uniform\n",
      "draw equal to the current values of the normal integrals implied by the current\n",
      "value of the thresholds. For example, if τ2 = 0 and τ3 = 1, the respondent’s\n",
      "value of XTi β = 0, and his/her value of y = 2—meaning that the observed\n",
      "response falls in the category bounded by τ2 and τ3, the minimum and max-\n",
      "imum for the uniform draw would be .5 and .84, respectively. These are the\n",
      "values of the cumulative distribution function at 0 and 1, and so the uniform\n",
      "draw representing the cumulative area under the density at which we want a\n",
      "value z is between .5 and .84.\n",
      "\n",
      "Below is the R program that implements the ordered probit model for a\n",
      "five-category ordinal outcome:\n",
      "\n",
      "#R program for ordinal probit model\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\bookheal.dat\")[,1:7])\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\bookheal.dat\")[,8])\n",
      "\n",
      "t=matrix(0,6)\n",
      "\n",
      "t[1]=-Inf; t[2]=0; t[6]=Inf\n",
      "\n",
      "t[3]=qnorm(sum(y<=2)/nrow(y),-qnorm(sum(y==1)/nrow(y),0,1),1)\n",
      "\n",
      "t[4]=qnorm(sum(y<=3)/nrow(y),-qnorm(sum(y==1)/nrow(y),0,1),1)\n",
      "\n",
      "t[5]=qnorm(sum(y<=4)/nrow(y),-qnorm(sum(y==1)/nrow(y),0,1),1)\n",
      "\n",
      "b=matrix(0,7); vb=solve(t(x)%*%x); ch=chol(vb)\n",
      "\n",
      "write(c(1,0,0,0,0,0,0,0,0,0,0),file=\"c:\\\\oprob_gibbs.out\",\n",
      "\n",
      "append=T, ncolumns=11)\n",
      "\n",
      "for(i in 2:100000){\n",
      "\n",
      "#simulate latent data from truncated normal distributions\n",
      "\n",
      "xb=as.matrix(x%*%b)\n",
      "\n",
      "ystar=qnorm(runif(nrow(y),min=pnorm(t[y],xb,1),\n",
      "\n",
      "max=pnorm(t[y+1],xb,1)),mean=xb,1)\n",
      "\n",
      "#simulate thresholds\n",
      "\n",
      "for(k in 3:5){t[k]=runif(1,min=max(ystar[y==k-1]),\n",
      "\n",
      "max=min(ystar[y==k]))}\n",
      "\n",
      "#simulate beta vector from appropriate mvn\n",
      "\n",
      "b=vb%*%(t(x)%*%ystar) + t((rnorm(7,mean=0,sd=1))%*%ch)\n",
      "\n",
      "write(c(i,t(b),t[3],t[4],t[5]), file=\"c:\\\\oprob_gibbs.out2\",\n",
      "\n",
      "append=T, ncolumns=11)\n",
      "\n",
      "if(i%%10==0){print(c(i,t(b),t[3],t[4],t[5]),digits=2)}\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 223\n",
      "\n",
      "}\n",
      "\n",
      "The program follows the same general structure as the one for the di-\n",
      "chotomous probit model. First, the data are read from the file, variables are\n",
      "defined and initiated, and the starting values for all parameters are written\n",
      "to the output file. Notice that there are six thresholds defined. The first and\n",
      "last are set to +∞ and −∞, respectively, and the second threshold is set to\n",
      "0 to identify the model. The remaining three thresholds are initialized based\n",
      "on the proportion of the sample falling in each category of the outcome. The\n",
      "mean argument to the qnorm function is -qnorm(sum(y==1)/nrow(y),0,1)\n",
      "because the normal distribution must be shifted so that the proportion of\n",
      "cases falling below the second threshold of 0 is equal to the proportion of\n",
      "individuals who fall in the first category of the outcome variable.\n",
      "\n",
      "After all variables are initialized, the Gibbs sampler begins. First, given\n",
      "the current values of the regression parameters and thresholds, latent data\n",
      "are simulated to replace the observed ordinal outcome. As we did in the di-\n",
      "chotomous probit model program, we make use of R’s ability to handle entire\n",
      "vectors simultaneously, and so the latent data simulation is performed in one\n",
      "step. Notice that I use the second method for simulating from the doubly\n",
      "truncated normal distribution: The minimum and maximum values for the\n",
      "latent data for an individual are determined by applying the qnorm function\n",
      "to the thresholds that bound the observed response.\n",
      "\n",
      "Once the latent data have been simulated, the three free thresholds are\n",
      "simulated from uniform distributions on the interval between the largest latent\n",
      "value simulated for the category below the threshold and the smallest latent\n",
      "value simulated for the category above the threshold. Finally, given a complete\n",
      "set of latent data, the regression parameters are simulated as before.\n",
      "\n",
      "8.2.3 Ordinal probit model example: Black–white differences in\n",
      "health\n",
      "\n",
      "For an example of the ordinal probit model, I extend the example in the pre-\n",
      "vious section using self-rated health as the outcome measure. Additionally,\n",
      "rather than using a person-year file, I use only the 1992 wave of the study.\n",
      "I ran the Gibbs sampler above several times for 100,000 iterations each time\n",
      "using different starting values for the threshold parameters. Although there\n",
      "was no need to run the Gibbs sampler for so many iterations, the algorithm\n",
      "is extremely fast, and thus there is little cost to doing so. The results of the\n",
      "three runs suggested that the regression parameters converged quickly, but\n",
      "the threshold parameters converged slowly. Figure 8.10 shows trace plots of\n",
      "the three estimated threshold parameters from the first run of the algorithm\n",
      "in which the thresholds were initialized at values that were higher than they\n",
      "should be. As the figure indicates, the threshold parameters did not converge\n",
      "until after 20,000 or so iterations. Slow convergence—and mixing—of thresh-\n",
      "old parameters occurs when the sample size is large enough that the minimum\n",
      "\n",
      "\n",
      "\n",
      "224 8 Generalized Linear Models\n",
      "\n",
      "simulated latent score for one category and the maximum simulated score for\n",
      "the prior category are similar, so that the conditional uniform density for the\n",
      "threshold is narrow. A solution to this dilemma is to use Cowles’ algorithm\n",
      "(1996). In Cowles’ algorithm, the Gibbs sampling step for sampling the thresh-\n",
      "olds is replaced with an MH step. I do not present an example here using this\n",
      "alternative, primarily because the speed of the Gibbs sampler in R nullifies\n",
      "the need to seek out a more efficient alternative (but see Chapter 10).\n",
      "\n",
      "0 200 400 600 800 1000\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "2\n",
      ".5\n",
      "\n",
      "3\n",
      ".0\n",
      "\n",
      "Iteration\n",
      "\n",
      "T\n",
      "h\n",
      "\n",
      "re\n",
      "sh\n",
      "\n",
      "o\n",
      "ld\n",
      "\n",
      "s\n",
      "\n",
      "Fig. 8.10. Trace plot for threshold parameters in ordinal probit model.\n",
      "\n",
      "Figure 8.11 replicates the results of Figure 8.4 in showing the distributions\n",
      "of latent scores for several cases. Specifically, the figure shows the latent dis-\n",
      "tributions for five sample members, each of whom had a different observed\n",
      "response to the health item. In order to better illustrate the process of draw-\n",
      "ing latent scores from doubly truncated normal distributions, the figure shows\n",
      "all five latent trait distributions together. The far-left distribution is the la-\n",
      "tent distribution for an individual who responded “1” to the item; the second\n",
      "distribution is for an individual who responded “2” on the observed item;\n",
      "and so on. Although the heights of the densities are not consistent because\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 225\n",
      "\n",
      "the widths of the densities vary due to placement of the thresholds, the five\n",
      "distributions appear to be “slices” of an overall latent trait distribution. It is\n",
      "important to note that each individual’s distribution appears to overlap the\n",
      "adjacent distributions. The distributions overlap because the thresholds are\n",
      "updated at every iteration—when a threshold is low, the acceptable range for\n",
      "the latent traits in the categories split by the threshold is affected.\n",
      "\n",
      "−2 −1 0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".5\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "1\n",
      ".5\n",
      "\n",
      "Y*\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "y=1\n",
      "y=2\n",
      "\n",
      "y=3\n",
      "y=4\n",
      "\n",
      "y=5\n",
      "\n",
      "Fig. 8.11. Distributions of latent health scores for five persons with different ob-\n",
      "served values of y.\n",
      "\n",
      "Table 8.4 shows the results of the ordinal probit model algorithm, along\n",
      "with maximum likelihood results. With the exception of the intercept and the\n",
      "thresholds, the maximum likelihood and Bayesian (posterior mean) estimates\n",
      "are virtually identical. Men have better health, and nonwhites have poorer\n",
      "health. Persons who have been hospitalized have worse health, as do persons\n",
      "with more physical conditions. Age, and doctor visits do not have a significant\n",
      "effect on health, but the body mass index has a marginally significant effect,\n",
      "even after controlling for the physical health measures.\n",
      "\n",
      "\n",
      "\n",
      "226 8 Generalized Linear Models\n",
      "\n",
      "The difference between the maximum likelihood and Bayesian estimates for\n",
      "the intercept and thresholds reflects different parameterizations of the model.\n",
      "STATA (the package used to obtain the maximum likelihood estimates) fixes\n",
      "the intercept for the model to 0 but estimates all thresholds. In contrast, we\n",
      "allowed the intercept to be estimated but constrained the second threshold to\n",
      "be equal to 0. The only difference in these parameterizations is in the location\n",
      "of the latent distribution: For our parameterization, the latent distribution is\n",
      "shifted to the right relative to the latent distribution for STATA’s parame-\n",
      "terization. Notice that subtracting our intercept from each of the thresholds\n",
      "(and the intercept itself) yields roughly the same estimates for the thresh-\n",
      "olds obtained by STATA. The only striking difference, ultimately, between\n",
      "the STATA results and the Gibbs sampling results is in the psuedo-R2. Al-\n",
      "though the Gibbs sampler estimate ranges between .105 and .182, with a mean\n",
      "of .144, the STATA estimate is substantially smaller at .049. The reason for\n",
      "this seemingly large discrepancy is that there are many different methods for\n",
      "calculating pseudo-r-square, and the method I used differs from STATA’s.8\n",
      "\n",
      "Table 8.4. Maximum likelihood and Gibbs sampling results for ordinal probit model\n",
      "example.\n",
      "\n",
      "Variable MLE (STATA) Gibbs Sampler Estimates\n",
      "\n",
      "Intercept 0 (fixed) 1.86(.15)***\n",
      "Age −0.02(.002)*** −0.02(.002)***\n",
      "Female −0.001(.03) −0.001(.04)\n",
      "Black −0.91(.33)** −0.90(.33)**\n",
      "Years of Schooling 0.09(.01)*** 0.09(.01)***\n",
      "South −0.13(.04)** −0.13(.04)***\n",
      "Age-by-Black 0.008(.005) 0.008(.005)#\n",
      "τ1 −∞ (fixed) −∞ (fixed)\n",
      "τ2 −1.85(.15) 0 (fixed)\n",
      "τ3 −1.04(.15) 0.83(.03)\n",
      "τ4 −0.15(.15) 1.72(.03)\n",
      "τ5 0.79(.15) 2.65(.03)\n",
      "τ6 ∞ (fixed) −∞ (fixed)\n",
      "Pseudo-R2 0.049 .14[.11, .18]\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The p-values are based on one-\n",
      "sided tail probabilities that the parameter exceeds 0 truncated to the classical cut-\n",
      "points of #p < .1, *p < .05, **p < .01.\n",
      "\n",
      "The approach used here to produce a pseudo-R2 is the same approach as\n",
      "described earlier for the dichotomous probit model. In OLS regression, one\n",
      "approach to computing R2 is to subtract the ratio of the error variance to the\n",
      "8 STATA uses r2 = 1−LL/L0, where L0 is the maximum likelihood function value\n",
      "\n",
      "for a model with only an intercept and LL is the value for the full model.\n",
      "\n",
      "\n",
      "\n",
      "8.2 The ordinal probit model 227\n",
      "\n",
      "total variance for the outcome variable—the proportion of the total variance\n",
      "that is error variance, in other words—from 1. The only difference, therefore,\n",
      "between the OLS approach and the one used in the dichotomous and ordinal\n",
      "probit models is that the variance of the latent data are used, rather than the\n",
      "variance of the observed outcome variable as is used in OLS. The net result is\n",
      "that the distribution for R2 will tend to be slightly wider in the probit models\n",
      "than in the OLS model, reflecting the additional uncertainty introduced by\n",
      "the crude measurement of the latent data.\n",
      "\n",
      "One issue that I discussed earlier in the chapter is whether, with an ordi-\n",
      "nal outcome variable, it is necessary to use some sort of GLM as opposed to\n",
      "simply using OLS regression. Although it is true that the differences between\n",
      "OLS and GLM results tend to decrease as the number of outcome categories\n",
      "increases, the actual number of categories needed in the outcome may change\n",
      "from model to model. Thus, if we are really interested in deciding whether to\n",
      "use a GLM or OLS regression, we may choose to estimate both models and de-\n",
      "cide whether they produce a substantially different fit. The classical approach\n",
      "offers little in the way of formal comparison between nonnested models; the\n",
      "Bayesian approach, on the other hand, offers unlimited ability to make for-\n",
      "mal comparisons. One such comparison that can be made is to compare the\n",
      "distributions for R2 from the OLS model to the pseudo-R2 distribution for\n",
      "the ordinal probit model. Figure 8.12 shows the distributions of R2 for both\n",
      "the ordinal probit model and an OLS model. As the figure shows, it appears\n",
      "that the models fit more-or-less equally well, with the ordinal probit fitting\n",
      "perhaps only slightly better. The mean R2 for the probit and OLS models\n",
      "were .144 and .138, respectively. From a Bayesian view, these distributions\n",
      "can be formally compared. The probability that the ordinal probit model fits\n",
      "better than the OLS model is p(R2probit > R\n",
      "\n",
      "2\n",
      "OLS), which can be computed\n",
      "\n",
      "as the proportion of R2 values in the ordinal probit distribution for R2 that\n",
      "exceed the maximum R2 value in the OLS distribution for R2. This value is\n",
      "0. The probability that the OLS model fits worse than the probit model is\n",
      ".00025. Thus, the results suggest that the two models are indistinguishable.\n",
      "\n",
      "Can comparisons be made across coefficients in the two models? Unfor-\n",
      "tunately, no: Given that the probit model assumes that the error variance\n",
      "is 1, while the error variance parameter is estimated in the OLS regression\n",
      "model, the coefficients cannot be directly compared across models. Bayesians\n",
      "have no trouble making comparisons across probability distributions, but we\n",
      "should always determine whether our comparisons make sense. In comparing\n",
      "the probit and OLS models, the coefficients are in different scales, and so the\n",
      "coefficients cannot be directly compared.9\n",
      "\n",
      "The results of the ordinal probit model can be used to produce a distribu-\n",
      "tion of a health crossover age, just as we did in the dichotomous probit model,\n",
      "\n",
      "9 Interestingly, however, in this particular example, the error variance parameter\n",
      "is close to 1 in the OLS regression model, and so the coefficients and standard\n",
      "errors are remarkably similar.\n",
      "\n",
      "\n",
      "\n",
      "228 8 Generalized Linear Models\n",
      "\n",
      "0.05 0.10 0.15 0.20\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "0\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "R−Square\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "OLS\n",
      "Probit\n",
      "\n",
      "Fig. 8.12. Distributions of R2 and pseudo-R2 from OLS and probit models, respec-\n",
      "tively.\n",
      "\n",
      "using the distributions for the “black” parameter and the age-by-black inter-\n",
      "action parameter. Table 8.5 provides a number of summaries for the health\n",
      "crossover age. The central age (mean or median) for the health crossover is\n",
      "substantially higher than the mortality crossover age. In fact, the various sum-\n",
      "mary measures suggest that there is a low probability that a health crossover\n",
      "occurs during the normal human life span. Taken together, the results of the\n",
      "dichotomous probit and ordinal probit models indicate that, although mor-\n",
      "tality rates for blacks fall below those for whites in later life, blacks have\n",
      "poorer health at all ages. A stronger approach to reaching this conclusion\n",
      "would require a model that simultaneously considers health and mortality—a\n",
      "multivariate model. Multivariate models are the subject of Chapter 10.\n",
      "\n",
      "8.3 Conclusions\n",
      "\n",
      "In this chapter we have covered two commonly used generalized linear\n",
      "models—the dichotomous probit model and the ordinal probit model. For\n",
      "\n",
      "\n",
      "\n",
      "8.4 Exercises 229\n",
      "\n",
      "Table 8.5. Various summary measures for the black-white health crossover age.\n",
      "\n",
      "Description of Summary Measure Summary (Age)\n",
      "\n",
      "Maximum Likelihood estimate of crossover age 113.8\n",
      "Posterior Mean (all values) 157.7\n",
      "Posterior Median (all values included) 112.2\n",
      "Posterior Mean (of values 0-200) 117.1\n",
      "95% Interval [-236.0, 680.0]\n",
      "90% Interval [75.7, 370.6]\n",
      "p(0 < Age < 200) .831\n",
      "p(0 < Age < 120) .535\n",
      "p(0 < Age < 78) .003\n",
      "p(80 < Age < 100) .264\n",
      "\n",
      "each model, we discussed a single Gibbs sampler. A key difference between\n",
      "these samplers and that for the linear regression model in the last chapter is\n",
      "the need to sample latent data from truncated normal distributions. Thus, we\n",
      "discussed three strategies for performing such simulation.\n",
      "\n",
      "We also spent considerable time in this chapter demonstrating how to make\n",
      "use of the Gibbs sampler to generate samples from distributions of quantities\n",
      "not directly included in the model (e.g., the crossover age), as well as how\n",
      "to perform basic residual analyses in the probit model. We will continue dis-\n",
      "cussing the benefits of the Bayesian approach in the remaining two chapters,\n",
      "but in the next chapter—which covers hierarchical modeling—we will pri-\n",
      "marily focus on the ease with which the Bayesian approach can incorporate\n",
      "hierarchical structure to the data and model.\n",
      "\n",
      "8.4 Exercises\n",
      "\n",
      "1. Develop an MH algorithm for the dichotomous probit model.\n",
      "2. Develop a rejection sampler for simulating from a truncated normal dis-\n",
      "\n",
      "tribution using a triangular density, rather than a uniform density, as\n",
      "discussed in Section 8.1.3.\n",
      "\n",
      "3. Starting with the Gibbs sampler shown earlier for the dichotomous probit\n",
      "model, modify it to make it a hybridized Gibbs-MH algorithm. That is,\n",
      "continue to sample the latent data at each iteration (a Gibbs sampling\n",
      "step), but update the parameters using MH steps.\n",
      "\n",
      "4. Write an algorithm to estimate the parameters in the dichotomous probit\n",
      "model using the binomial likelihood function-with link function. The like-\n",
      "lihood function will contain normal integrals. Compare results between\n",
      "this algorithm and the Gibbs sampler. Are there any differences in the\n",
      "speed of convergence? Are the parameter estimates and standard errors\n",
      "comparable?\n",
      "\n",
      "\n",
      "\n",
      "230 8 Generalized Linear Models\n",
      "\n",
      "5. Construct an MH algorithm to estimate the parameters in the ordinal\n",
      "probit model.\n",
      "\n",
      "6. There are several alternative methods for calculating pseudo-R2 values in\n",
      "GLMs. One such approach in a dichotomous probit model is to construct\n",
      "a two-by-two table of observed and predicted outcomes, where individuals\n",
      "with Xβ > 0 assigned to have a predicted score of “1” (otherwise they\n",
      "receive a predicted score of “0”). The R2 is then simply the proportion\n",
      "of correctly classified individuals. Under a Bayesian approach, we would\n",
      "obtain multiple values for β and, hence, multiple possible R2 values. Per-\n",
      "form this process and compare the result with what is obtained using\n",
      "the method I described in the chapter. How does the distribution of the\n",
      "outcome variable influence the difference between these two types of R2s?\n",
      "\n",
      "7. Develop a strategy for handling missing data in the probit model (dichoto-\n",
      "mous or ordinal). Assume the data are MAR.\n",
      "\n",
      "\n",
      "\n",
      "9\n",
      "\n",
      "Introduction to Hierarchical Models\n",
      "\n",
      "One of the important features of a Bayesian approach is the relative ease\n",
      "with which hierarchical models can be constructed and estimated using Gibbs\n",
      "sampling. In fact, one of the key reasons for the recent growth in the use of\n",
      "Bayesian methods in the social sciences is that the use of hierarchical models\n",
      "has also increased dramatically in the last two decades.\n",
      "\n",
      "Hierarchical models serve two purposes. One purpose is methodological;\n",
      "the other is substantive. Methodologically, when units of analysis are drawn\n",
      "from clusters within a population (communities, neighborhoods, city blocks,\n",
      "etc.), they can no longer be considered independent. Individuals who come\n",
      "from the same cluster will be more similar to each other than they will be\n",
      "to individuals from other clusters. Therefore, unobserved variables may in-\n",
      "duce statistical dependence between observations within clusters that may\n",
      "be uncaptured by covariates within the model, violating a key assumption\n",
      "of maximum likelihood estimation as it is typically conducted when indepen-\n",
      "dence of errors is assumed. Recall that a likelihood function, when observations\n",
      "are independent, is simply the product of the density functions for each ob-\n",
      "servation taken over all the observations. However, when independence does\n",
      "not hold, we cannot construct the likelihood as simply. Thus, one reason for\n",
      "constructing hierarchical models is to compensate for the biases—largely in\n",
      "the standard errors—that are introduced when the independence assumption\n",
      "is violated. See Ezell, Land, and Cohen (2003) for a thorough review of the\n",
      "approaches that have been used to correct standard errors in hazard model-\n",
      "ing applications with repeated events, one class of models in which repeated\n",
      "measurement yields hierarchical clustering.\n",
      "\n",
      "In addition to the methodological need for hierarchical models, substan-\n",
      "tively we may believe that there are differences in how predictors in a regres-\n",
      "sion model influence an outcome of interest across clusters, and we may wish\n",
      "to model these differences. In other words, the influence of predictors may\n",
      "be context-dependent, a notion that is extremely important and relevant to a\n",
      "social scientific—especially sociological—understanding of the world. For ex-\n",
      "ample, the emergence of hierarchical modeling in education research occurred\n",
      "\n",
      "\n",
      "\n",
      "232 9 Introduction to Hierarchical Models\n",
      "\n",
      "because there is a natural nesting of students within classes (and classes within\n",
      "schools, schools within communities, and so on), and grades, test performance,\n",
      "etc. may be dependent on teacher quality, making students in one class dif-\n",
      "ferent from those in another class. In other words, student performance may\n",
      "be dependent on the teacher—the environmental context of classes.\n",
      "\n",
      "In this chapter, I discuss simple hierarchical models in general as well as hi-\n",
      "erarchical linear regression models. I conclude the chapter with a brief discus-\n",
      "sion of terminological issues that make hierarchical modeling seem mysterious\n",
      "and complicated. I recommend Gelman et al. (1995) for an in-depth exposi-\n",
      "tion of the Bayesian approach to a variety of hierarchical models, both the\n",
      "simple hierarchical models discussed in the next section as well as hierarchical\n",
      "regression models discussed later in the chapter. I recommend Raudenbush\n",
      "and Bryk (2002) and Snijders and Bosker (1999) for thorough coverage of the\n",
      "classical approach to hiearchical linear regression models.\n",
      "\n",
      "9.1 Hierarchical models in general\n",
      "\n",
      "Hierarchical models are models in which there is some sort of hierarchical\n",
      "structure to the parameters and potentially to the covariates if the model is\n",
      "a regression model. I begin by discussing the simpler case in which the model\n",
      "of interest is not a regression model with covariates, but rather is simply\n",
      "hierarchical in the parameters.\n",
      "\n",
      "Recall that Bayes’ Theorem is often expressed as:\n",
      "\n",
      "p(θ | data)︸ ︷︷ ︸ ∝ p(data | θ)︸ ︷︷ ︸ × p(θ)︸︷︷︸\n",
      "posterior ∝ likelihood × prior\n",
      "\n",
      "This equation itself reveals a simple hierarchical structure in the parameters,\n",
      "because it says that a posterior distribution for a parameter is equal to a\n",
      "conditional distribution for data under the parameter (first level) multiplied\n",
      "by the marginal (prior) probability for the parameter (a second, higher, level).\n",
      "Put another way, the posterior distribution is the prior distribution weighted\n",
      "by the observed information.\n",
      "\n",
      "This hierarchical structure of the parameters need not stop at one higher\n",
      "level; instead, the conditioning structure in theory can continue ad infini-\n",
      "tum. For instance, suppose we have a model that contains an added layer\n",
      "of hierarchy. Suppose we have J observations within each of G groups:\n",
      "y11, . . . , yJ1, y12, . . . , yJ2, . . . , y1G, . . . , yJG, and we assume that the data are\n",
      "distributed within groups according to some distribution Q with parameter\n",
      "θ, but that each group has its own parameter (θg). Thus:\n",
      "\n",
      "yig ∼ Q(θg).\n",
      "\n",
      "\n",
      "\n",
      "9.1 Hierarchical models in general 233\n",
      "\n",
      "Suppose we assume further that these parameters θg arise from a common dis-\n",
      "tribution W with parameter γ (this parameter is called a “hyperparameter”).\n",
      "So:\n",
      "\n",
      "θg ∼ W (γ).\n",
      "\n",
      "Finally, assume γ has some vague distribution like a uniform:\n",
      "\n",
      "γ ∼ U(−100, 100).\n",
      "\n",
      "A posterior distribution for all unknown parameters would then be (after\n",
      "substituting the densities Q and W into the conditional structure below):\n",
      "\n",
      "p(γ, θ|y) ∝ p(y | θ, γ)p(θ | γ)p(γ).\n",
      "\n",
      "To see how this hierarchical structure “works,” notice that the last two terms\n",
      "here [p(θ | γ)p(γ)], when multiplied together, yield a joint distribution for γ\n",
      "and θ [p(θ , γ)]. Thus, we are left with a marginal joint distribution for the\n",
      "two parameters, which is then multiplied by a sampling density for the data\n",
      "[p(y | θ , γ)]. Bayes’ theorem tells us that the multiple of this marginal joint\n",
      "density for the parameters and the sampling density for the data, given the\n",
      "parameters, yields a posterior density for all of the parameters.\n",
      "\n",
      "Ultimately we might not be interested much in the posterior distributions\n",
      "for the group level parameters (θg), but rather in the posterior distribution\n",
      "for the hyperparameter γ that structures the distribution of the group level\n",
      "parameters. In other words, we may be interested only in the marginal distri-\n",
      "bution for γ:\n",
      "\n",
      "p(γ|y) ∝\n",
      "∫\n",
      "\n",
      "p(y|θ, γ)p(θ|γ)p(γ)dθ.\n",
      "\n",
      "As we have discussed throughout the last several chapters, this integration is\n",
      "performed stochastically via MCMC methods as we sample from the condi-\n",
      "tional posterior distributions for each parameter.\n",
      "\n",
      "This result demonstrates the simplicity with which a Bayesian approach\n",
      "can handle hierarchical structure in data or parameters. We could very easily,\n",
      "if desired, add subsequent layers to the structure, and we can also break each\n",
      "layer of the structure into regression components.\n",
      "\n",
      "9.1.1 The voting example redux\n",
      "\n",
      "In Chapter 3, I illustrated Bayes’ Theorem with a voting example from 2004\n",
      "pre-election polls. In that example, we considered the posterior probability\n",
      "that Kerry would win the election in Ohio using the most recent poll as\n",
      "the current data and data from three previous polls as prior information.\n",
      "We assumed a binomial likelihood function/sampling density for the current\n",
      "polling data (x) given the proportion of voters who would vote for Kerry (K),\n",
      "\n",
      "\n",
      "\n",
      "234 9 Introduction to Hierarchical Models\n",
      "\n",
      "and we used a beta distribution as the prior for K, with the number of votes\n",
      "for Kerry and Bush in the previous polls being represented by the parameters\n",
      "α and β, respectively. To summarize, our posterior density was:\n",
      "\n",
      "p(K|α, β, X) ∝ K556(1−K)511︸ ︷︷ ︸ K941(1−K)1007︸ ︷︷ ︸ .\n",
      "current data previous poll data\n",
      "(likelihood) (prior)\n",
      "\n",
      "In the original example I noted that, although the four polls we used\n",
      "appeared to show some trending, complete data from all available polls from\n",
      "various polling organizations did not suggest any trending, justifying our com-\n",
      "bination of the previous pollng data into a single prior distribution for α and\n",
      "β. As an alternative approach, without trending, the polls could be considered\n",
      "as separate samples drawn from the same population, each one providing con-\n",
      "ditionally independent information regarding the parameters α and β. In that\n",
      "case, we could consider that each poll’s results were the result of a unique,\n",
      "poll-specific parameter Ki, with the Ki being random realizations from the\n",
      "beta distribution with hyperparameters α and β. This approach recasts the\n",
      "voting example as a hierarchical model with the following structure:\n",
      "\n",
      "p(α, β, K|X) ∝ p(X|K)︸ ︷︷ ︸ p(K|α, β)︸ ︷︷ ︸ p(α, β)︸ ︷︷ ︸ .\n",
      "likelihood prior hyperprior\n",
      "\n",
      "Here, and throughout the remainder of the chapter, I suppress notation in\n",
      "the conditional distributions when a particular quantity does not directly\n",
      "depend on a higher level parameter. For example, the likelihood function here\n",
      "ultimately depends on the hyperparameters α and β; however, it only depends\n",
      "on these parameters through the prior for K, and so, I do not spell out the\n",
      "complete likelihood as p(X|K, α, β).\n",
      "\n",
      "The likelihood portion of the model is the product of the sampling densities\n",
      "for the four polls:\n",
      "\n",
      "p(X|K) ∝\n",
      "4∏\n",
      "\n",
      "i=1\n",
      "\n",
      "Kxii (1−Ki)\n",
      "ni−xi .\n",
      "\n",
      "The prior densities for each K (K1 . . .K4) are beta densities; their product is\n",
      "the full prior density:\n",
      "\n",
      "p(K|α, β) ∝\n",
      "4∏\n",
      "\n",
      "i=1\n",
      "\n",
      "(\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")\n",
      "Kα−1i (1−Ki)\n",
      "\n",
      "β−1.\n",
      "\n",
      "Finally, we must establish hyperpriors for the hyperparameters α and β. How-\n",
      "ever, before we consider the form of the hyperprior, let’s consider the full\n",
      "expression for the posterior density:\n",
      "\n",
      "\n",
      "\n",
      "9.1 Hierarchical models in general 235\n",
      "\n",
      "p(α, β, K|x) ∝(∏4\n",
      "i=1 K\n",
      "\n",
      "xi\n",
      "i (1−Ki)\n",
      "\n",
      "ni−xi\n",
      ")(∏4\n",
      "\n",
      "i=1\n",
      "\n",
      "(\n",
      "Γ (α+β)\n",
      "\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")\n",
      "Kα−1i (1−Ki)\n",
      "\n",
      "β−1\n",
      ")\n",
      "\n",
      "p(α, β).\n",
      "\n",
      "We can simplify this posterior distribution by combining like products as\n",
      "follows:\n",
      "\n",
      "p(α, β, K|x) ∝\n",
      "\n",
      "(\n",
      "Γ (α+β)\n",
      "\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4 (∏4\n",
      "i=1 K\n",
      "\n",
      "xi+α−1\n",
      "i (1−Ki)\n",
      "\n",
      "ni−xi+β−1\n",
      ")\n",
      "\n",
      "p(α, β).\n",
      "(9.1)\n",
      "\n",
      "The key difference between the current approach and as it was presented in\n",
      "the original example in Chapter 3 is that the current data were assumed to\n",
      "be simply the most recent polling data, and the previous three polls were\n",
      "combined and assumed to be fixed quantities representing the values of α\n",
      "and β. Under the current approach, in contrast, the previous polling data—\n",
      "rather than being treated as fixed prior information—are also considered to\n",
      "arise from a random process governed by the hyperparameters α and β. When\n",
      "these parameters were assumed to be fixed, the posterior density only involved\n",
      "the single parameter K. Now, however, the full posterior involves each Ki in\n",
      "addition to α and β. Before, the leading expression involving the gamma\n",
      "function [Γ (α + β)/(Γ (α)Γ (β))] could be dropped as a normalizing constant,\n",
      "because α and β were, in fact, constant. However, under the hierarchical\n",
      "approach they are now considered random variables, and terms involving them\n",
      "cannot simply be dropped. Indeed, although the individual K parameters are\n",
      "still of interest, interest centers primarily on α and β, which are thought to\n",
      "be the population parameters governing the proportion of voters who would\n",
      "vote for Kerry and which drive each individual poll result.\n",
      "\n",
      "A Gibbs sampling strategy, then, should involve sampling the α, β, and\n",
      "each K from their conditional posterior distributions. The conditional pos-\n",
      "terior distributions for each K, after eliminating terms in the posterior in\n",
      "Equation 9.1 that do not involve them, are easily seen to be beta distribu-\n",
      "tions with parameters A = xi + α and B = ni − xi + β:\n",
      "\n",
      "p(Ki|α, β, xi) ∝ Kxi+α−1i (1−Ki)\n",
      "ni−xi+β−1.\n",
      "\n",
      "The conditional posterior distributions for α and β are not as simple. Con-\n",
      "sider the posterior for α. If we eliminate terms not involving α, the posterior\n",
      "for α is:\n",
      "\n",
      "(\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4 4∏\n",
      "i=1\n",
      "\n",
      "Kxi+α−1i p(α, β).\n",
      "\n",
      "This posterior can be simplified considerably if we use a “trick” to allow the\n",
      "combination of the exponents. If we take the log and exponentiate simultane-\n",
      "ously, we obtain:\n",
      "\n",
      "\n",
      "\n",
      "236 9 Introduction to Hierarchical Models\n",
      "\n",
      "(\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4\n",
      "exp\n",
      "\n",
      "{\n",
      "ln\n",
      "\n",
      "(\n",
      "4∏\n",
      "\n",
      "i=1\n",
      "\n",
      "Kxi+α−1i\n",
      "\n",
      ")}\n",
      "p(α, β).\n",
      "\n",
      "The exponents can be brought down in front of the logarithm, the product of\n",
      "the logs become sums, and we obtain:\n",
      "\n",
      "(\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4\n",
      "exp\n",
      "\n",
      "{\n",
      "4∑\n",
      "\n",
      "i=1\n",
      "\n",
      "(xi + α− 1) ln Ki\n",
      "\n",
      "}\n",
      "p(α, β).\n",
      "\n",
      "At this point, we can expand the summation, distribute the three terms in\n",
      "front of the logarithms, and group like terms. We can also again remove terms\n",
      "that do not involve α. We are left with:\n",
      "\n",
      "p(α|β, K, x) ∝\n",
      "(\n",
      "\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4\n",
      "exp\n",
      "\n",
      "{\n",
      "α\n",
      "\n",
      "(\n",
      "4∑\n",
      "\n",
      "i=1\n",
      "\n",
      "lnKi\n",
      "\n",
      ")}\n",
      "p(α, β).\n",
      "\n",
      "A similar strategy reveals that the posterior density for β is:\n",
      "\n",
      "p(β|α, K, x) ∝\n",
      "(\n",
      "\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4\n",
      "exp\n",
      "\n",
      "{\n",
      "β\n",
      "\n",
      "(\n",
      "4∑\n",
      "\n",
      "i=1\n",
      "\n",
      "ln(1−Ki)\n",
      "\n",
      ")}\n",
      "p(α, β).\n",
      "\n",
      "What remains is the specification of the prior density p(α, β). Ideally, we\n",
      "may like a prior that is relatively noninformative. However, in this particular\n",
      "example, we must be careful, because these conditional posterior densities are\n",
      "not of known forms and, with too vague of a prior, will not be proper.\n",
      "\n",
      "Recall that the hyperparameters α and β of the beta distribution can be\n",
      "viewed as prior successes and failures, respectively, and are therefore con-\n",
      "strained to be nonnegative. In the example in Chapter 3, we fixed these pa-\n",
      "rameters at constants to represent the successes/failures from the first three\n",
      "surveys in Ohio. Now, in contrast, we want to specify distributions for them.\n",
      "An appropriate distribution that would constrain these parameters to be non-\n",
      "negative is the gamma distribution, which itself has two parameters, say C\n",
      "and D. If we assume that α and β have independent prior distributions, then\n",
      "p(α, β) = p(α)p(β), and we can assign each a gamma distribution prior:\n",
      "\n",
      "p(α) ∝ αCα−1 exp (−Dαα)\n",
      "p(β) ∝ βCβ−1 exp (−Dββ) .\n",
      "\n",
      "This hyperprior yields the following conditional posterior for α:\n",
      "\n",
      "p(α|β, K, x,Cα, Dα) ∝\n",
      "(\n",
      "\n",
      "Γ (α + β)\n",
      "Γ (α)Γ (β)\n",
      "\n",
      ")4\n",
      "αCα−1 exp\n",
      "\n",
      "{\n",
      "α\n",
      "\n",
      "(\n",
      "4∑\n",
      "\n",
      "i=1\n",
      "\n",
      "lnKi −Dα\n",
      "\n",
      ")}\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "9.1 Hierarchical models in general 237\n",
      "\n",
      "A comparable result can be obtained for β. All that remains is to specify\n",
      "values for C and D in each hyperprior.\n",
      "\n",
      "Given parameters C and D, the mean of a gamma distribution is equal\n",
      "to C/D, and the variance is equal to C/D2. We may choose to set these pa-\n",
      "rameters at values that reflect our prior knowledge. Numerous previous polls\n",
      "throughout the country had showed the race to be virtually a dead heat, and\n",
      "so, we may choose comparable values of C and D for both prior distributions.\n",
      "The typical poll conducted throughout the fall by different polling organiza-\n",
      "tions consisted of about 500 or so potential voters, roughly half of which were\n",
      "expected to vote for Kerry. So, we may choose values of C and D such that\n",
      "C/D = 250. We can capture prior uncertainty in this estimate by specify-\n",
      "ing the variance to be large. For example, if we choose a standard deviation\n",
      "to be equal to 100, then C/D2 = 10, 000, and so C = 6.25 and D = .025.\n",
      "To evaluate the influence of the hyperparameter specification, I varied these\n",
      "parameters and conducted several runs of the Gibbs sampler, as discussed\n",
      "below.\n",
      "\n",
      "Below is a hybrid Gibbs sampler/MH algorithm for simulating the param-\n",
      "eters of the model. Although the K parameters, conditional on the data and\n",
      "values for α and β, can be drawn directly from beta distributions, the α and β\n",
      "hyperparameters are not known forms and must therefore be simulated using\n",
      "MH steps:\n",
      "\n",
      "#MCMC algorithm for hierarchical beta-binomial model\n",
      "\n",
      "a=matrix(10,100000);b=matrix(10,100000); acca=0; accb=0\n",
      "\n",
      "y=matrix(c(556,346,312,284),4); n=matrix(c(1067,685,637,628),4)\n",
      "\n",
      "k=matrix((y)/n,m,4,byrow=T)\n",
      "\n",
      "apost<-function(f,g,k){\n",
      "\n",
      "post=4*(lgamma(f+g)-lgamma(f)-lgamma(g)) + f * sum(log(k))\n",
      "\n",
      "post=post+(6.25-1)*log(f)-(f*.025)\n",
      "\n",
      "return(post)\n",
      "\n",
      "}\n",
      "\n",
      "bpost<-function(f,g,k){\n",
      "\n",
      "post=4*(lgamma(f+g)-lgamma(f)-lgamma(g)) + g * sum(log(1-k))\n",
      "\n",
      "post=post+(6.25-1)*log(g)-(g*.025)\n",
      "\n",
      "return(post)\n",
      "\n",
      "}\n",
      "\n",
      "for(i in 2:100000){\n",
      "\n",
      "#draw a\n",
      "\n",
      "a[i]=a[i-1]+rnorm(1,0,20)\n",
      "\n",
      "if(a[i]>0){\n",
      "\n",
      "acca=acca+1\n",
      "\n",
      "newpost=apost(a[i],b[i-1],k[i-1,])\n",
      "\n",
      "oldpost=apost(a[i-1],b[i-1],k[i-1,])\n",
      "\n",
      "if(log(runif(1,min=0,max=1))>(newpost-oldpost))\n",
      "\n",
      "\n",
      "\n",
      "238 9 Introduction to Hierarchical Models\n",
      "\n",
      "{a[i]=a[i-1]; acca=acca-1}\n",
      "\n",
      "}\n",
      "\n",
      "if(a[i]<0){a[i]=a[i-1]}\n",
      "\n",
      "#draw b\n",
      "\n",
      "b[i]=b[i-1]+rnorm(1,0,20)\n",
      "\n",
      "if(b[i]>0){\n",
      "\n",
      "accb=accb+1\n",
      "\n",
      "newpost=bpost(a[i],b[i],k[i-1,])\n",
      "\n",
      "oldpost=bpost(a[i],b[i-1],k[i-1,])\n",
      "\n",
      "if(log(runif(1,min=0,max=1))>(newpost-oldpost))\n",
      "\n",
      "{b[i]=b[i-1]; accb=accb-1}\n",
      "\n",
      "}\n",
      "\n",
      "if(b[i]<0){b[i]=b[i-1]}\n",
      "\n",
      "#draw k from beta distributions\n",
      "\n",
      "k[i,]=rbeta(4,(y+a[i]),(n-y+b[i]))\n",
      "\n",
      "if(i%%10==0){print(c(i,a[i],b[i],acca/i,accb/i))}\n",
      "\n",
      "}\n",
      "\n",
      "This program is fairly straightforward. First, matrices are established for\n",
      "the α and β parameters, and acceptance rate variables are also constructed\n",
      "for monitoring the MH steps used to simulate them. Next, the data, including\n",
      "votes for Kerry (y), poll sizes (n), and proportions favoring Kerry (k), are\n",
      "established. The next two program blocks are functions that evaluate the\n",
      "conditional log-posterior densities for α and β, respectively, given values of\n",
      "these parameters, the previous value for the observed sample proportions, and\n",
      "a prior distribution (the second line of each function is the hyperprior).\n",
      "\n",
      "The program then proceeds to simulate 100,000 draws from the posterior\n",
      "for all the parameters. The α and β parameters are drawn using MH steps.\n",
      "Candidates are generated from normal proposals with a standard deviation\n",
      "set to produce an approximate acceptance rate of 50%. Once a candidate\n",
      "is generated, the log-posterior is evaluated at the candidate values for these\n",
      "parameters and the previous values. I have structured these blocks so that the\n",
      "candidate parameter is assumed to be accepted and is evaluated for rejection.\n",
      "If the candidate is less than 0, or the log of the uniform draw exceeds the ratio\n",
      "of the log-posterior at the current versus previous values, the current value\n",
      "of the parameter is reset to the previous value, and the acceptance tally is\n",
      "reduced by one. Once values of these parameters have been drawn, each Ki\n",
      "parameter is drawn from the appropriate beta distribution.\n",
      "\n",
      "The key parameters of interest in the model include the individual survey\n",
      "proportions (K1 . . .K4) and the population proportion implied by the α and β\n",
      "parameters, which is equal to α/(α+β). Table 9.1 shows posterior summaries\n",
      "of these parameters under a variety of specifications for C and D in the\n",
      "hyperpriors for α and β. The first four columns of the table show the gamma\n",
      "distribution hyperprior specifications for the α and β parameters of the prior\n",
      "\n",
      "\n",
      "\n",
      "9.1 Hierarchical models in general 239\n",
      "\n",
      "distribution. These values for the hyperpriors were chosen to examine how\n",
      "sensitive the posterior inferences are to prior specification.\n",
      "\n",
      "The first two columns show the mean and standard deviation of the gamma\n",
      "hyperprior distribution for α, respectively; the third and fourth columns show\n",
      "the mean and standard deviation of the hyperprior for β. Recall from above\n",
      "that the mean of the gamma distribution for α can be considered as previous\n",
      "votes for Kerry, and the variance/standard deviation of this distribution can\n",
      "be viewed as a measure of our uncertainty in this number of previous votes.\n",
      "Similarly, the mean of the gamma distribution for β can be considered as\n",
      "previous votes for Bush, and its standard deviation reflects our uncertainty\n",
      "in this number. Thus, the first specification implies that previous polls have\n",
      "shown an equal—and small—number of votes for both candidates, and the\n",
      "relatively large standard deviation of each (10) suggests that we are not very\n",
      "certain of these numbers.\n",
      "\n",
      "Thus, the first row shows the posterior inference when the prior informa-\n",
      "tion is fairly weak. That is, this hyperprior specification implies that we have\n",
      "prior information equivalent to 10 previous votes for Kerry and 10 for Bush,\n",
      "with a fairly large standard deviation reflecting considerable uncertainty about\n",
      "these numbers of votes. In contrast, the final hyperprior specification implies\n",
      "that we have prior information equivalent to 2,500 votes for Kerry and 500\n",
      "votes for Bush, and that our confidence in these numbers is relatively strong\n",
      "(standard deviation of only 50, compared with the number of prior votes).\n",
      "\n",
      "The bottom two rows of the table show the results under two alternative\n",
      "approaches to the hierarchical approach discussed here. The first row at the\n",
      "bottom shows the results obtained if the four polls are analyzed independently;\n",
      "the second shows the results obtained if the data from all polls are pooled and\n",
      "given a noninformative prior distribution—an equivalent approach to treating\n",
      "the most recent polling data as the current data and the earlier three polls as\n",
      "prior information (see Chapter 3).\n",
      "\n",
      "Overall, all the hyperprior specifications lead to similar posterior inference\n",
      "for the prior distribution mean α/(α + β) and for each of the polls, with the\n",
      "exception of the most informative specification which shows heavy favoritism\n",
      "for Kerry (2,500 prior votes versus 500). Under that specification, the posterior\n",
      "mean for the second level beta prior distribution is pulled strongly away from\n",
      "the mean implied by the polling data and toward the prior.\n",
      "\n",
      "A couple of comments are warranted regarding these results. First, notice\n",
      "that pooling the data led to a posterior mean of .497 for Kerry’s proportion\n",
      "of the vote, and that a similar proportion was obtained using α/(α+β) in the\n",
      "hierarchical model, except for the final one with the strongest and most un-\n",
      "balanced hyperprior. However, although the posterior means are comparable,\n",
      "the posterior standard deviation for this proportion tended to be much larger\n",
      "under the hierarchical approach. The reason for this result is that, under the\n",
      "hierarchical approach, the distribution for α/(α+β) captures the range of the\n",
      "survey specific Ki parameters, each of which contains its own variability. Un-\n",
      "der the pooled-data approach, on the other hand, three of the Ki are assumed\n",
      "\n",
      "\n",
      "\n",
      "240 9 Introduction to Hierarchical Models\n",
      "\n",
      "Table 9.1. Results of hierarchical model for voting example under different gamma\n",
      "hyperprior specifications.\n",
      "\n",
      "Gamma Priors Posterior Inferences\n",
      "α β\n",
      "\n",
      "C\n",
      "D\n",
      "\n",
      "q\n",
      "C\n",
      "\n",
      "D2\n",
      "C\n",
      "D\n",
      "\n",
      "q\n",
      "C\n",
      "\n",
      "D2\n",
      "α\n",
      "\n",
      "α+β\n",
      "K1 K2 K3 K4\n",
      "\n",
      "10 10 10 10 .493(.048) .520(.015) .505(.019) .490(.019) .454(.019)\n",
      "100 100 100 100 .493(.021) .516(.014) .502(.017) .491(.018) .463(.018)\n",
      "250 100 250 100 .494(.015) .513(.014) .501(.016) .491(.016) .470(.017)\n",
      "250 100 100 100 .494(.016) .514(.014) .501(.016) .491(.017) .469(.017)\n",
      "2500 50 500 50 .586(.008) .572(.010) .574(.010) .572(.010) .567(.010)\n",
      "\n",
      "Separate Models NA .521(.015) .505(.019) .490(.020) .452(.020)\n",
      "Pooled Data .497(.009) NA NA NA NA\n",
      "\n",
      "Note: The hyperpriors are gamma distributions for both α and β. The hyperpa-\n",
      "rameters C and D in each gamma distribution were set to produce the means and\n",
      "standard deviations shown (C/D and\n",
      "\n",
      "p\n",
      "C/D2, respectively). The posterior quan-\n",
      "\n",
      "tities are the posterior mean of the beta prior distribution, α/(α + β), and the\n",
      "posterior means for each of the sample proportions (posterior standard deviations\n",
      "are in parentheses).\n",
      "\n",
      "be known, fixed quantities, reducing variability in the overall mean. Second,\n",
      "notice that it is generally the case that the variability for each Ki parameter\n",
      "is smaller than that obtained under the separate-models approach. The rea-\n",
      "son for this result is that, by combining all samples into a single, hierarchical\n",
      "model, each Ki distribution “borrows strength” from the common linkage of\n",
      "all the polls provided by the hyperparameters α and β.\n",
      "\n",
      "9.2 Hierarchical linear regression models\n",
      "\n",
      "The example in the previous section shows a basic hierarchical model in which\n",
      "the model parameters, but not the data, were structured hierarchically—all\n",
      "of the data were measured at the same level (individual polls). It is common\n",
      "in social science research, however, to have hierarchical structure to the data,\n",
      "that is, to have variables collected at different levels. In these cases, social\n",
      "scientists often turn to hierarchical models to capture variation at different\n",
      "levels of analysis. Because these models involve variables measured at differ-\n",
      "ent levels, they are sometimes called “multilevel models.” Most commonly,\n",
      "individuals are nested within physical or geographic units, or time-specific\n",
      "measures are nested within individuals. As a few examples of the former type\n",
      "of nesting, consider students within classrooms or individuals within neigh-\n",
      "borhoods. As an example of the latter type of nesting, consider a panel study\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 241\n",
      "\n",
      "in which individuals are measured repeatedly across time. In such a case, the\n",
      "“group” is the individual, and the time-specific measures are nested within\n",
      "the individual. The examples here will follow this latter format—time-specific\n",
      "measures nested within individuals—although the underlying concepts of hi-\n",
      "erarchy are identical.\n",
      "\n",
      "I discuss several types of such hierarchical regression models, beginning\n",
      "with an example that evaluates the extent to which Internet usage influences\n",
      "income using a two-wave panel study.1 These data are from the 2000 and 2001\n",
      "Current Population Survey Computer Use and Internet Supplement. This sup-\n",
      "plement measured, among other variables, individual use of computers and the\n",
      "Internet in 2000 and again in 2001 and allows us to examine the relationship\n",
      "between Internet usage and wages across a brief, but important, period of time\n",
      "when availability of broadband Internet connectivity was exploding. Wages in\n",
      "these examples have been transformed to 1982 dollars and are recoded into\n",
      "log-dollars per hour for additional analyses not presented here.\n",
      "\n",
      "At the end of the chapter, I turn to an example that examines factors that\n",
      "influence health trajectories for individuals across age using a four-wave study\n",
      "(the National Health Epidemiologic Follow-up Surveys) discussed in previous\n",
      "chapters.\n",
      "\n",
      "9.2.1 Random effects: The random intercept model\n",
      "\n",
      "Generally, the goal of hierarchical modeling is to determine the extent to\n",
      "which factors measured at different levels influence an outcome using a typical\n",
      "regression modeling framework. OLS regression, however, is inappropriate,\n",
      "because of the lack of independence of errors for observations within groups.\n",
      "Thus, an alternative model must be developed to compensate for this lack of\n",
      "independence.\n",
      "\n",
      "The foundation for the hierarchical regression model is the simple random\n",
      "effects model. Assume, as an example, that we observe a collection of indi-\n",
      "viduals twice over a two-year period and ask their income at each point in\n",
      "time. It is most likely the case that each individual’s income changes only\n",
      "slightly over the time period, and so, we could model the data such that each\n",
      "individual receives his/her own intercept (or mean). In equation form:\n",
      "\n",
      "yit = αi + eit,\n",
      "\n",
      "with αi ∼ N(α, τ2) and eit ∼ N(0, σ2). This specification shows that the out-\n",
      "come of interest (income; y) is considered a function of “variables” measured\n",
      "at two different levels: αi is an individual (group) level variable, and eit is a\n",
      "time-specific (individual) random error term.\n",
      "\n",
      "An alternative, but equivalent, way to specify this model is to use proba-\n",
      "bility notation. This approach clarifies the hierarchical nature of the model:\n",
      "1 I thank Bart Bonikowski and Paul DiMaggio for allowing me to use their Inter-\n",
      "\n",
      "net/income data in the examples.\n",
      "\n",
      "\n",
      "\n",
      "242 9 Introduction to Hierarchical Models\n",
      "\n",
      "yit ∼ N(αi, σ2)\n",
      "αi ∼ N(α, τ2)\n",
      "α ∼ N(m, s2)\n",
      "\n",
      "τ2 ∼ IG(a, b)\n",
      "σ2 ∼ IG(c, d).\n",
      "\n",
      "This specification says that an individual’s time-specific income is a ran-\n",
      "dom normal variable with a mean equal to an individual-specific mean and\n",
      "some variance. The second equation shows that the individual-specific means\n",
      "themselves come from a (normal) distribution with a mean equal to some\n",
      "population mean and some variance. Finally, the last three equations spec-\n",
      "ify hyperprior distributions for the population grand mean α, the population\n",
      "variance (around the mean) τ2, and the error variance σ2. The hyperprior\n",
      "distribution for the population mean is specified here to be normal, with pa-\n",
      "rameters m and s2; Without prior knowledge, these parameters should be\n",
      "specified to make the hyperprior vague (e.g., say m = 0 and s2 = 10, 000).\n",
      "The hyperprior distributions for the population variance and the error vari-\n",
      "ance are inverse gamma distributions, with parameters a and b and c and d,\n",
      "respectively. Once again, without prior information, these parameters should\n",
      "be fixed to make the hyperprior vague.\n",
      "\n",
      "In addition to being a simple random effects model, this model is some-\n",
      "times called a “random intercept model,” because the model can be viewed\n",
      "as a regression model with each αi considered a group-specific intercept term\n",
      "arising from a (normal) probability distribution (at this point, with no covari-\n",
      "ates included).\n",
      "\n",
      "To implement a Gibbs sampler for this model, we first need to construct the\n",
      "posterior distribution. The posterior distribution for this model is straightfor-\n",
      "ward to derive following the hierarchical modeling structure using conditional\n",
      "distributions presented at the beginning of the chapter. The parameters of in-\n",
      "terest in the posterior distribution are the individual αi, the population mean\n",
      "α, its variance τ2, and the residual variance σ2, and so our posterior density\n",
      "is:\n",
      "\n",
      "p(α, τ2, αi, σ\n",
      "2|Y ) ∝ p(Y |αi, σ2)p(αi|α, τ2)p(α|m, s2)p(τ2|c, d)p(σ2|a, b).\n",
      "\n",
      "To complete the specification of the posterior distribution, we simply need to\n",
      "replace each term with its actual distribution. As discussed above, the data\n",
      "are assumed to be normally distributed, and so the likelihood term is:\n",
      "\n",
      "p(Y |αi, σ2) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "2∏\n",
      "t=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "σ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(yit − αi)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 243\n",
      "\n",
      "The distribution for each αi is also normal and is:\n",
      "\n",
      "p(αi|α, τ2) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "τ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(αi − α)2\n",
      "\n",
      "2τ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "The remaining terms are hyperprior distributions for the population mean\n",
      "(α), population random effects variance (τ2), and residual variance (σ2). As\n",
      "mentioned above, α is assumed to come from a normal distribution with pa-\n",
      "rameters m and s2, and the two variance parameters are assumed to come\n",
      "from inverse gamma distributions with parameters a and b and c and d, re-\n",
      "spectively. This implies the following joint hyperprior distribution:\n",
      "\n",
      "p(α|m, s2)p(τ2|a, b)p(σ2|c, d) ∝\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "s2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(α−m)2\n",
      "\n",
      "2s2\n",
      "\n",
      "}\n",
      "×\n",
      "\n",
      "1\n",
      "(τ2)a+1\n",
      "\n",
      "exp\n",
      "{\n",
      "−b/(τ2)\n",
      "\n",
      "}\n",
      "×\n",
      "\n",
      "1\n",
      "(σ2)c+1\n",
      "\n",
      "exp\n",
      "{\n",
      "−d/(σ2)\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "The full posterior, then, is simply the product of these three terms—the likeli-\n",
      "hood, prior, and hyperprior distributions. Although the posterior distribution\n",
      "can be simplified considerably by carrying out the multiplication of exponen-\n",
      "tials and combining like terms, it is simpler to derive the conditionals for the\n",
      "Gibbs sampler by leaving the posterior written as is. For the Gibbs sampler,\n",
      "we need the conditional distributions for each of the parameters; deriving\n",
      "them from the posterior is a simple but tedious matter of selecting only the\n",
      "terms that contain the parameter of interest, discarding all other multiplica-\n",
      "tive terms as proportionality constants, and simplifying/rearranging what’s\n",
      "left to determine the resulting distribution. If we begin with the parameter α,\n",
      "the relevant terms in the posterior are:\n",
      "\n",
      "p(α|.) ∝ p(αi|α, τ2)p(α)\n",
      "\n",
      "∝\n",
      "\n",
      "(\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "τ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(αi − α)2\n",
      "\n",
      "2τ2\n",
      "\n",
      "})\n",
      "1\n",
      "√\n",
      "\n",
      "s2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(α−m)2\n",
      "\n",
      "2s2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "From this expression, the leading fractions involving the variances can be\n",
      "removed as normalizing constants (they do not depend on α), and the expo-\n",
      "nential expressions can be combined to obtain:\n",
      "\n",
      "p(α|.) ∝ exp\n",
      "{(\n",
      "\n",
      "−\n",
      "1\n",
      "2\n",
      "\n",
      ")(\n",
      "τ2(α−m)2 + s2\n",
      "\n",
      "∑n\n",
      "i=1(αi − α)\n",
      "\n",
      "2\n",
      "\n",
      "τ2s2\n",
      "\n",
      ")}\n",
      ".\n",
      "\n",
      "Next, we can expand the numerator of the exponential, extract terms not\n",
      "involving α as constants, and we have:\n",
      "\n",
      "p(α|.) ∝ exp\n",
      "{(\n",
      "\n",
      "−\n",
      "1\n",
      "2\n",
      "\n",
      ")(\n",
      "τ2α2 − 2τ2αm− 2s2α\n",
      "\n",
      "∑\n",
      "αi + ns2α2\n",
      "\n",
      "τ2s2\n",
      "\n",
      ")}\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "244 9 Introduction to Hierarchical Models\n",
      "\n",
      "Rearranging terms, we obtain:\n",
      "\n",
      "p(α|.) ∝ exp\n",
      "{(\n",
      "\n",
      "−\n",
      "1\n",
      "2\n",
      "\n",
      ")(\n",
      "(τ2 + ns2)α2 − 2α(τ2m + s2\n",
      "\n",
      "∑\n",
      "αi)\n",
      "\n",
      "τ2s2\n",
      "\n",
      ")}\n",
      ".\n",
      "\n",
      "As we did in Chapter 3, we can complete the square in α, and we find that\n",
      "the conditional posterior for α is:\n",
      "\n",
      "p(α|.) ∝ N\n",
      "(\n",
      "\n",
      "τ2m + s2\n",
      "∑\n",
      "\n",
      "αi\n",
      "τ2 + ns2\n",
      "\n",
      ",\n",
      "τ2s2\n",
      "\n",
      "τ2 + ns2\n",
      "\n",
      ")\n",
      "(9.2)\n",
      "\n",
      "The conditional posterior distribution for each αi is even easier to obtain.\n",
      "Once again, we begin with terms involving only αi. We should realize, however,\n",
      "that, for each individual i, the only relevant terms in the product are those\n",
      "involving that particular individual. Thus, the conditional posterior for person\n",
      "i (∀i) is:\n",
      "\n",
      "p(αi|.) ∝ p(Y |αi, σ2)p(αi|α, τ2)\n",
      "\n",
      "∝\n",
      "\n",
      "(\n",
      "2∏\n",
      "\n",
      "t=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "σ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(yit − αi)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "})(\n",
      "1\n",
      "√\n",
      "\n",
      "τ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(αi − α)2\n",
      "\n",
      "2τ2\n",
      "\n",
      "})\n",
      ".\n",
      "\n",
      "We can follow the same steps as for α, and we obtain:\n",
      "\n",
      "p(αi|.) ∝ exp\n",
      "{(\n",
      "\n",
      "−\n",
      "1\n",
      "2\n",
      "\n",
      ")(\n",
      "(2τ2 + σ2)α2i − 2αi(τ\n",
      "\n",
      "2\n",
      "∑\n",
      "\n",
      "yit + σ2α)\n",
      "τ2σ2\n",
      "\n",
      ")}\n",
      ".\n",
      "\n",
      "If we complete the square in αi, we find that:\n",
      "\n",
      "p(αi|.) ∝ N\n",
      "(\n",
      "\n",
      "τ2\n",
      "∑\n",
      "\n",
      "yit + σ2α\n",
      "2τ2 + σ2\n",
      "\n",
      ",\n",
      "τ2σ2\n",
      "\n",
      "2τ2 + σ2\n",
      "\n",
      ")\n",
      ". (9.3)\n",
      "\n",
      "The variance parameters σ2 and τ2 can be derived following the same strategy.\n",
      "The conditional posterior for σ2 is:\n",
      "\n",
      "p(σ2|.) ∝ p(Y |αi, σ2)p(σ2|a, b).\n",
      "\n",
      "After substitution we obtain:\n",
      "\n",
      "p(σ2|.) ∝\n",
      "\n",
      "(\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "2∏\n",
      "t=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "σ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(yit − αi)2)\n",
      "2σ2\n",
      "\n",
      "})\n",
      "1\n",
      "\n",
      "(σ2)c+1\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "d\n",
      "\n",
      "σ2\n",
      "\n",
      "}\n",
      ",\n",
      "\n",
      "and after some simplification, we get:\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 245\n",
      "\n",
      "p(σ2|.) ∝\n",
      "(\n",
      "σ2\n",
      ")−(n+c+1)\n",
      "\n",
      "exp\n",
      "\n",
      "\n",
      "\n",
      "−\n",
      "(∑n\n",
      "\n",
      "i=1\n",
      "\n",
      "∑2\n",
      "t=1(yit − αi)\n",
      "\n",
      "2 + 2d\n",
      ")\n",
      "\n",
      "2σ2\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "This result shows that the conditional posterior for σ2 is an inverse gamma\n",
      "distribution:\n",
      "\n",
      "p(σ2|.) ∝ IG\n",
      "\n",
      "(\n",
      "n + c ,\n",
      "\n",
      "∑n\n",
      "i=1\n",
      "\n",
      "∑2\n",
      "t=1(yit − αi)\n",
      "\n",
      "2 + 2d\n",
      "2\n",
      "\n",
      ")\n",
      ". (9.4)\n",
      "\n",
      "The conditional posterior for τ can be derived similarly. The posterior is:\n",
      "\n",
      "p(τ2|.) ∝ p(αi|α, τ2)p(τ2)\n",
      "\n",
      "∝\n",
      "\n",
      "(\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "1\n",
      "√\n",
      "\n",
      "τ2\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "(αi − α)2\n",
      "\n",
      "2τ2\n",
      "\n",
      "})\n",
      "1\n",
      "\n",
      "(τ2)a+1\n",
      "exp\n",
      "\n",
      "{\n",
      "−\n",
      "\n",
      "b\n",
      "\n",
      "τ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "After simplification, we obtain:\n",
      "\n",
      "p(τ2|.) ∝ IG\n",
      "(\n",
      "\n",
      "n/2 + a + 1 ,\n",
      "∑n\n",
      "\n",
      "i=1(αi − α)\n",
      "2 + 2b\n",
      "\n",
      "2\n",
      "\n",
      ")\n",
      "(9.5)\n",
      "\n",
      "(see Exercises).\n",
      "Given a complete set of conditional posterior distributions, we can im-\n",
      "\n",
      "plement a Gibbs sampler for the model by sequentially drawing from these\n",
      "conditionals. Below is an R program that conducts the Gibbs sampling:\n",
      "\n",
      "#R program for simple random effects model\n",
      "\n",
      "#read data\n",
      "\n",
      "y=as.matrix(read.table(\"c:\\\\internet_examp.dat\")[,3:4])\n",
      "\n",
      "m=0; s2=10000; a=c=.001; b=d=.001; tau2=1; sigma2=1; malpha=0\n",
      "\n",
      "n=nrow(y)\n",
      "\n",
      "for(i in 1:20000){\n",
      "\n",
      "#draw alpha_i\n",
      "\n",
      "alpha= rnorm(n,\n",
      "\n",
      "mean=(((tau2*(y[,1]+y[,2]))+sigma2*malpha)/(2*tau2+sigma2)),\n",
      "\n",
      "sd=sqrt((tau2*sigma2)/(2*tau2+sigma2)))\n",
      "\n",
      "#draw malpha\n",
      "\n",
      "malpha=rnorm(1,\n",
      "\n",
      "mean=(tau2*m+s2*sum(alpha))/((tau2+n*s2)),\n",
      "\n",
      "sd=sqrt((tau2*s2)/((tau2+n*s2))))\n",
      "\n",
      "#draw tau2\n",
      "\n",
      "tau2=rgamma(1, shape=(n/2+a), rate=(sum((alpha-malpha)^2)+2*b)/2)\n",
      "\n",
      "\n",
      "\n",
      "246 9 Introduction to Hierarchical Models\n",
      "\n",
      "tau2=1/tau2\n",
      "\n",
      "#draw sigma2\n",
      "\n",
      "sigma2=rgamma(1, shape=n+c, rate=(sum((y-alpha)^2) +2*d)/2)\n",
      "\n",
      "sigma2=1/sigma2\n",
      "\n",
      "#write results to file\n",
      "\n",
      "if(i%%10==0 | i==1)\n",
      "\n",
      "{print(c(i,alpha[1],malpha,tau2,sigma2))\n",
      "\n",
      "write(c(i,alpha[1],malpha,tau2,sigma2),\n",
      "\n",
      "file=\"c:\\\\bart2.out\",append=T,ncol=5)}\n",
      "\n",
      "}\n",
      "\n",
      "As with previous programs, the first block reads in the data and estab-\n",
      "lishes starting (and fixed) values for the parameters. The hyperparameters\n",
      "associated with the hyperpriors for α, τ2, and σ2 are fixed to 0, 10,000, .001,\n",
      ".001, .001, and .001, respectively, in order to ensure that the hyperparameters\n",
      "have little influence on the results (see Exercises). The starting values for the\n",
      "population/grand mean (α) as well as for τ2 and σ2 are arbitrarily set to\n",
      "benign values.\n",
      "\n",
      "Subsequent sections of the program constitute nothing more than itera-\n",
      "tively sampling from the conditional posterior distributions derived above.\n",
      "\n",
      "Although this R program is relatively short, the derivation of the condi-\n",
      "tional distributions was a tedious process. Fortunately, however, a software\n",
      "package exists that allows us to simulate values from the posterior distribu-\n",
      "tions for the parameters of this model more directly: WinBugs. WinBugs is\n",
      "a freely available software package that simplifies Gibbs sampling for a va-\n",
      "riety of models. The syntax for WinBugs is substantially similar to that of\n",
      "R, but many of the conditional posterior distribution derivations are done for\n",
      "us by WinBugs, reducing the need to derive the conditional posterior distri-\n",
      "butions manually. For example, a WinBugs program for the same example\n",
      "involves nothing more than specifying the likelihood, prior, and hyperprior\n",
      "distributions and parameter as follows:\n",
      "\n",
      "#Winbugs program for simple random effects model\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:9249)\n",
      "\n",
      "{\n",
      "\n",
      "for(j in 1:2)\n",
      "\n",
      "{\n",
      "\n",
      "y[i,j]~dnorm(alpha[i],sigma2inv)\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(malpha,tau2inv)\n",
      "\n",
      "}\n",
      "\n",
      "malpha~dnorm(0,1.0E-4)\n",
      "\n",
      "tau2inv~dgamma(.01,.01)\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 247\n",
      "\n",
      "tau2<-1/sqrt(tau2inv)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "}\n",
      "\n",
      "The syntax in this program is similar to that of R with a few exceptions.\n",
      "First, the tilde is used to simulate from distributions. Second, “< −” is used\n",
      "to assign values to variables.2 Third, the parameterization of the normal dis-\n",
      "tribution in WinBugs involves a precision parameter rather than a variance\n",
      "parameter. The precision is simply the inverse of the variance, and so, we can\n",
      "recover the variance parameter simply by inverting the draw from the gamma\n",
      "distribution for the precision parameters.\n",
      "\n",
      "The key results from the R program, the equivalent WinBugs program,\n",
      "and the equivalent maximum likelihood results obtained from STATA (ver-\n",
      "sions 8 and 9 were used throughout) using the xtreg procedure are presented\n",
      "in Table 9.2. As the results show, all three approaches yielded virtually the\n",
      "same results and therefore lead to the same conclusions. The Bayesian results,\n",
      "however, whether from R or WinBugs, yield more information by default than\n",
      "the STATA results, because the Bayesian approach yields distributions for all\n",
      "parameters/quantities of interest, including the variance parameters.\n",
      "\n",
      "Table 9.2. Results of hierarchical model for two-wave panel of income and Internet\n",
      "use data.\n",
      "\n",
      "Variable R WinBugs STATA xtreg\n",
      "\n",
      "Population Mean (α) 2.103(.005) 2.103(.005) NA\n",
      "Intercept NA NA 2.103(.005)√\n",
      "\n",
      "τ2 0.434(.004) 0.434(.004) 0.434√\n",
      "σ2 0.311(.002) 0.311(.002) 0.311\n",
      "\n",
      "τ2/(τ2 + σ2) 0.661(.006) 0.660(.006) 0.660\n",
      "\n",
      "Note: Posterior means (and posterior standard deviations) are reported for R and\n",
      "WinBugs algorithms. Generalized least squares estimates (and standard errors) are\n",
      "reported for STATA.\n",
      "\n",
      "Overall, these results indicate that mean log wages are 2.103 log-dollars\n",
      "per hour with a standard deviation of .434 log-dollars. Within individuals,\n",
      "the standard deviation of wages was .311 log-dollars, and the ratio of the\n",
      "between-individual to total variance is about 66%. This result suggests that\n",
      "much of the variation we observe in log-wages—as we might expect—is due\n",
      "\n",
      "2 This syntax can also be used in R, but I have generally not done so throughout\n",
      "the text.\n",
      "\n",
      "\n",
      "\n",
      "248 9 Introduction to Hierarchical Models\n",
      "\n",
      "to differences between individuals and not within individuals across the two-\n",
      "year period. As a side note, the total variance in hourly wages is equal to τ2 +\n",
      "σ2. Because we obtain estimates for both of these variances—the “between-\n",
      "individual” and “within-individual” variances—hieararchical models like this\n",
      "one are sometimes called “variance components” models.\n",
      "\n",
      "The next step in our hierarchical modeling approach is to allow variation\n",
      "in the group level parameters to be functions of group level variables and\n",
      "to let the individual level (here, time-specific level) random error term to be\n",
      "a function of individual level variables. First, for example, we could include\n",
      "group level characteristics in our model by decomposing the random intercept\n",
      "into a regression on group level variables. For example, suppose we now wish\n",
      "to determine whether sex influences respondents’ wages. In that case, we can\n",
      "specify the model as:\n",
      "\n",
      "yit ∼ N(αi + α(1)sexi, σ2)\n",
      "αi ∼ N(α(0), τ2)\n",
      "\n",
      "α(0) ∼ N(m0, s0)\n",
      "σ2 ∼ IG(a, b)\n",
      "\n",
      "α(1) ∼ N(m1, s1)\n",
      "τ2 ∼ IG(c, d).\n",
      "\n",
      "Essentially the only substantial difference between this and the previous model\n",
      "is that the individual-specific intercept has now been decomposed into a pop-\n",
      "ulation intercept and an effect of sex. A WinBugs program for this model is\n",
      "simple to specify from these distributions:\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:9249)\n",
      "\n",
      "{\n",
      "\n",
      "for(t in 1:2)\n",
      "\n",
      "{\n",
      "\n",
      "y[i,t]~dnorm(alpha[i],sigma2inv)\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(mu[i],tau2inv)\n",
      "\n",
      "mu[i]<-alpha0+alpha1*sex[i]\n",
      "\n",
      "}\n",
      "\n",
      "alpha0~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha1~dnorm(0,1.0E-4)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "tau2inv~dgamma(.01,.01)\n",
      "\n",
      "tau2<-1/sqrt(tau2inv)\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 249\n",
      "\n",
      "}\n",
      "\n",
      "In this program, I have specified independent (univariate) normal distribution\n",
      "priors for the population mean and the parameter representing the influence\n",
      "of sex. The fact that I have specified independent priors, however, does not\n",
      "imply that the two parameters are necessarily uncorrelated in the posterior.\n",
      "In fact, the two parameters are highly negatively correlated, as Figure 9.1\n",
      "shows.\n",
      "\n",
      "1.97 1.98 1.99 2.00 2.01\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "6\n",
      "0\n",
      "\n",
      ".1\n",
      "8\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".2\n",
      "2\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "4\n",
      "\n",
      "α(0)\n",
      "\n",
      "α\n",
      "(1\n",
      "\n",
      ")\n",
      "\n",
      "corr(α(0),α(1))=−.71\n",
      "\n",
      "Fig. 9.1. Two-dimensional trace plot of α(0) and α(1) parameters (dashed lines at\n",
      "posterior means for each parameter).\n",
      "\n",
      "The posterior mean for the adjusted population mean (α(0)) was 1.99 (s.d.\n",
      "= .007), and the mean for the influence of sex (α(1)) was .225 (s.d. = .0098),\n",
      "indicating that males have higher log wages. The only additional change be-\n",
      "tween this and the previous model is the magnitude of τ2. Recall that τ2\n",
      "\n",
      "reflects unexplained between-individual variation in the random intercept for\n",
      "log-wages. With the inclusion of sex as an explantory variable differentiating\n",
      "individuals’ wages, τ2 has been reduced. Its posterior mean is now .419 (s.d.\n",
      "of .004), which is a reduction of 3.5% over the mean value obtained under the\n",
      "\n",
      "\n",
      "\n",
      "250 9 Introduction to Hierarchical Models\n",
      "\n",
      "previous model. This reduction can be viewed as an R2 term; put another\n",
      "way, sex differences account for 3.5% of the between-individual variance in\n",
      "wages.\n",
      "\n",
      "Additional time-invariant variables can be easily included to further ac-\n",
      "count for between-individual variation in wages. But what if we would like\n",
      "to consider the influence of time-varying covariates? For example, suppose we\n",
      "are interested in examining the extent to which Internet usage at a given point\n",
      "in time influences wages at the same point in time. Our data include time-\n",
      "specific measures of Internet usage, measured at the same points in time that\n",
      "wages are measured. There are two ways we can accomplish this goal. First,\n",
      "we can allow such covariates to influence the time-specific outcomes directly:\n",
      "\n",
      "yit ∼ N(αi + α(1)sexi + α(2)Internetit, σ2)\n",
      "αi ∼ N(α(0), τ2)\n",
      "\n",
      "α(0) ∼ N(m0, s0)\n",
      "α(1) ∼ N(m1, s1)\n",
      "α(2) ∼ N(m2, s2)\n",
      "σ2 ∼ IG(a, b)\n",
      "τ2 ∼ IG(c, d)\n",
      "\n",
      "In this model, time-specific wages are considered a function of individual ran-\n",
      "dom intercepts and time-specific Internet usage indicators, and the random\n",
      "intercepts are considered a function of a grand mean and an indicator for sex.3\n",
      "\n",
      "A WinBugs program to implement this model is as follows:\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:9249)\n",
      "\n",
      "{\n",
      "\n",
      "for(t in 1:2)\n",
      "\n",
      "{\n",
      "\n",
      "y[i,t]~dnorm(mu[i,t],sigma2inv)\n",
      "\n",
      "mu[i,t]<-(alpha[i]+alpha1*sex[i])+alpha2*internet[i,t]\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(alpha0,tau2inv)\n",
      "\n",
      "}\n",
      "\n",
      "alpha0~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha1~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha2~dnorm(0,1.0E-4)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "3 An equivalent way of specifying this model is: yit ∼ N(αi + α(2)Internetit, σ2),\n",
      "with αi ∼ N(α0 + α(1)sexi, τ2).\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 251\n",
      "\n",
      "tau2inv~dgamma(.01,.01)\n",
      "\n",
      "tau2<-1/sqrt(tau2inv)\n",
      "\n",
      "}\n",
      "\n",
      "This program is only slightly more complicated than the previous pro-\n",
      "grams. The only substantial differences are that (1) we have included the new\n",
      "parameter (α(2)) within the double loop (i, t), and (2) we have incorporated a\n",
      "prior distribution for it. The results of this model suggest that Internet usage\n",
      "does, in fact, influence income. The posterior mean for the influence of Inter-\n",
      "net usage is .18 (s.d. = .0075), and the intercept (α(0)) falls to 1.86 (s.d. =\n",
      ".009).\n",
      "\n",
      "9.2.2 Random effects: The random coefficient model\n",
      "\n",
      "As written, the last model in the previous section forces the effect of Internet\n",
      "usage to be constant across time: There was only a single parameter repre-\n",
      "senting the effect of Internet usage on wages. This constraint may introduce\n",
      "error into the model if, in fact, the influence of Internet usage on wages varies\n",
      "across time. Thus, a second way we can include this time-varying variable is\n",
      "to allow the influence of Internet usage to vary across time. This model is:\n",
      "\n",
      "yit ∼ N(αi + α(1)sexi + α(2t)Internetit, σ2)\n",
      "αi ∼ N(α(0), τ2)\n",
      "\n",
      "α(0) ∼ N(m0, s0)\n",
      "α(1) ∼ N(m1, s1)\n",
      "\n",
      "α(21) ∼ N(m2, s2)\n",
      "α(22) ∼ N(m3, s3)\n",
      "\n",
      "σ2 ∼ IG(a, b)\n",
      "τ2 ∼ IG(c, d)\n",
      "\n",
      "The alterations of the WinBugs program to accommodate this new param-\n",
      "eter are very slight: The alpha2 parameter must be subscripted appropriately\n",
      "(i.e., alpha2[t]), and an additional hyperprior distribution must be incorpo-\n",
      "rated. By some terminologies, we can now call the model a random coefficient\n",
      "model, because a slope—and not simply an intercept—is now considered a\n",
      "function of other variables.4\n",
      "\n",
      "4 It may be easier to recognize that allowing alpha2 to vary across time implies\n",
      "that alpha2 is now a slope, and not simply an intercept, if we consider that our\n",
      "current representation is equivalent to specifying α2 to be a function of a dummy\n",
      "variable reflecting time of measurement: α2 = β0 + β1I(t = 2), where β1 is a\n",
      "regression slope.\n",
      "\n",
      "\n",
      "\n",
      "252 9 Introduction to Hierarchical Models\n",
      "\n",
      "The results of this model do not vary substantially from those obtained\n",
      "when the effect of Internet usage was treated as constant. However, the in-\n",
      "fluence of Internet usage at time 1 was found to be .167 (s.d. = .009), while\n",
      "the effect of Internet usage at time 2 was .188 (s.d. = .008). A distribution\n",
      "for a new variable representing the difference between these parameters was\n",
      "constructed in order to determine whether this difference is greater than 0;\n",
      "99.9% of the mass of the resulting distribution was above 0 (posterior mean of\n",
      ".02; s.d. = .006), which indicates that Internet usage indeed influenced wages\n",
      "to a greater extent at the second wave of the study than at the first wave.\n",
      "\n",
      "From a substantive perspective, this result seems to be more consistent\n",
      "with the view that Internet usage influences income than the view that wages\n",
      "influence Internet usage. That is, Internet availability has become less depen-\n",
      "dent on income over time as the hardware for accessing the Internet (i.e.,\n",
      "computers and modems), as well as Internet service, has become cheaper. If\n",
      "wages influenced Internet usage, on the other hand, we might expect the influ-\n",
      "ence of wages on Internet use to decrease rather than increase over the period\n",
      "of observation. Thus, the result we obtained may be explained such that Inter-\n",
      "net usage builds social capital, allowing individuals to find or acquire better,\n",
      "higher paying jobs.\n",
      "\n",
      "One could still argue, however, that higher paying jobs have become in-\n",
      "creasingly dependent on Internet usage/access, and that a polarization of the\n",
      "labor market is occurring. Thus, higher paid workers have increasingly come\n",
      "to use the Internet, while lower paid jobs continue to not require Internet\n",
      "access/use.\n",
      "\n",
      "The relationship between Internet usage and income may not just vary\n",
      "across time; it may vary across individuals. For example, individuals in low-\n",
      "income, low-skill occupations may get less of a return to their income from\n",
      "using the Internet. In contrast, individuals in high-skilled occupations may get\n",
      "a large return to their income from using the Internet. In order to examine\n",
      "this possibility, we can alter the model so that the α(2) parameter varies by\n",
      "individual (i) rather than by time (t). Thus, the model becomes:\n",
      "\n",
      "yit ∼ N(αi + α(1)sexi + α(2i)Internetit, σ2)\n",
      "αi ∼ N(α(0), τ2)\n",
      "\n",
      "α(2i) ∼ N(α(20), τ22 )\n",
      "α(0) ∼ N(m0, s0)\n",
      "α(1) ∼ N(m1, s1)\n",
      "\n",
      "α(20) ∼ N(m2, s2)\n",
      "σ2 ∼ IG(a, b)\n",
      "τ2 ∼ IG(c, d)\n",
      "τ22 ∼ IG(e, f)\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 253\n",
      "\n",
      "This model is easily implemented in WinBugs with only minor changes to our\n",
      "previous programs:\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:9249)\n",
      "\n",
      "{\n",
      "\n",
      "for(t in 1:2)\n",
      "\n",
      "{\n",
      "\n",
      "y[i,t]~dnorm(mu[i,t],sigma2inv)\n",
      "\n",
      "mu[i,t]<-alpha[i]+alpha1*sex[i]+alpha2[i]*internet[i,t]\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(alpha0,tau2inv)\n",
      "\n",
      "alpha2[i]~dnorm(alpha20,tau20inv)\n",
      "\n",
      "}\n",
      "\n",
      "alpha0~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha1~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha20~dnorm(0,1.0E-4)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "tau2inv~dgamma(.01,.01)\n",
      "\n",
      "tau2<-1/sqrt(tau2inv)\n",
      "\n",
      "tau20inv~dgamma(.01,.01)\n",
      "\n",
      "tau20<-1/sqrt(tau20inv)\n",
      "\n",
      "}\n",
      "\n",
      "The results of this model suggest that there is considerable variation in\n",
      "the relationship between Internet usage and income across individuals. The\n",
      "estimated mean effect of Internet usage (α(2i)) was .205, and the estimated\n",
      "standard deviation for this effect (τ2) was .224. This result yields (under the\n",
      "assumption that the random effect α(2) is normally distributed) a 95% proba-\n",
      "bility interval for the influence of Internet usage of [-.234, .644], which indicates\n",
      "that Internet usage may be, in some cases, harmful to wages (playing games\n",
      "at the office, lowering productivity?!).\n",
      "\n",
      "What factors determine the influence of Internet usage on wages? In other\n",
      "words, why do some people appear to benefit from using the Internet, whereas\n",
      "others do not? We have previously decomposed the individual-specific random\n",
      "intercepts into an adjusted intercept and an effect of respondent’s sex. When\n",
      "we begin to allow regression parameters (like the the one capturing the in-\n",
      "fluence of Internet usage) to vary across individuals, we can also decompose\n",
      "it into a regression on higher level factors. For example, suppose we assumed\n",
      "that sex not only influenced the random intercept for wages, but also that it\n",
      "influences the extent to which Internet usage affects income. We can easily\n",
      "incorporate this idea into our model as follows. I switch notation slightly to\n",
      "avoid confusion:\n",
      "\n",
      "\n",
      "\n",
      "254 9 Introduction to Hierarchical Models\n",
      "\n",
      "yit ∼ N(αi + βiInternetit, σ2)\n",
      "αi ∼ N(α(0) + α(1)sexi, τ2α)\n",
      "βi ∼ N(β(0) + β(1)sexi, τ2β)\n",
      "\n",
      "α(0) ∼ N(m1, s1)\n",
      "α(1) ∼ N(m2, s2)\n",
      "β(0) ∼ N(m3, s3)\n",
      "β(1) ∼ N(m4, s4)\n",
      "τ2α ∼ IG(a, b)\n",
      "τ2β ∼ IG(c, d)\n",
      "σ2 ∼ IG(e, f)\n",
      "\n",
      "This model clarifies the hierarchical structuring of the data and parame-\n",
      "ters. Each individual’s income is a function of his/her own intercept and slope,\n",
      "and these individual-level intercepts and slopes are determined, in part, by\n",
      "sex—a characteristic that differentiates individuals. The model consists of\n",
      "seven vague hyperprior distributions, one for each of the parameters that are\n",
      "not themselves endogenous within the model.\n",
      "\n",
      "This model is sometimes called a multilevel or hierarchical model with\n",
      "cross-level interactions. The cross-level interactions, although not immedi-\n",
      "ately apparent in the above specification, can be observed if we revert to\n",
      "the equation-based, more classical representation of the model. Under that\n",
      "approach:\n",
      "\n",
      "yit = αi + βiInternetit + eit\n",
      "αi = α(0) + α(1)sexi + ui\n",
      "βi = β(0) + β(1)sexi + vi,\n",
      "\n",
      "with appropriate specifications for the variances of the errors at each level. If\n",
      "we then substitute the expressions for αi and βi into the first equation, we\n",
      "obtain:\n",
      "\n",
      "yit =\n",
      "\n",
      "α(0) + α(1)sexi + ui + β(0)Internetit + β(1)sexi× Internetit + viInternetit + eit.\n",
      "\n",
      "In this representation, we have a grand mean (α(0)) and an individual ad-\n",
      "justment to it (ui), a main effect of sex (α(1)), a time-constant main effect\n",
      "of Internet usage (β0) and an individual adjustment to it (vi), an interaction\n",
      "effect between sex and Internet usage (β(1)), and an error term (eit). The in-\n",
      "teraction term is considered a cross-level interaction, because sex is measured\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 255\n",
      "\n",
      "at the individual level (the “group” in this context), whereas Internet usage is\n",
      "measured at the within-individual level. Historically, prior to the widespread\n",
      "use of hierarchical modeling, this model was estimated simply using OLS re-\n",
      "gression with the relevant interaction. However, as we have discussed, and as\n",
      "this equation shows, the OLS approach is not optimal, because it absorbs the\n",
      "various random quantities (i.e., ui, viinternetit, and eit) into a single error\n",
      "term for each individual. These error terms are assumed to be independent\n",
      "across time-specific observations, but, as the single subscripting for ui and vi\n",
      "suggest, they are not truly independent.\n",
      "\n",
      "Returning to the Bayesian specification, the model can be implemented\n",
      "very easily in WinBugs with the following code:\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:9249)\n",
      "\n",
      "{\n",
      "\n",
      "for(t in 1:2)\n",
      "\n",
      "{\n",
      "\n",
      "y[i,t]~dnorm(mu[i,t],sigma2inv)\n",
      "\n",
      "mu[i,t]<-alpha[i]+beta[i]*internet[i,t]\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(ma[i],tauinv.alpha)\n",
      "\n",
      "beta[i]~dnorm(mb[i],tauinv.beta)\n",
      "\n",
      "ma[i]<-alpha0 + alpha1*sex[i]\n",
      "\n",
      "mb[i]<-beta0 + beta1*sex[i]\n",
      "\n",
      "}\n",
      "\n",
      "alpha0~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha1~dnorm(0,1.0E-4)\n",
      "\n",
      "beta0~dnorm(0,1.0E-4)\n",
      "\n",
      "beta1~dnorm(0,1.0E-4)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "tauinv.alpha~dgamma(.01,.01)\n",
      "\n",
      "tau.alpha<-1/sqrt(tauinv.alpha)\n",
      "\n",
      "tauinv.beta~dgamma(.01,.01)\n",
      "\n",
      "tau.beta<-1/sqrt(tauinv.beta)\n",
      "\n",
      "}\n",
      "\n",
      "The key results of this model are not only that men make higher wages than\n",
      "women (α(0) = 1.86; α(1) = .20), but also that Internet usage has substantially\n",
      "higher returns for men than for women (β(0) = .18; β(1) = .05). In fact, based\n",
      "on these point estimates, the return to income of Internet usage for men is\n",
      "28% greater than it is for women. A 95% interval estimate of this percentage\n",
      "is [11%, 48%].\n",
      "\n",
      "\n",
      "\n",
      "256 9 Introduction to Hierarchical Models\n",
      "\n",
      "9.2.3 Growth models\n",
      "\n",
      "Often, we may wish to include time as one of our variables affecting an out-\n",
      "come. For example, in the previous model, we allowed the effect of Internet\n",
      "usage on wages to vary across individuals, but we could also consider that\n",
      "wages grow at differential rates for individuals. Similarly, we found earlier\n",
      "that the influence of Internet usage on wages varied across time. We may\n",
      "therefore consider specifying a model in which wages are expected to grow at\n",
      "differential rates for individuals, with Internet usage influencing the rate of\n",
      "growth. This type of model is often called a “growth model,” or “latent growth\n",
      "model,” because we are modeling the time-specific outcomes as realizations of\n",
      "an underlying growth process that unfolds across age/time at the individual\n",
      "level. Such a model may look like:\n",
      "\n",
      "yit ∼ N(αi + βitit, σ2)\n",
      "αi ∼ N(α(0) + α(1)sexi + α(2)Interneti, τ2α)\n",
      "βi ∼ N(β(0) + β(1)sexi + β(2)interneti, τ2β)\n",
      "\n",
      "α(0) ∼ N(m1, s1)\n",
      "α(1) ∼ N(m2, s2)\n",
      "α(2) ∼ N(m3, s3)\n",
      "β(0) ∼ N(m4, s4)\n",
      "β(1) ∼ N(m5, s5)\n",
      "β(2) ∼ N(m6, s6)\n",
      "τ2α ∼ IG(a, b)\n",
      "τ2β ∼ IG(c, d)\n",
      "σ2 ∼ IG(e, f).\n",
      "\n",
      "Although this model has a lengthy specification, it is has a fairly straight-\n",
      "forward interpretation. Individual wages are expected to start and grow at\n",
      "individual-specific levels and rates (αi and βi, respectively). An individual’s\n",
      "specific level and rate is then seen as depending on his/her sex and Internet\n",
      "usage. The remaining lines of the model specification are simply hyperpriors\n",
      "for the various parameters.\n",
      "\n",
      "A couple of notes are in order regarding the growth model presented above.\n",
      "First, I have included Internet usage measured only at the first point in time.\n",
      "The reason for this is that the model is underidentified if we attempt to es-\n",
      "timate it with Internet usage treated as a time-varying covariate influencing\n",
      "individual-specific effects of time (see Exercises). Second, given that this model\n",
      "only consists of two waves of data, the model is only measuring the extent\n",
      "to which sex and Internet usage influence change in wages over a single time\n",
      "interval, making the model nothing more than a slightly different parame-\n",
      "terization of a change score regression model. Third, because of the limited\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 257\n",
      "\n",
      "number of waves, some additional constraints must be enforced. One is that\n",
      "the error variance σ2 must be constrained to be time invariant. Often, growth\n",
      "models allow this parameter to vary across time (see Bollen and Curran 2006),\n",
      "but here we simply cannot allow that, given our limitation of having only two\n",
      "time-specific measures per person. The results of this model can be found in\n",
      "Table 9.3.\n",
      "\n",
      "Table 9.3. Results of “growth” model for two-wave panel of income and Internet\n",
      "use data.\n",
      "\n",
      "Parameter Meaning Parameter Posterior Mean(s.d.)\n",
      "\n",
      "Adjusted intercept for time-1 wages α0 1.74(.016)\n",
      "Influence of sex on wages α1 0.259(.015)\n",
      "Influence of Internet on wages α2 0.296(.016)\n",
      "Adjusted intercept for change in wages β0 0.033(.009)\n",
      "Influence of sex on change in wages β1 −0.013(.009)\n",
      "Influence of Internet on change in wages β2 0.006(.009)\n",
      "Residual variance in wages σ2 0.308(.002)\n",
      "Residual variance in time-1 wages τ2α 0.383(.004)\n",
      "Residual variance in change in wages τ2β 0.061(.006)\n",
      "\n",
      "Note: Posterior means (and posterior standard deviations) are reported.\n",
      "\n",
      "These results indicate that sex and Internet usage each influence baseline\n",
      "wages, with men earning more than women (see α(1)) and Internet users earn-\n",
      "ing more than nonusers (see α2). Indeed, the Internet effect is roughly 20%\n",
      "larger than the sex effect. The results also indicate that wages grew slightly\n",
      "across the one-year time period (see β(0)). Wages grew less for men (see β(1)),\n",
      "but more for Internet users (see β(2)), although this effect was slight at best\n",
      "(observe the posterior standard deviation for β(2) compared with its mean).\n",
      "These results may also be written in equation form to clarify their interpre-\n",
      "tation:\n",
      "\n",
      "E(wagesit) = αi + βi\n",
      "E(αi) = 1.74 + .259malei + .296Interneti1\n",
      "E(βi) = .033− .013malei + .006interneti1.\n",
      "\n",
      "For a fuller, more detailed example involving more time points of measure-\n",
      "ment, I examine health trajectories of individuals across a 20-year span. My\n",
      "assumption is that health tends to decline across the life course of individuals,\n",
      "and that baseline health and the rate of decline in health are a function of age,\n",
      "sex, race, area and type of residence, and education. My primary interest is in\n",
      "examining how socioeconomic status (measured by education) influences the\n",
      "\n",
      "\n",
      "\n",
      "258 9 Introduction to Hierarchical Models\n",
      "\n",
      "health differential across time. One hypothesis in the literature—the cumula-\n",
      "tive advantage hypothesis—argues that the health gap between high and low\n",
      "SES groups widens across age as a function of the cumulative disadvantage\n",
      "that low SES generates across the life course (see Lynch 2003). At young ages,\n",
      "risk factors like smoking and lack of health care access matter little, because\n",
      "most young adults are quite healthy. However, across age, exposure to risk\n",
      "factors accumulates and produces a larger health differential. An alternate\n",
      "hypothesis is the age-as-leveler hypothesis. This hypothesis argues that the\n",
      "health gap narrows across age because age overwhelms all risk factors—the\n",
      "biological effect of aging supercedes any socially based risk factor (see House\n",
      "et al. 1994). Often a selective mortality argument is also advanced to support\n",
      "this hypothesis: that the observed health gap at a particular age is ultimately\n",
      "a between-individual measure, and only the health of survivors is observed.\n",
      "Thus, those with the poorest health have been eliminated from the observed\n",
      "population, and the gap is simply a comparison of a robust subset of lower\n",
      "SES individuals with higher SES individuals. In other words, there are differ-\n",
      "ent populations being compared at young and older ages (see Lynch 2003 for\n",
      "extensive discussion).\n",
      "\n",
      "A life course perspective suggests that we should examine trajectories of\n",
      "health for individuals, and that selective mortality should be “controlled out.”\n",
      "One way to do this is to allow decedents to be included in the model, rather\n",
      "than to exclude them, as cross-sectional analyses must do (because only sur-\n",
      "vivors can be observed in a cross-section). A Bayesian growth model can easily\n",
      "handle the unbalanced data that result from mortality, and health trajecto-\n",
      "ries can even be estimated for individuals for whom we only observe a sin-\n",
      "gle measure. Their trajectories become a compromise between their observed\n",
      "measures and those of persons with similar covariate profiles who do survive.\n",
      "Ultimately, this approach underestimates the rate of decline in health, because\n",
      "surviving low-SES individuals drive the estimate of the mean growth rate, and\n",
      "surely decedents have/had steeper—but unobserved—rates of health decline.\n",
      "However, this argument implies that the finding with regard to the cumulative\n",
      "advantage hypothesis are conservative.\n",
      "\n",
      "For this example, I again use the data from the National Health and Nutri-\n",
      "tion Examination Survey (NHANES) and its follow-ups, the National Health\n",
      "Epidemiologic Follow-up Surveys (NHEFS) (see Chapter 8 for a description).\n",
      "After eliminating individuals who were missing on one or another variable\n",
      "in the analyses and individuals whose final status in 1992 was unknown, the\n",
      "analytic sample consisted of 6,403 persons.\n",
      "\n",
      "In this example, I include only individuals who were between 30 and 34\n",
      "years of age at baseline, because age presents a problem in these analyses:\n",
      "The variable “age” represents both age and cohort. Research has shown that\n",
      "a common pattern for the health gap between individuals with low versus high\n",
      "SES across age is divergent until midlife and then convergent after (see House\n",
      "et al. 1994). This pattern is a function of two things: selective mortality and\n",
      "cohort change in the importance of education in affecting health (see Lynch\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 259\n",
      "\n",
      "2003). Thus, for the sake of simplicity in this example, I restrict the analyses\n",
      "to the 608 individuals who fall in this age range, eliminating cohort effects.\n",
      "\n",
      "I include age (mean = 32.0, s.d. = 1.4), sex (male = 1, 41.6%), race (non-\n",
      "white = 1, 12.3%), region (south = 1, 28.1%), urban residence (urban = 1,\n",
      "23.2%), and education (in years, mean = 12.6, s.d. = 2.6, minimum = 0,\n",
      "maximum = 17) as second-level covariates that may influence the random in-\n",
      "tercept and slope factors. The outcome measure is self-rated health measured\n",
      "on a 5-point Likert scale ranging from excellent health (5) to poor health (1).\n",
      "Health measured on a 5-point scale is known to be a reliable and valid indica-\n",
      "tor of health (especially at younger ages), and the data are fairly symmetric,\n",
      "with a slight skew toward excellent health. I expect that individuals random\n",
      "intercepts are relatively high, and that in general, health declines between\n",
      "30 and 55—the age range covered by the study. Furthermore, I expect that\n",
      "education differentiates health at baseline, with higher educated individuals\n",
      "having better health than lower educated ones. Finally, if the cumulative ad-\n",
      "vantage hypothesis is true at least prior to age 55, education serves to reduce\n",
      "the rate of decline in health. This hypothesis implies that the growth rate\n",
      "in health is negative in general, but that education’s influence on the growth\n",
      "rate is positive.\n",
      "\n",
      "Below is the WinBugs program specifying the growth model:\n",
      "\n",
      "model\n",
      "\n",
      "{\n",
      "\n",
      "for(i in 1:608)\n",
      "\n",
      "{\n",
      "\n",
      "for(t in 1:pyrs[i])\n",
      "\n",
      "{\n",
      "\n",
      "h[i,t]~dnorm(mu[i,t],sigma2inv)\n",
      "\n",
      "mu[i,t]<-alpha[i]+beta[i]*yr[i,t]\n",
      "\n",
      "}\n",
      "\n",
      "alpha[i]~dnorm(ma[i],tauinv.alpha)\n",
      "\n",
      "beta[i]~dnorm(mb[i],tauinv.beta)\n",
      "\n",
      "ma[i]<-alpha0 + alpha1*age[i] + alpha2*male[i] + alpha3*nonw[i] +\n",
      "\n",
      "alpha4*south[i] + alpha5*urban[i] + alpha6*educ[i]\n",
      "\n",
      "mb[i]<-beta0 + beta2*male[i] + beta3*nonw[i] +\n",
      "\n",
      "beta4*south[i] + beta5*urban[i] + beta6*educ[i]\n",
      "\n",
      "}\n",
      "\n",
      "alpha0~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha1~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha2~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha3~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha4~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha5~dnorm(0,1.0E-4)\n",
      "\n",
      "alpha6~dnorm(0,1.0E-4)\n",
      "\n",
      "beta0~dnorm(0,1.0E-4)\n",
      "\n",
      "beta2~dnorm(0,1.0E-4)\n",
      "\n",
      "beta3~dnorm(0,1.0E-4)\n",
      "\n",
      "beta4~dnorm(0,1.0E-4)\n",
      "\n",
      "\n",
      "\n",
      "260 9 Introduction to Hierarchical Models\n",
      "\n",
      "beta5~dnorm(0,1.0E-4)\n",
      "\n",
      "beta6~dnorm(0,1.0E-4)\n",
      "\n",
      "sigma2inv~dgamma(.01,.01)\n",
      "\n",
      "sigma2<-1/sqrt(sigma2inv)\n",
      "\n",
      "tauinv.alpha~dgamma(.01,.01)\n",
      "\n",
      "tau.alpha<-1/sqrt(tauinv.alpha)\n",
      "\n",
      "tauinv.beta~dgamma(.01,.01)\n",
      "\n",
      "tau.beta<-1/sqrt(tauinv.beta)\n",
      "\n",
      "}\n",
      "\n",
      "This program, although longer than our previous growth model program\n",
      "because of the inclusion of additional level 2 covariates, is only slightly differ-\n",
      "ent from it. In order for WinBugs to handle unbalanced data—that is, data\n",
      "collected at different times and on different numbers of occasions for different\n",
      "respondents—I include a variable called pyrs, which tells the program at how\n",
      "many occasions the respondent was interviewed, and time of measurement\n",
      "is treated as a time-specific, individual-level variable. Individuals who die—\n",
      "or are lost—before the first follow-up (after baseline) contribute only a single\n",
      "person-year record and single measure of time. Persons who die—or are lost—\n",
      "before the second follow-up contribute two person-year records, etc. In these\n",
      "data, there are 16 persons who contribute one person-year record, 7 persons\n",
      "who contribute two records, 6 who contribute three, and 579 who contribute\n",
      "the maximum of four. These data provide some initial indication that there\n",
      "is some education-based selective mortality: The mean for education among\n",
      "persons who contribute 4 person-records is 12.7, whereas the mean for those\n",
      "who contribute fewer records is 10.9. In other words, the less-educated die\n",
      "earlier than the more-educated.\n",
      "\n",
      "The remainder of the model is virtually identical to the one presented\n",
      "earlier, only with more covariates and therefore more hyperprior distributions.\n",
      "One note is in order: I do not include the effect of respondent’s age on growth.\n",
      "The reason for this is that for age to influence the growth rate, either (1) the\n",
      "underlying latent health trajectories must be assumed to be nonlinear or (2)\n",
      "there are cohort differences in growth rates (see Mehta and West 2000).\n",
      "\n",
      "I ran the program for 10,000 iterations and retained the last 1,000 samples\n",
      "for inference. Figure 9.2 shows 200 sampled values for the random intercepts\n",
      "and random slopes for four individuals. Person 1 only survived through the\n",
      "first wave of the study; person 17 survived through two waves; person 24\n",
      "survived through three waves; and person 35 survived through all four waves.\n",
      "As the figure shows, the scatter of points is widest for person 1, reflecting\n",
      "the lack of certainty about this individual’s true random intercept and slope\n",
      "values due to the existence of only one observed measure for his health. As\n",
      "the number of time points observed increases, the variance in the random\n",
      "intercept and slope for each individual decreases. For example, in the bottom\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 261\n",
      "\n",
      "right plot, the random intercept and slope scatter is centered very narrowly\n",
      "over approximately (3, −.08), which indicates that we are fairly certain that\n",
      "this individual’s latent trajectory starts around 3 health units at baseline and\n",
      "declines about .08 units per year.\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●●●●● ●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "0 1 2 3 4 5 6\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      ".2\n",
      "\n",
      "Person 1\n",
      "\n",
      "Random Intercept (α1)\n",
      "\n",
      "R\n",
      "a\n",
      "\n",
      "n\n",
      "d\n",
      "\n",
      "o\n",
      "m\n",
      "\n",
      " S\n",
      "lo\n",
      "\n",
      "p\n",
      "e\n",
      " (\n",
      "\n",
      "β 1\n",
      ")\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "●\n",
      "●●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "●● ●\n",
      "\n",
      "●●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "0 1 2 3 4 5 6\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      ".2\n",
      "\n",
      "Person 17\n",
      "\n",
      "Random Intercept (α17)\n",
      "R\n",
      "\n",
      "a\n",
      "n\n",
      "\n",
      "d\n",
      "o\n",
      "\n",
      "m\n",
      " S\n",
      "\n",
      "lo\n",
      "p\n",
      "\n",
      "e\n",
      " (\n",
      "\n",
      "β 1\n",
      "7\n",
      ")\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●● ●\n",
      "●●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●●\n",
      "\n",
      "●●\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "● ●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●●\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●● ●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "● ●●\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "● ●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "0 1 2 3 4 5 6\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      ".2\n",
      "\n",
      "Person 24\n",
      "\n",
      "Random Intercept (α24)\n",
      "\n",
      "R\n",
      "a\n",
      "\n",
      "n\n",
      "d\n",
      "\n",
      "o\n",
      "m\n",
      "\n",
      " S\n",
      "lo\n",
      "\n",
      "p\n",
      "e\n",
      " (\n",
      "\n",
      "β 2\n",
      "4\n",
      ")\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●●\n",
      "● ●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●● ●●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●● ●●\n",
      "\n",
      "●\n",
      "●● ●\n",
      "\n",
      "●●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●● ●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●●\n",
      "\n",
      "●●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "● ●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "0 1 2 3 4 5 6\n",
      "\n",
      "−\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      ".2\n",
      "\n",
      "Person 35\n",
      "\n",
      "Random Intercept (α35)\n",
      "\n",
      "R\n",
      "a\n",
      "\n",
      "n\n",
      "d\n",
      "\n",
      "o\n",
      "m\n",
      "\n",
      " S\n",
      "lo\n",
      "\n",
      "p\n",
      "e\n",
      " (\n",
      "\n",
      "β 3\n",
      "5\n",
      ")\n",
      "\n",
      "Fig. 9.2. Scatterplots of four persons’ random intercepts and slopes from growth\n",
      "curve model of health (posterior means superimposed as horizontal and vertical\n",
      "dashed lines).\n",
      "\n",
      "Table 9.4 presents the posterior means and standard deviations for the\n",
      "model parameters. The columns in the table report the influence of each co-\n",
      "variate on the random intercept and random slope. The intercept for the\n",
      "random intercept term was 4.52. Older persons (recall the age range was only\n",
      "30-34) reported worse health than younger persons at baseline (−.06). Men\n",
      "\n",
      "\n",
      "\n",
      "262 9 Introduction to Hierarchical Models\n",
      "\n",
      "reported better health at baseline than women (.05), and persons from the\n",
      "South reported worse health (−.05), but these effects were not substantially\n",
      "different from 0, based on posterior probabilities that the parameters were\n",
      "greater than (or less than) 0, truncated to the p-value ranges used by classical\n",
      "statistics (i.e., p < .05). Nonwhites and persons living in urban areas reported\n",
      "worse health than whites and persons living in other areas. Finally, education\n",
      "had a strong, positive effect on baseline health.\n",
      "\n",
      "Almost none of the covariates influenced the random slope. The inter-\n",
      "cept for the random slope was negative, implying that the tendency was for\n",
      "health to decline slightly across the 20-year period. Males and nonwhites had\n",
      "a slightly steeper decline in health, although these effects would not be sta-\n",
      "tistically significant by classical standards. Persons from the South and from\n",
      "urban areas had shallower declines in health than persons from other areas,\n",
      "although, again, these effects would not be statistically significant by classical\n",
      "standards. Finally, education had the expected positive effect (.001, p < .1),\n",
      "indicating that health trajectories do diverge across age (for the range from\n",
      "age 30 to age 55), such that persons with more education experience a shal-\n",
      "lower decline in health across age than persons with less education. Indeed,\n",
      "although the coefficient’s magnitude appears small, the results indicate that\n",
      "a person with 17 years of schooling (the maximum) would experience a rate\n",
      "of health decline only 43% as great as a person with 0 years of schooling and\n",
      "only 76% as great as a person with 12 years of schooling.\n",
      "\n",
      "Table 9.4. Results of growth curve model of health across time.\n",
      "\n",
      "Variable Random Intercept Random Slope\n",
      "\n",
      "Intercept 4.52(.70)*** −0.03(.01)**\n",
      "Age −0.06(.02)**\n",
      "Male 0.05(.08) −0.006(.005)\n",
      "Nonwhite −0.54(.11)*** −0.006(.008)\n",
      "South −0.05(.08) 0.003(.005)\n",
      "Urban −0.23(.08)*** 0.003(.005)\n",
      "Education 0.11(.01)*** 0.001(.0009)#\n",
      "Variance 0.37(.03) 0.001(.0001)\n",
      "Within-ind. Variance 0.42(.02)\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The p-values are the probabilities\n",
      "that the parameter exceeds 0 (either positively or negatively), truncated to the\n",
      "classical cutpoints of #p < .1, *p < .05, **p < .01, ***p < .001.\n",
      "\n",
      "The results can be used in two ways to predict health trajectories. First,\n",
      "we may directly use the simulated latent intercepts and slopes for individuals\n",
      "in the sample (as shown in Figure 9.2). For example, we could use the poste-\n",
      "\n",
      "\n",
      "\n",
      "9.2 Hierarchical linear regression models 263\n",
      "\n",
      "rior means for these simulated intercepts and slopes to construct an expected\n",
      "trajectory: yit = µαi + µβi × t. Second, we may use the posterior distribu-\n",
      "tions for the model parameters—the covariate effects—to compute predicted\n",
      "latent intercepts and slopes for persons with particular covariate profiles. This\n",
      "approach allows us to predict trajectories for individuals out of the sample,\n",
      "in addition to those in the sample. Person 1 shown in Figure 9.2 was a 31-\n",
      "year-old nonwhite male living in a non-southern, urban area with 11 years of\n",
      "schooling. Based on the posterior means for the effects of the covariates, this\n",
      "individual would have a predicted intercept of 3.22 for his health trajectory\n",
      "and a predicted slope of −.022.\n",
      "\n",
      "Figure 9.3 shows these two types of predicted trajectories for the four\n",
      "individuals shown in the previous figure, along with their observed health\n",
      "measures. The solid line in each graph shows the predicted trajectory based\n",
      "on the posterior means of the simulated, individual-specific random intercepts\n",
      "and slopes (i.e., the simulated values from Figure 9.2). The dashed line in each\n",
      "graph shows the model predicted trajectory based on the posterior means of\n",
      "the parameters applied to each individual’s covariate profile.\n",
      "\n",
      "There is a substantial difference between these two trajectories, as well as\n",
      "between either trajectory and the observed health measures. This variation\n",
      "reflects the different types of (error) variance captured by the model. The\n",
      "discrepancies between the solid-line trajectories and the observed health mea-\n",
      "sures are captured by the within-individual error variance parameter σ2. In\n",
      "brief, we do not expect each individual’s health measure to fall exactly on the\n",
      "solid line, because a number of unobserved factors may “bump” an individual\n",
      "off of his/her expected, latent health trajectory at any point in time. Instead,\n",
      "what the model has attempted to capture is the best fitting line for the ob-\n",
      "served health measures. This error may be reduced by including time-specific\n",
      "measures into the model as we did in the previous section in the model in\n",
      "which we included Internet usage as a time-varying covariate.\n",
      "\n",
      "The discrepancies between the solid and dashed-line trajectories, on the\n",
      "other hand, reflect the extent of between-individual variation captured (or\n",
      "not!) by the covariates in the model. Put another way, if the covariates per-\n",
      "fectly explained all differences between individuals’ health trajectories, the\n",
      "solid and dashed lines would perfectly coincide. The fact that these lines are\n",
      "not overlapping suggests that our covariates do a poor job differentiating indi-\n",
      "viduals in the sample. This conclusion is foretold by the lack of strong results\n",
      "in Table 9.4, especially with respect to the general lack of effect of covari-\n",
      "ates on the latent growth rate. Indeed, if we consider the estimated rate of\n",
      "decline in health for each individual in Figure 9.3, all four individuals are\n",
      "expected to have similar, shallow rates of health decline that obviously do\n",
      "not match the observed health declines (or those predicted by the simulated\n",
      "individual-specific random effects). In contrast, the estimated intercepts for\n",
      "these trajectories show greater variability, reflecting the stronger effects of the\n",
      "covariates in predicting baseline health. In an additional model (not shown),\n",
      "I re-estimated this growth model with no covariates to obtain estimates of\n",
      "\n",
      "\n",
      "\n",
      "264 9 Introduction to Hierarchical Models\n",
      "\n",
      "the variance of the mean latent intercept and slope. An R2 for the effects\n",
      "of the covariates on the estimated latent intercept was found by computing\n",
      "1− τ2α,cov/τ2α,nocov, using the posterior means for these variance parameters\n",
      "from the two models. A similar calculation was performed for the variance of\n",
      "the latent slope (τ2β). The results indicated that the covariates reduced the\n",
      "between-individual variance in the latent intercept by 29% (i.e., R2 = .29),\n",
      "but the covariates reduced the between-individual variance in the latent slope\n",
      "by only 10%. These results confirm that our covariates have little effect on the\n",
      "latent slope, and therefore, it is no surprise that our two types of predicted\n",
      "trajectories differ substantially.\n",
      "\n",
      "As a final note on growth modeling, the use of growth models has been\n",
      "rapidly expanding in psychology and sociology over the last decade, in part\n",
      "because the growing availability of longitudinal (panel) data has enabled the\n",
      "investigation of life course processes for which growth modeling is well suited.\n",
      "Additionally, growth models have become increasingly popular, because they\n",
      "can be estimated via a variety of software packages, including HLM and var-\n",
      "ious structural equation modeling packages (see Willett and Sayer 1994; see\n",
      "also McArdle and Epstein 1987, Meredith and Tisak 1990 and Rogosa and\n",
      "Willett 1985). The HLM approach closely resembles the modeling strategy\n",
      "developed in this section. The structural equation modeling approach, on the\n",
      "other hand, is in some ways more intuitive, although it is mathematically\n",
      "equivalent to the Bayesian and HLM approaches.5 However, that approach\n",
      "typically requires balanced data—that is, data that have been collected at\n",
      "the same time and at all times for all individuals in the sample. This lat-\n",
      "ter requirement can be relaxed by assuming that individuals who are missing\n",
      "at one or more occasions are missing at random and estimating the model\n",
      "using a full information maximum likelihood (FIML) estimator. The former\n",
      "restriction, however, is not easily relaxed. However, estimating the model us-\n",
      "ing a Bayesian approach or using other hierarchical modeling packages offer a\n",
      "straightforward way to handling unbalanced data. For more details on latent\n",
      "growth modeling within a structural equation modeling framework, I highly\n",
      "recommend Bollen and Curran (2006).\n",
      "\n",
      "9.3 A note on fixed versus random effects models and\n",
      "other terminology\n",
      "\n",
      "One issue that makes understanding hierarchical models difficult is the ter-\n",
      "minology that different disciplines and statistical paradigms use to describe\n",
      "various features of the models. In this section, I hope to clarify some of the\n",
      "terminology, although there is certain to be some disagreement regarding my\n",
      "\n",
      "5 In fact, for each growth model example presented here, I estimated the equivalent\n",
      "model using a structural equation approach. The results were nearly identical.\n",
      "\n",
      "\n",
      "\n",
      "9.3 A note on fixed versus random effects models and other terminology 265\n",
      "\n",
      "●\n",
      "\n",
      "● ● ●0 5 10 15 20\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Person 1\n",
      "\n",
      "Years Since Baseline\n",
      "\n",
      "H\n",
      "e\n",
      "\n",
      "a\n",
      "lth\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●0 5 10 15 20\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Person 17\n",
      "\n",
      "Years Since Baseline\n",
      "\n",
      "H\n",
      "e\n",
      "\n",
      "a\n",
      "lth\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●0 5 10 15 20\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Person 24\n",
      "\n",
      "Years Since Baseline\n",
      "\n",
      "H\n",
      "e\n",
      "\n",
      "a\n",
      "lth\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "0 5 10 15 20\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "3\n",
      "\n",
      "4\n",
      "5\n",
      "\n",
      "Person 35\n",
      "\n",
      "Years Since Baseline\n",
      "\n",
      "H\n",
      "e\n",
      "\n",
      "a\n",
      "lth\n",
      "\n",
      "Fig. 9.3. Predicted trajectories and observed health for four persons: The solid lines\n",
      "are the predicted trajectories based on the posterior means of the random intercepts\n",
      "and slopes from Figure 9.2; and the dashed lines are the predicted trajectories based\n",
      "on the individuals’ covariate profiles and posterior means of the parameters in Ta-\n",
      "ble 9.4\n",
      "\n",
      "use of terms. To be sure, many of the terms used in discussions of hierarchical\n",
      "modeling have not had static definitions over time, adding to the confusion.\n",
      "\n",
      "First, the terms “fixed effects” and “random effects” are frequently tossed\n",
      "about in discussions of hierarchical modeling. From a Bayesian perspective,\n",
      "controversy over these terms is often much ado about nothing, because from\n",
      "a Bayesian view (1) parameters are seen as random quantities arising from\n",
      "proper probability distributions, making all effects “random”; and (2) fixed\n",
      "effects models generally contain “random” effects, making the distinction be-\n",
      "tween fixed and random effects models somewhat dubious. Consider the OLS\n",
      "\n",
      "\n",
      "\n",
      "266 9 Introduction to Hierarchical Models\n",
      "\n",
      "regression model Y = Xβ + e, which is often considered to be a fixed ef-\n",
      "fects regression model. In this model, X is considered a fixed variable matrix,\n",
      "and β is considered a fixed regression parameter vector—i.e., “fixed effects.”\n",
      "From a classical statistical standpoint, the only random quantity in this model\n",
      "is the vector e, which is generally portrayed as random by the expression\n",
      "e ∼ N(0, σ2eIn). In other words, in the classical representation, e is a random\n",
      "effect because it comes from a specified probability distribution. The β vector,\n",
      "on the other hand, is considered fixed—these parameters are what they are\n",
      "in the population and do not stem from a probability distribution. From a\n",
      "Bayesian view, however, β may be considered as a vector of random effects,\n",
      "because we can produce a posterior probability distribution for the vector.\n",
      "The only difference between the Bayesian approach to this model and the\n",
      "classical approach is that the classical approach implicitly assumes uniform\n",
      "prior distributions on β, whereas a Bayesian approach makes this assumption\n",
      "explicit in the formulation of the prior. Whether we consider β fixed or ran-\n",
      "dom, nonetheless, one could argue that the model is a random effects model\n",
      "with some fixed effects (β) if the priors are left unspecified.\n",
      "\n",
      "Next, consider the basic random effects model considered in this chapter\n",
      "in which individuals have their “own” intercepts or means:\n",
      "\n",
      "yit ∼ N(αi, σ2),\n",
      "\n",
      "with αi ∼ N(α0, τ2). From a Bayesian perspective, this model is considered\n",
      "a random effects model, because the αi are treated as arising from a normal\n",
      "distribution with parameters α0 and τ2. A classical statistician, on the other\n",
      "hand, might introduce a dummy variable for each observation, coupled with a\n",
      "β for each dummy variable, and call this model a fixed effects model, because\n",
      "the β vector could be considered a fixed parameter vector. In other words,\n",
      "the classical statistician may specify the model as an OLS regression model,\n",
      "Y = Xβ + e, again with X being a matrix of dummy variables, β being a\n",
      "vector of effects of these dummy variables, and e ∼ N(0, σ2e) being considered\n",
      "the only random quantity. The data structure in this specification would be a\n",
      "person-year matrix, with each individual contributing t rows, with X having\n",
      "dummy variables for each person-record corresponding to each person. From\n",
      "a Bayesian view, this is a random effects model, but from a classical view, this\n",
      "is still a fixed effects model. The Bayesian, however, recognizes that, again,\n",
      "the only difference between these models is the explicit statement that each\n",
      "αi (intercept/mean) has a proper prior distribution; the classical statistician\n",
      "again implicitly assumes uniform priors distributions on these “fixed” effects.\n",
      "\n",
      "The next step in our modeling process in this chapter was to incorporate\n",
      "additional individual-level (level 2) variables via essentially the decomposition\n",
      "of the intercept term into a regression on individual-level factors. Specifically,\n",
      "we allowed individuals’ αi to be a function of their sex. One representation of\n",
      "this model is:\n",
      "\n",
      "\n",
      "\n",
      "9.3 A note on fixed versus random effects models and other terminology 267\n",
      "\n",
      "yit ∼ N(αi, σ2)\n",
      "αi ∼ N(α(0) + α(1)sexi, τ2),\n",
      "\n",
      "along with appropriate (vague) hyperpriors for the hyperparameters α(0), α(1),\n",
      "σ2, and τ2. Alternatively, but equivalently, the model may be specified as we\n",
      "did earlier:\n",
      "\n",
      "yit ∼ N(αi + α(1)sexi, σ2)\n",
      "αi ∼ N(α0, τ2),\n",
      "\n",
      "again with appropriate priors for α(0), α(1), σ2, and τ2. A Bayesian then\n",
      "would call this a random intercept model. The classical statistician, on the\n",
      "other hand, would write this model as:\n",
      "\n",
      "yit = αi + α1sexi + eit\n",
      "eit ∼ N(0, σ2)\n",
      "αi = α0 + ui\n",
      "uit ∼ N(0, τ2).\n",
      "\n",
      "After substituting the third equation into the first, we would obtain:\n",
      "\n",
      "yit = α0 + ui + α1sexi + eit.\n",
      "\n",
      "Under this representation, the classical statistician would claim that α0 and\n",
      "α1 are fixed effects, and that the only random effects are ui and eit. If ui\n",
      "is considered a component of α0, then the model could be called a random\n",
      "intercept model with fixed effects. Once again, however, the Bayesian would\n",
      "argue that the explicit assignment of proper priors for α0 and α1 makes the\n",
      "model a random effects model: The classical approach is implicitly assuming\n",
      "uniform priors on these parameters.\n",
      "\n",
      "In subsequent steps of our modeling building process, we included Internet\n",
      "usage as a time-varying (level 1) variable, and we eventually allowed the influ-\n",
      "ence of Internet usage on wages to vary across individuals and we allowed the\n",
      "individual-specific influence of Internet usage to be a function of individuals’\n",
      "sex:\n",
      "\n",
      "yit ∼ N(αi + βiInternetit, σ2)\n",
      "αi ∼ N(α0 + α1sexi, τ2α)\n",
      "βi ∼ N(β0 + β1sexi, τ2β),\n",
      "\n",
      "once again with appropriate hyperprior distributions for the higher level hy-\n",
      "perparameters. Using Bayesian terminology, this model is a “random coeffi-\n",
      "cients” model, because the regression coefficient βi is allowed to vary across\n",
      "individuals. The classical approach, however, would find\n",
      "\n",
      "\n",
      "\n",
      "268 9 Introduction to Hierarchical Models\n",
      "\n",
      "yit = α0 + α1sexi + ui + β0Internetit + β1sexiInternetit + viInternetit + eit\n",
      "\n",
      "after subsitution and might call the model a fixed effects model with random\n",
      "intercepts, random coefficients, and cross-level interactions.\n",
      "\n",
      "To make a long story short, all of these models are considered hierarchi-\n",
      "cal models because there is a hierarchical structure to the parameters. They\n",
      "may also be called multilevel models because the variables in the models are\n",
      "measured at different levels (time-specific measures and individual-level mea-\n",
      "sures). Additionally, all the models contain random effects and may therefore\n",
      "be called random effects models, despite the fact that the classical statistician\n",
      "may prefer to include the term “fixed effects” in describing them. When the\n",
      "regression parameters—and not simply the intercepts—are allowed to vary\n",
      "across individuals, they may be called “random coefficient models.” When\n",
      "time is included as a variable and its influence—a random coefficient—is al-\n",
      "lowed to vary across individuals, the model may be called a “(latent) growth\n",
      "(curve) model.” Finally, all of these models are sometimes called “mixed mod-\n",
      "els,” because they generally include both fixed and random effects when the\n",
      "terms “fixed” and “random” are applied to distinguish between effects that\n",
      "have implicit versus explicit prior distributions.\n",
      "\n",
      "9.4 Conclusions\n",
      "\n",
      "In this chapter, we have covered considerable ground. We began by discussing\n",
      "how we can use the conditional probability rule to produce hierarchical struc-\n",
      "ture in the parameters and obtain a posterior distribution for all parameters\n",
      "in a simple model without covariates. We then discussed how hierarchical re-\n",
      "gression models can easily be constructed to capture hierarchical structure in\n",
      "both the parameters and the data with variables measured at different lev-\n",
      "els. Finally, we showed how the general hiearchical linear regression model\n",
      "can be specialized to examine growth in the outcome over time by including\n",
      "time in the model as a covariate. As the chapter demonstrated, the Bayesian\n",
      "approach is naturally suited to hierarchical modeling. Indeed, the Bayesian\n",
      "approach handles hierarchicality so easily that virtually no text on Bayesian\n",
      "statistics omits hierarchical modeling, and I can find no Bayesian text that\n",
      "only covers hierarchical modeling. For further reading on Bayesian hierarchi-\n",
      "cal modeling, as I said at the beginning of the chapter, I recommend Gelman\n",
      "et al. (1995). I also recommend Gill (2002) for an introductory exposition,\n",
      "and I suggest Spiegelhalter et al. (1996) for an illustrative example of growth\n",
      "modeling.\n",
      "\n",
      "\n",
      "\n",
      "9.5 Exercises 269\n",
      "\n",
      "9.5 Exercises\n",
      "\n",
      "1. Show the steps in the derivation of the conditional posterior distribution\n",
      "for τ in Equation 9.5.\n",
      "\n",
      "2. Explain why the values chosen to complete the hyperprior specification in\n",
      "Section 9.2.1 are noninformative.\n",
      "\n",
      "3. Explain in your own words why the first growth model presented in Sec-\n",
      "tion 9.2.3 cannot allow Internet usage to be a time-varying variable in the\n",
      "model as it is specified.\n",
      "\n",
      "4. Using your own data, write an R routine to estimate a growth model.\n",
      "Then write a WinBugs routine to estimate the same model. Are the results\n",
      "similar?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "\n",
      "Introduction to Multivariate Regression\n",
      "Models\n",
      "\n",
      "In the social sciences, we commonly use multiple outcome variables in an-\n",
      "swering a research question rather than a single outcome variable. In these\n",
      "cases, we may prefer to construct “multivariate models” rather than a series\n",
      "of univariate models like those presented in the previous three chapters. Typ-\n",
      "ically, when social scientists refer to multivariate models, they mean simply\n",
      "that they are estimating a model with more than one variable; however, for\n",
      "a statistician, the term “multivariate model” generally implies that a model\n",
      "has more than one outcome/response variable. A variety of different types of\n",
      "multivariate response models exists; in this chapter, we will consider only two\n",
      "of them: the multivariate linear regression model and the multivariate probit\n",
      "model. My goal in this chapter is not to provide an exhaustive account of\n",
      "multivariate models, a task that would require an entire book. Instead, my\n",
      "goal is to present the basic ideas of multivariate modeling.\n",
      "\n",
      "There are two primary reasons for constructing a multivariate model in\n",
      "place of a set of univariate ones: (1) to eliminate biases or improve efficiency\n",
      "over using a set of univariate models, and (2) to facilitate the use of model\n",
      "parameters from a joint model in making inference for unestimated parame-\n",
      "ters/quantities. We will consider the first reason first in reconsidering the or-\n",
      "dinary least squares (OLS) regression models for “niceness” from Chapter 7.\n",
      "Then we will consider the latter reason in an example for the multivariate\n",
      "probit model.\n",
      "\n",
      "10.1 Multivariate linear regression\n",
      "\n",
      "10.1.1 Model development\n",
      "\n",
      "The most basic type of multivariate regression model is the extension of the\n",
      "OLS regression model to handle an M -length vector of responses Y for each\n",
      "individual in a sample, rather than a single response outcome (i.e., M = 1).\n",
      "For example, reconsidering the example from Chapter 7 in which we evaluated\n",
      "\n",
      "\n",
      "\n",
      "272 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "whether Southerners are “nicer” than persons who have always lived in other\n",
      "regions of the country, we could have treated the four outcomes—empathy,\n",
      "selflessness, tolerance, and altruistic acts—as a vector of responses for each\n",
      "individual rather than as separate responses. The result would have been a\n",
      "single model, rather than four separate ones. In that case, the X variables\n",
      "would have been the same for each equation, but each X would have been\n",
      "allowed to have a different effect on each outcome. This model can be expressed\n",
      "just as the OLS regression model:\n",
      "\n",
      "Y = Xβ + e,\n",
      "\n",
      "but the dimensions of the model matrices are expanded to handle the addi-\n",
      "tional responses. In this representation, Y and e are now n×M, rather than\n",
      "n× 1, with the additional columns representing the additional dimensions of\n",
      "response. X is still n× k, but now β is expanded to be k ×M to allow each\n",
      "X to have a distinct influence on each outcome. Thus, the expanded matrix\n",
      "representation appears as:\n",
      "\n",
      "\n",
      " y11 . . . y1M... . . . ...\n",
      "\n",
      "yn1 . . . ynM\n",
      "\n",
      "\n",
      " =\n",
      "\n",
      "\n",
      " x11 . . . x1k... . . . ...\n",
      "\n",
      "xn1 . . . xnk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " β11 . . . β1M... . . . ...\n",
      "\n",
      "βk1 . . . βkM\n",
      "\n",
      "\n",
      " +\n",
      "\n",
      "\n",
      " e11 . . . e1M... . . . ...\n",
      "\n",
      "en1 . . . enM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "n×M n× k k ×M n×M\n",
      "\n",
      ",\n",
      "\n",
      "(10.1)\n",
      "When the errors from each regression equation are unrelated, i.e., (eT e)ij =\n",
      "0,∀i 6= j, the model is equivalent to running independent univariate OLS\n",
      "regression models. In that case, the OLS solution of β̂ = (XT X)−1(XT Y ) is\n",
      "still the optimal classical solution—only the dimensionality of the solution has\n",
      "changed—and, under uniform priors, this solution is still the posterior mean\n",
      "for β. That is, p(β|X, Y ) ∼ N(β̂OLS, σ\n",
      "\n",
      "2\n",
      "e(X\n",
      "\n",
      "T X)−1), where σ2e is a diagonal\n",
      "matrix of error variances.\n",
      "\n",
      "However, when errors are correlated across equations, the univariate and\n",
      "multivariate approaches are not necessarily equivalent. It can be shown that,\n",
      "when the cross-equation errors are correlated, the independent OLS solutions\n",
      "will not be the same as the multivariate solution if (1) the covariate vectors are\n",
      "not identical across equations; that is, if the X vectors differ across equations,\n",
      "or (2) if cross-equation constraints are imposed. For example, suppose we\n",
      "allowed education to influence the empathy outcome but not the other three\n",
      "outcomes. Alternatively, suppose we forced education’s influence on empathy\n",
      "and altruistic acts to be the same, or we fixed its effect to be some constant. In\n",
      "these cases, this model is called the “seemingly-unrelated regression model”\n",
      "in econometrics (see Zellner 1962), and the OLS solution will no longer be\n",
      "“best”—it will be less efficient than the multivariate solution.\n",
      "\n",
      "To obtain the multivariate solution, we begin with construction of a mul-\n",
      "tivariate likelihood function. Whereas with the univariate linear regression\n",
      "\n",
      "\n",
      "\n",
      "10.1 Multivariate linear regression 273\n",
      "\n",
      "model the likelihood function involved the use of a univariate normal sam-\n",
      "pling density for the observations (error term), the multivariate regression\n",
      "model requires us to use the multivariate normal distribution for the ob-\n",
      "servations (error terms). The multivariate normal likelihood function, when\n",
      "Y ∼ MV N(Xβ, Σ) is:\n",
      "\n",
      "L(β, Σ|X, Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "|Σ|−(1/2) exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2\n",
      "eTi Σ\n",
      "\n",
      "−1ei\n",
      "\n",
      "}\n",
      ", (10.2)\n",
      "\n",
      "where ei is the M × 1 vector of errors for the ith individual.\n",
      "To make the analysis fully Bayesian, it is common to assume independent\n",
      "\n",
      "priors on β and Σ, and to use improper uniform (U(−∞,∞)) priors on the\n",
      "elements of β and a noninformative prior for Σ. A common noninformative\n",
      "prior for Σ is the Jeffreys prior: p(Σ) ∝ |Σ|−(M+1)/2. With these priors, the\n",
      "posterior is simply:\n",
      "\n",
      "p(β, Σ|X, Y ) ∝ |Σ|−(M+n+1/2) exp\n",
      "{\n",
      "−\n",
      "\n",
      "1\n",
      "2\n",
      "tr(SΣ−1)\n",
      "\n",
      "}\n",
      ", (10.3)\n",
      "\n",
      "where S =\n",
      "∑n\n",
      "\n",
      "i=1 eie\n",
      "T\n",
      "i is the M ×M matrix of the sums of cross-products of\n",
      "\n",
      "errors, and tr(SΣ−1) is simply an equivalent expression to\n",
      "∑\n",
      "\n",
      "eTi Σ\n",
      "−1ei.\n",
      "\n",
      "Written this way, deriving the conditional distribution for Σ (p(Σ|β, X, Y ))\n",
      "is straightforward: It is recognizable as inverse Wishart, with scale matrix S\n",
      "and n degrees of freedom.1\n",
      "\n",
      "Deriving the conditional posterior distribution for β is not as simple. How-\n",
      "ever, Zellner (1962) and others (e.g., Gelman et al. 1995; Judge et al. 1985)\n",
      "have shown that the multivariate regression model from Equation 10.1 can\n",
      "be rewritten as a univariate regression, where the columns of Y and e can be\n",
      "“stacked” (this is called the “vec()” operator in matrix algebra) so that Y is\n",
      "Mn× 1, rather than n×M , as is e. β can also be stacked to be Mk× 1, and\n",
      "the X matrix can be be constructed as a block-diagonal matrix to yield the\n",
      "following alternative specification for the multivariate model:\n",
      "\n",
      "1 As with other normal-inverse gamma and multivariate normal-inverse Wishart\n",
      "setups, we can derive the marginal distribution for the covariance matrix; however,\n",
      "it is just as easy under Gibbs sampling to use the conditional posterior distribution\n",
      "for the matrix.\n",
      "\n",
      "\n",
      "\n",
      "274 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "\n",
      "y11\n",
      "...\n",
      "\n",
      "yn1\n",
      "y12\n",
      "...\n",
      "\n",
      "yn2\n",
      "...\n",
      "\n",
      "y1m\n",
      "...\n",
      "\n",
      "ynm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "X1 0 . . . 0\n",
      "0 X2 . . . 0\n",
      "...\n",
      "\n",
      "...\n",
      ". . .\n",
      "\n",
      "...\n",
      "0 0 . . . XM\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "β11\n",
      "...\n",
      "\n",
      "βk1\n",
      "β12\n",
      "...\n",
      "\n",
      "βk2\n",
      "...\n",
      "\n",
      "β1m\n",
      "...\n",
      "\n",
      "βkm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "+\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "e11\n",
      "...\n",
      "\n",
      "en1\n",
      "e12\n",
      "...\n",
      "\n",
      "en2\n",
      "...\n",
      "\n",
      "e1m\n",
      "...\n",
      "\n",
      "enm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ". (10.4)\n",
      "\n",
      "In this representation, Xm is the mth repetition of the n× k matrix X. How-\n",
      "ever, it is not necessary for Xi ≡ Xj—i.e., Xi and Xj may contain different\n",
      "numbers of variables—but if the same X matrix is used in all equations, pro-\n",
      "gramming can be made relatively simple, as we will see. The solution for the\n",
      "regression coefficients is:\n",
      "\n",
      "β̂ = (XT Ω−1X)−1(XT Ω−1Y ), (10.5)\n",
      "\n",
      "which is simply the generalized least squares (GLS) estimator. The standard\n",
      "error is then (XT Ω−1X)−1. The GLS solution for β̂ is the same as the ex-\n",
      "pected value for β in a Bayesian analysis with the noninformative priors we\n",
      "have selected, and so the conditional posterior distribution for the regression\n",
      "coefficients is:\n",
      "\n",
      "f(β|Σ,X, Y ) ∼ MV N((XT Ω−1X)−1(XT Ω−1Y ) , (XT Ω−1X)−1). (10.6)\n",
      "\n",
      "In this distribution, Ω = Σ ⊗ In (and Ω−1 = Σ−1 ⊗ In), where ⊗ indicates\n",
      "Kronecker multiplication, and In is the n-dimensional identity matrix. The\n",
      "Kronecker product of matrices A and B is defined as:\n",
      "\n",
      "A⊗B =\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "A11B A12B . . . A1JB\n",
      "A21B A22B . . . A2JB\n",
      "\n",
      "...\n",
      "...\n",
      "\n",
      ". . .\n",
      "...\n",
      "\n",
      "AI1B AI2B . . . AIJB\n",
      "\n",
      "\n",
      " (10.7)\n",
      "\n",
      "That is, each element of the first matrix is replaced with the multiple of that\n",
      "element by the entire second matrix. Thus, if matrix A is I × J , and matrix\n",
      "B is R× C, the result of the Kronecker product A⊗B will be IR× JC.\n",
      "\n",
      "Mathematically, the multivariate regression solution for the regression co-\n",
      "efficients seems straightforward, and a Gibbs sampler seems to be an easy\n",
      "repetition of two steps: (1) Simulate the regression parameters from their\n",
      "conditional posterior distribution defined in Equation 10.3, and (2) simulate\n",
      "\n",
      "\n",
      "\n",
      "10.1 Multivariate linear regression 275\n",
      "\n",
      "Σ from the appropriate inverse Wishart distribution, using the error cross-\n",
      "product matrix S that can be obtained after step (1).\n",
      "\n",
      "Unfortunately, it is not so easy to implement this Gibbs sampler with\n",
      "typical social science data because of the size of the Ω matrix. Simulating the\n",
      "regression parameters when the sample size is small is simple. However, when\n",
      "the sample size is large, the Ω matrix becomes unwieldy. For example, in the\n",
      "OLS regression example from Chapter 7, the sample size is n = 2, 313 (with\n",
      "the cases with missing data listwise deleted), and the dimensionality of the\n",
      "outcome is M = 4. Thus, the Ω matrix is the Kronecker product of a 4 × 4\n",
      "matrix with a 2, 313×2, 313 matrix—a 9, 252×9, 252 matrix. This matrix has\n",
      "more than 85 million elements, which is computationally impossible to handle\n",
      "directly. First, the memory requirements for a matrix of this size may exceed\n",
      "the capacity of the computer and/or software (this matrix would require some\n",
      "680 Mb of memory). Second, even if we could construct a matrix of this size,\n",
      "performing repeated computations with this matrix would be extremely costly,\n",
      "in terms of the time it would take to run the Gibbs sampler. So, how can we\n",
      "proceed?\n",
      "\n",
      "Judge et al (1985:468) show that the GLS solution for the β vector can be\n",
      "written as:\n",
      "\n",
      "2\n",
      "6664\n",
      "\n",
      "β1\n",
      "β2\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "βM\n",
      "\n",
      "3\n",
      "7775 =\n",
      "\n",
      "2\n",
      "66664\n",
      "\n",
      "σ11(XT1 X1) σ\n",
      "12(XT1 X2) . . . σ\n",
      "\n",
      "1M (XT1 XM )\n",
      "\n",
      "σ21(XT2 X1) σ\n",
      "22(XT2 X2) . . . σ\n",
      "\n",
      "2M (XT2 XM )\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      ". . .\n",
      ".\n",
      ".\n",
      ".\n",
      "\n",
      "σM1(XTM X1) σ\n",
      "M2(XTM X2) . . . σ\n",
      "\n",
      "MM (XTM XM )\n",
      "\n",
      "3\n",
      "77775\n",
      "−1 266664\n",
      "\n",
      "PM\n",
      "i=1 σ\n",
      "\n",
      "1i(XT1 Yi)PM\n",
      "i=1 σ\n",
      "\n",
      "2i(XT2 Yi)\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".PM\n",
      "i=1 σ\n",
      "\n",
      "Mi(XTM Yi)\n",
      "\n",
      "3\n",
      "77775 ,\n",
      "(10.8)\n",
      "\n",
      "where M is the dimension of the outcome, so that each β is a vector; σij is\n",
      "the (i, j)th element of Σ−1; Xm is the X matrix for the mth equation; and\n",
      "Yi is the n× 1 vector for outcome i.\n",
      "\n",
      "If Xi ≡ Xj ,∀i 6= j—that is, the same regressors are used in all equations–\n",
      "the first matrix can be reduced to Σ−1 ⊗ (XT X). Although this expression\n",
      "involves the use of a Kronecker product, the matrices involved are not over-\n",
      "whelmingly large. Σ−1 is M ×M , and XT X is k × k, and so the Kronecker\n",
      "product is just Mk × Mk. In the OLS regression example, this would be\n",
      "(4)(9) × (4)(9), which is a very manageable matrix size. The latter term in\n",
      "Equation 10.8 can also be written slightly differently to simplify computation.\n",
      "If we compute XT Y using the original n ×M specification for Y and post-\n",
      "multiply it by Σ−1, we end up with a k ×M matrix to which we can apply\n",
      "the vec() operator to convert it to a km × 1 column vector. This approach\n",
      "yields the latter term in the equation.\n",
      "\n",
      "10.1.2 Implementing the algorithm\n",
      "\n",
      "Below is an R program implementing Gibbs sampling for simulating the pa-\n",
      "rameters from the model. The example is the same example as in Chapter 7\n",
      "(with missing data deleted):\n",
      "\n",
      "\n",
      "\n",
      "276 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "#R program for multivariate regression\n",
      "\n",
      "x<-as.matrix(read.table(\"c:\\\\mvn_examp.dat\")[,2:10])\n",
      "\n",
      "y<-as.matrix(read.table(\"c:\\\\mvn_examp.dat\")[,11:14])\n",
      "\n",
      "d=4;k=9\n",
      "\n",
      "b<-matrix(0,(d*k)); s<-diag(d)\n",
      "\n",
      "for(i in 2:1000){\n",
      "\n",
      "#draw b from mvn\n",
      "\n",
      "vb=solve(solve(s)%x%(t(x)%*%x))\n",
      "\n",
      "mn=vb%*%(as.vector(t(x)%*%y%*%t(solve(s))))\n",
      "\n",
      "b=mn+t(rnorm((d*k),0,1)%*%chol(vb))\n",
      "\n",
      "#draw s from inverse wishart\n",
      "\n",
      "e=matrix((as.vector(y)-(diag(d)%x%x%*%b)),nrow(y),d)\n",
      "\n",
      "v=t(e)%*%e\n",
      "\n",
      "s=riwish(nrow(y)-1,v)\n",
      "\n",
      "print(c(b[1],b[10],b[19],b[28],s[1,1],s[2,2],s[3,3],s[4,4]))\n",
      "\n",
      "write(c(t(b),t(s)),file=\"c:\\\\mvn2.out\",ncolumns=52,append=T)\n",
      "\n",
      "}\n",
      "\n",
      "The program first reads the data into two matrices x and y and assigns\n",
      "the number of dimensions of the outcome (d=4) and covariate vector (k=9). It\n",
      "then creates a vector for the regression coefficients and a matrix for the error\n",
      "covariance and assigns starting values for each—all regression coefficients are\n",
      "assumed to be 0; the error covariance matrix is set to an identity.\n",
      "\n",
      "The Gibbs sampler begins first with the construction of the variance matrix\n",
      "for the parameters (vb), which is also the first part of the regression vector\n",
      "mean. The remainder of the regression vector mean is then computed (mn), and\n",
      "then the regression vector is drawn from its multivariate normal conditional\n",
      "distribution. Once the regression vector is drawn, the matrix of errors—as\n",
      "in the original model specification in Equation 10.1—is computed, and the\n",
      "cross-product matrix of the errors is computed (v). This matrix is used as\n",
      "the scale matrix S in the inverse Wishart distribution for drawing the error\n",
      "covariance matrix Σ.\n",
      "\n",
      "Table 10.1 shows the results of the multivariate regression model. If you\n",
      "compare these results with those of Table 7.2 in Chapter 7, you will see that the\n",
      "results are substantially similar. Indeed, only a few coefficients and posterior\n",
      "standard deviations change at all, and those that do change do not change\n",
      "substantially. That is, if one considers the implied posterior distributions for\n",
      "the parameters, their overlap overshadows their slight differences.\n",
      "\n",
      "As I stated above, when the X matrix is the same for all equations as it is\n",
      "in this example, there is nothing gained—beyond elegance—by using a multi-\n",
      "variate model rather than a series of univariate OLS regressions. However, if\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 277\n",
      "\n",
      "Table 10.1. Results of multivariate regression of measures of “niceness” on three\n",
      "measures of region.\n",
      "\n",
      "Outcome\n",
      "\n",
      "Variable Empathy Tolerance Selflessness Altruistic Acts\n",
      "\n",
      "Intercept 19.64(.57)*** 3.16(.15)*** 8.72(.29)*** 9.16(.79)***\n",
      "Age 0.018(.006)** −0.008(.002)*** 0.01(.003)*** −0.03(.01)***\n",
      "Male −2.41(.19)*** −0.35(.05)*** −0.87(.10)*** 0.31(.26)\n",
      "White 0.53(.25)* 0.09(.07)# 0.17(.13)# 0.001(.33)\n",
      "Yrs. Schooling 0.06(.03)* 0.004(.01) 0.07(.02)*** 0.32(.05)***\n",
      "Income ($1,000s) 0.003(.003) 0.0006(.0009) 0.001(.002) 0.02(.004)***\n",
      "Continuous South 0.65(.22)** 0.23(.06)*** 0.25(.11)* 0.40(.31)#\n",
      "South Out-Migrant 0.41(.50) 0.11(.13) −0.34(.25)# 0.001(.68)\n",
      "South In-Migrant 0.34(.35) 0.23(.09)** 0.16(.17) −0.03(.46)p\n",
      "\n",
      "σ2e 4.61 1.22 2.31 6.24\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The Bayesian p-values are based\n",
      "on one-sided tail probabilities that the parameter exceeds 0 truncated to the classical\n",
      "cut-points of #p < .1, *p < .05, **p < .01, ***p < .001.\n",
      "\n",
      "we were to exclude a variable from one (or more) equations, the multivariate\n",
      "approach will differ from the univariate approach. The reason is that the error\n",
      "covariance structure enables information from X variables in one equation to\n",
      "bear on the equation(s) in which that particular X is missing. In that case,\n",
      "the multivariate approach is more efficient.\n",
      "\n",
      "It may be a rare situation in which we choose to allow the X matrices\n",
      "to vary across equations, but there are additional reasons to use multivariate\n",
      "models. First, we may have interrelationships between endogenous variables\n",
      "in our models—that is, we may choose to allow a y variable from one equation\n",
      "have a regression effect on a y in another equation. In econometrics, these are\n",
      "called “simultaneous equation models.” In sociology and other social sciences,\n",
      "such modeling is often called “path analysis.” More general models, which\n",
      "allow for measurement error and the estimation of relationships between latent\n",
      "constructs—called structural equation models—can also be estimated within\n",
      "a Bayesian framework. Discussing such models here is beyond the scope of this\n",
      "chapter and book, but see the conclusion for suggestions for further reading.\n",
      "\n",
      "A second reason for estimating a multivariate model rather than a series of\n",
      "univariate models is if interest centers not on the model parameters themselves\n",
      "but instead on functions of the model parameters. I present such examples in\n",
      "the next sections in the context of multivariate probit models.\n",
      "\n",
      "10.2 Multivariate probit models\n",
      "\n",
      "As we discussed at the beginning of Chapter 8, it is rare in social science\n",
      "to have continuous outcomes. Sometimes, we may be interested in estimating\n",
      "models that have either multiple ordinal outcomes or a mixture of ordinal and\n",
      "\n",
      "\n",
      "\n",
      "278 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "continuous outcomes. On other occasions, we may be interested in a model\n",
      "that has a single, but nominal-level, outcome. In the former cases, we sim-\n",
      "ply need an extension of the multivariate regression model discussed in the\n",
      "previous section combined with a generalization that allows us to work with\n",
      "non-continuous outcomes, as discussed in Chapter 8—called a multivariate\n",
      "probit model. In the latter case, we need a similar, but more constrained,\n",
      "model in which the outcome variable is converted into a series of dichotomous\n",
      "outcomes that are treated as mutually exclusive in the model—called a multi-\n",
      "nomial probit model. Algorithms to estimate these two different models differ\n",
      "primarily in this mutual-exclusivity criterion.\n",
      "\n",
      "A model with multiple ordinal outcomes (including possibly multiple di-\n",
      "chotomous outcomes) is relatively straightforward to construct and estimate.\n",
      "We can use an algorithm similar to that for the multivariate linear model\n",
      "with only three alterations. First, we must include a Gibbs sampling step in\n",
      "which we sample latent data underlying the observed ordinal responses, just\n",
      "as we did in the generalized linear model (GLM) extension of the OLS regres-\n",
      "sion model. However, the latent data now must be sampled from truncated\n",
      "multivariate normal distributions rather than from truncated univariate nor-\n",
      "mal distributions, and such truncated multivariate normal simulation is not\n",
      "nearly as simple as truncated univariate normal simulation. Second, the er-\n",
      "ror covariance matrix can technically no longer be simulated from an inverse\n",
      "Wishart distribution. In order to identify the multivariate probit model, we\n",
      "must constrain the variances of the latent variables to be 1. This constraint\n",
      "makes simulating from the inverse Wishart distribution difficult or impossible;\n",
      "instead, we may use a Metropolis step to simulate the free parameters of the\n",
      "error correlation matrix. Third, as in the ordinal probit model in Chapter 8,\n",
      "when there are more than two outcome categories for a particular variable,\n",
      "the model will include free thresholds that must be estimated. In the following\n",
      "sections, I present two examples. The first example shows the basic approach\n",
      "to handling multivariate ordinal data. The second example shows how we can\n",
      "expand the model to handle “missing” data and use the results to construct\n",
      "distributions of quantities not directly estimated within the model that are a\n",
      "function of parameters from the multivariate equations.\n",
      "\n",
      "10.2.1 Model development\n",
      "\n",
      "Before we address the issues raised in the previous section, let’s consider the\n",
      "general multivariate probit model (see also Chib and Greenberg 1998). In the\n",
      "examples, I will limit the outcome to two dimensions, but there is no inherent\n",
      "need to do so. As a generic example, suppose we want to determine whether\n",
      "political views and party affiliations have changed over time (see DiMaggio,\n",
      "Evans, and Bryson 1996, who tackle a similar question of interest). The GSS\n",
      "has asked at least two relevant questions since 1972. One asks whether respon-\n",
      "dents view themselves as liberal or conservative, while the other asks respon-\n",
      "dents whether they consider themselves to be Democrats or Republicans. Both\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 279\n",
      "\n",
      "items are measured with 7-point Likert scale items and are therefore ordinal.\n",
      "For the purposes of this example, I have collapsed the variables into two three-\n",
      "category variables. Political orientation is measured as liberal, moderate, or\n",
      "conservative, and political affiliation is measured as Democrat, Independent,2\n",
      "\n",
      "or Republican. Thus, the two-dimensional outcome, rather than being bivari-\n",
      "ate normal, can be viewed as a three-by-three contingency table. Table 10.2\n",
      "shows the distribution of these data.\n",
      "\n",
      "As the table suggests, there appears to be some sort of relationship between\n",
      "political orientation and party affiliation. Overall, 44% of the sample falls in\n",
      "the diagonal cells, and another 44% falls just off the diagonal. A chi-square\n",
      "test confirms that the two variables are associated (χ2 = 3, 094, 4 df), and\n",
      "the Pearson correlation between the two variables is .28, a low-to-moderate\n",
      "positive correlation.3\n",
      "\n",
      "Table 10.2. Political orientation and party affiliation.\n",
      "\n",
      "Political Orientation\n",
      "\n",
      "Party Affiliation Liberal Moderate Conservative Total\n",
      "\n",
      "Democrat 5,133(14%) 5,697(15%) 3,357(9%) 14,187(38%)\n",
      "Independent 3,608(10%) 5,460(15%) 3,805(10%) 12,873(35%)\n",
      "Republican 1,337(4%) 3,159(9%) 5,472(15%) 9,968(27%)\n",
      "Total 10,078(27%) 14,316(39%) 12,634(34%) n = 37, 028\n",
      "\n",
      "Note: The data are from the 1972-2004 General Social Surveys. All years are covered,\n",
      "except 1979, 1981-2, 1992, 1995, 1997, 1999, 2001, and 2003. Percentages are of total\n",
      "sample and may not sum to 100 due to rounding.\n",
      "\n",
      "Just as we assumed a binomial likelihood function for the observed data\n",
      "in the dichotomous probit model and a multinomial likelihood function for\n",
      "the observed data in the ordinal probit model in Chapter 8, we can assume a\n",
      "multinomial likelihood function for the observed multivariate data. Thus, the\n",
      "likelihood function for the data is:\n",
      "\n",
      "2 This category includes individuals with leanings one way or the other who define\n",
      "themselves as independents.\n",
      "\n",
      "3 The Pearson correlation is known to be an incorrect measure of association be-\n",
      "tween ordinal variables. Instead, the polychoric correlation should be used. The\n",
      "polychoric correlation is a precursor to a multivariate probit model—it is the\n",
      "correlation between the “errors” in the multivariate probit model if there are no\n",
      "predictors. See the exercises. Also see Olsson 1979, Olsson, Drasgow, and Dorans\n",
      "1982, and Poon and Lee 1987.\n",
      "\n",
      "\n",
      "\n",
      "280 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "L(p|Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "(\n",
      "R∏\n",
      "\n",
      "r=1\n",
      "\n",
      "C∏\n",
      "c=1\n",
      "\n",
      "p\n",
      "yirc\n",
      "irc\n",
      "\n",
      ")\n",
      ", (10.9)\n",
      "\n",
      "where yirc is 1 if the ith individual’s response falls in the (r, c)th cell of the\n",
      "contingency table and is 0 otherwise. More generally, if there are K outcome\n",
      "variables Y (1) . . . Y (K), each of which is ordinal with c(k) categories, the\n",
      "likelihood function is:\n",
      "\n",
      "L(P |Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "\n",
      " c(1)∏\n",
      "\n",
      "a(1)=1\n",
      "\n",
      "c(2)∏\n",
      "a(2)=1\n",
      "\n",
      ". . .\n",
      "\n",
      "c(K)∏\n",
      "a(k)=1\n",
      "\n",
      "p\n",
      "yi,a(1)a(2)...a(K)\n",
      "i,a(1)a(2)...a(K)\n",
      "\n",
      "\n",
      " , (10.10)\n",
      "\n",
      "where yi,a(1)...a(K) = 1 if the ith individual’s response falls in the a(1) . . . a(K)th\n",
      "\n",
      "cell of the multinomial (and is 0 otherwise). The parenthetical “subscripting”\n",
      "is used to represent the dimensions of the outcome as well as parameters that\n",
      "vary by outcome/equation. In our current example, K = 2, and so c(1) = 3 is\n",
      "the number of categories in the political orientation variable, c(2) = 3 is the\n",
      "number of categories in the party affiliation variable, and a(1) and a(2) are in-\n",
      "dex variables. In this example, a(1) indexes the three outcomes of the political\n",
      "orientation variable, and a(2) indexes the three outcomes of the party affilia-\n",
      "tion variable. Thus, each individual’s contribution to the likelihood function is\n",
      "the product over the nine cells of the outcome contingency table with the re-\n",
      "spondent’s yi,a(1)a(2) outcome being an indicator for whether the respondent’s\n",
      "response falls in the a(1)a(2)th cell. So, Equation 10.9 (or Equation 10.10) can\n",
      "be expanded as:\n",
      "\n",
      "L(p|Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "(\n",
      "p\n",
      "\n",
      "yi,11\n",
      "i,11 · p\n",
      "\n",
      "yi,12\n",
      "i,12 · p\n",
      "\n",
      "yi,13\n",
      "i,13 · p\n",
      "\n",
      "yi,21\n",
      "i,21 · p\n",
      "\n",
      "yi,22\n",
      "i,22 · p\n",
      "\n",
      "yi,23\n",
      "i,23 · p\n",
      "\n",
      "yi,31\n",
      "i,31 · p\n",
      "\n",
      "yi,32\n",
      "i,32 · p\n",
      "\n",
      "yi,33\n",
      "i,33\n",
      "\n",
      ")\n",
      ".\n",
      "\n",
      "(10.11)\n",
      "Notice that each individual ultimately only contributes one component to the\n",
      "likelihood function—the pi,a(1)a(2) for which yi,a(1)a(2) = 1. The pi,a(1)a(2)\n",
      "are the cell probabilities, which depend on an individual’s covariate pro-\n",
      "file. The cell probabilities in the dichotomous and ordinal probit models\n",
      "in Chapter 8 were integrals of the univariate normal distribution, and the\n",
      "probabilities were linked to the covariates via the “link” function. For ex-\n",
      "ample, p(yi = 1) = Φ(XTi β, 1) in the dichotomous probit model and\n",
      "p(yi = k) = Φ(XTi β, 1, τk, τk+1) in the ordinal probit model, where τk and\n",
      "τk+1 are the thresholds that bound the latent continuous propensity to place\n",
      "the individual in category k of the observed ordinal outcome. We can take\n",
      "a similar latent variable approach in the multivariate probit model; in the\n",
      "multivariate probit model, the integrals are multivariate, such that:\n",
      "\n",
      "p(yi,a(1)a(2)...a(K) = 1) = ΦK\n",
      "(\n",
      "XTi β, Σ, τL, τU\n",
      "\n",
      ")\n",
      ". (10.12)\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 281\n",
      "\n",
      "Here, p(yi,a(1)a(2)...a(K) = 1) is the probability the ith individual’s response\n",
      "falls in the a(1)a(2) . . . a(K)th cell of the multinomial, ΦK() is the integral of\n",
      "the K-dimensional multivariate normal pdf, XTi β is the predicted multivari-\n",
      "ate (K × 1) z score for the ith individual, Σ is the error covariance matrix\n",
      "(diagonal elements are constrained to 1; off-diagonal elements are estimated),\n",
      "and τL and τU are K-length vectors of thresholds that bound the latent con-\n",
      "tinuous response (z) in multiple dimensions. That is, τ(1)a(1) is the lower\n",
      "threshold, and τ(1)a(1)+1 is the upper threshold, in the first dimension of the\n",
      "outcome bounding a response observed to be in category a(1). In our current\n",
      "example, the political orientation and party affiliation variables each have four\n",
      "thresholds, bounding the three possible outcome categories in each dimension.\n",
      "τ(1)1 = −∞, τ(1)2 = 0, τ(1)3 is estimated, and τ(1)4 = ∞. The same result\n",
      "is true for the vector τ(2). Thus, the probability that an individual falls in the\n",
      "(2,3) cell of the multinomial in this example is the integral of the bivariate\n",
      "normal with mean M = XTi β (a K×1 vector of means for the i\n",
      "\n",
      "th individual)\n",
      "and covariance matrix Σ:\n",
      "\n",
      "p(yi = (2, 3)) = Φ2(M,Σ, τ(1)2, τ(1)3; τ(2)3, τ(2)4).\n",
      "\n",
      "Figure 10.1 presents a graphic depiction of this integral. The contours\n",
      "represent the bivariate normal distribution for the latent propensities Z (or\n",
      "Y ∗) thought to underlie the multivariate ordinal response. The vertical and\n",
      "horizontal dotted lines are the thresholds that partition the latent distribution\n",
      "into the observed 3× 3 contingency table of ordinal responses, and the dark-\n",
      "outlined region of the contour plot is the probability that an individual falls in\n",
      "cell (2,3) of the observed contingency table—it is the integral of the bivariate\n",
      "normal distribution between lower and upper thresholds of the cell in the two\n",
      "dimensions.\n",
      "\n",
      "Multivariate normal integration is difficult and costly (see Drezner 1992\n",
      "and Schervish 1984, for example). Thus, instead, we can bring the latent data\n",
      "(Z) into the modeling strategy just as we developed an alternative represen-\n",
      "tation for the dichotomous and ordinal probit model in Chapter 8. In the\n",
      "univariate probit models discussed in Chapter 8, the latent traits were simu-\n",
      "lated from truncated normal distributions based on the observed outcome. In\n",
      "the multivariate probit model, the latent traits are simulated from truncated\n",
      "multivariate normal distributions. Once these latent data are simulated, the\n",
      "remainder of an MCMC algorithm for simulating the multivariate probit pa-\n",
      "rameters is virtually identical to the one we developed for the multivariate\n",
      "regression model in the previous sections, only requiring an additional step\n",
      "to simulate the free thresholds that categorize the continuous latent data into\n",
      "the observed ordinal bins:\n",
      "\n",
      "1. Select starting values for β and Σ, with Σii = 1,∀i.\n",
      "\n",
      "\n",
      "\n",
      "282 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "τ(1)1=− ∞ τ(1)2=0 τ(1)3=? τ(1)4=∞\n",
      "τ(2)1=− ∞\n",
      "\n",
      "τ(2)2=0\n",
      "\n",
      "τ(2)3=?\n",
      "\n",
      "τ(2)4=∞\n",
      "p(y=[2,3])=1\n",
      "\n",
      "Z1\n",
      "\n",
      "Z2\n",
      "\n",
      "Fig. 10.1. Depiction of a bivariate outcome space for continuous latent variables\n",
      "Z1 and Z2: The observed variables Y1 and Y2 are formed by the imposition of the\n",
      "vectors of thresholds in each dimension; and the darkened area is the probability\n",
      "that an individual’s response falls in the [2, 3] cell, conditional on the distribution\n",
      "for Z.\n",
      "\n",
      "2. Simulate Z|Y, X, β,Σ, τ ∼ TMV N(Xβ, Σ).\n",
      "\n",
      "3. Simulate τ(k)c|Z(k) ∼ U(max Z(k)c,minZ(k)c+1),∀k, c.\n",
      "\n",
      "4. Simulate β|Z,X, Σ ∼ MV N((XT Ω−1X)−1(XT Ω−1Z) , (XT Ω−1X)−1).\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 283\n",
      "\n",
      "5. Simulate Σ|Z, β, X ∼ IW (S, n), where S =\n",
      "∑n\n",
      "\n",
      "i=1(Z −Xiβ)(Z −Xiβ)\n",
      "T ,\n",
      "\n",
      "subject to the constraint the Σii = 1,∀i.\n",
      "\n",
      "6. Return to Step 2.\n",
      "\n",
      "The first step, finding starting values, is straightforward. I generally start\n",
      "all regression parameters at 0 and the error covariance matrix Σ as an identity\n",
      "matrix. The fourth step—simulation of the regression coefficients—requires\n",
      "very little discussion. Once the latent data Z replace the observed ordinal\n",
      "data Y and an appropriate Σ matrix is available, the conditional distribution\n",
      "for the regression parameters is identical to that of the multivariate regression\n",
      "model presented in the previous section. The second, third, and fifth steps,\n",
      "on the other hand, require some discussion.\n",
      "\n",
      "10.2.2 Step 2: Simulating draws from truncated multivariate\n",
      "normal distributions\n",
      "\n",
      "Simulating draws from truncated multivariate normal distributions is sub-\n",
      "stantially more complicated than simulating draws from truncated univariate\n",
      "normal distributions. One method for simulating such draws is the naive ap-\n",
      "proach we discussed in Chapter 8: We simulate from untruncated normal\n",
      "distributions until we obtain a draw from the appropriate region. For exam-\n",
      "ple, using the current example, if we wished to simulate a latent score for\n",
      "a person with an observed y = (2, 3) response, we would simulate from the\n",
      "entire bivariate normal distribution exemplified in Figure 10.1 until we ob-\n",
      "tained a draw that fell in the dark-outlined region. It is easy to see that this\n",
      "naive approach is extremely inefficient, and becomes increasingly so as the di-\n",
      "mensionality of the problem increases and as the number of categories in the\n",
      "ordinal variables increases. Basically, anything that reduces the ratio of the\n",
      "area under the multivariate normal curve in one cell relative to the total area\n",
      "under the multivariate normal curve reduces the naive approach’s efficiency.\n",
      "\n",
      "We have already discussed how to simulate from truncated univariate nor-\n",
      "mal distributions, and so, if we can break simulation from our truncated mul-\n",
      "tivariate normal distribution into a series of truncated univariate normal dis-\n",
      "tribution simulations, it seems our problem is solved. One way that we can\n",
      "break the multivariate simulation problem into a series of univariate ones is to\n",
      "decompose the joint density function f(x1, x2, . . . , xK−1, xK) into the prod-\n",
      "uct of conditional distributions (as we did to construct hierarchical models in\n",
      "Chapter 9) using the conditional probability rule that f(A,B) = f(A|B)f(B).\n",
      "We can carry this decomposition out beyond two variables as:\n",
      "\n",
      "f(x1, x2, . . . , xK) = f(xK |xK−1, . . . , x1) . . . f(x3|x1, x2)f(x2|x1)f(x1).\n",
      "(10.13)\n",
      "\n",
      "That this decomposition is correct can be seen by starting from the right end\n",
      "and recognizing that the first marginal times the first conditional leads to\n",
      "\n",
      "\n",
      "\n",
      "284 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "a joint distribution for two variables. This joint distribution is then a new\n",
      "marginal distribution, which, when multiplied by the next conditional distri-\n",
      "bution, yields a joint distribution for three variables, and so on:\n",
      "\n",
      "f(x1, x2, . . . , xK) = . . . f(x4|x1, x2, x3) f(x3|x1, x2) f(x2|x1) f(x1)\n",
      "\n",
      "f(x1, x2)\n",
      "\n",
      "f(x1, x2, x3)\n",
      "\n",
      "f(x1, x2, x3, x4)\n",
      "...\n",
      "\n",
      "Thus, we can first draw x1 from its appropriate univariate normal distribu-\n",
      "tion. Then, conditional on this draw, we can draw x2 from f(x2|x1). Then,\n",
      "conditional on these draws, we can draw x3 from f(x3|x1, x2), and so on.\n",
      "Earlier, we discussed that the conditional distribution for one variable (e.g.,\n",
      "x2) from a standard bivariate normal distribution given the other (e.g., x1)\n",
      "is univariate normal with a mean of ρx1 and variance 1 − ρ2. More gener-\n",
      "ally, it is known that the conditional distribution for a single variable in a\n",
      "multivariate normal distribution is itself normal. If Z ∼ MV N(µ,Σ), then\n",
      "(Za|µ(−a) = M) ∼ N(µ∗, Σ∗), where:\n",
      "\n",
      "µ∗ = µa + Σa.Σ\n",
      "−1\n",
      "(−a)\n",
      "\n",
      "(\n",
      "µ(−a) −M\n",
      "\n",
      ")\n",
      "(10.14)\n",
      "\n",
      "Σ∗ = Σaa −Σa.Σ−1(−a)Σ.a. (10.15)\n",
      "\n",
      "In these expressions, Σ(−a) is the multivariate covariance matrix obtained by\n",
      "omitting row and column a, Σa. is the ath row of Σ (similarly, Σ.a is the ath\n",
      "\n",
      "column of Σ), and M is the current value of the normal draws for the other\n",
      "(i.e., −a) dimensions.\n",
      "\n",
      "Thus, sampling from a multivariate normal distribution involves simply\n",
      "sampling the first variable from its univariate marginal distribution; then sam-\n",
      "pling the second variable from its univariate conditional distribution, which\n",
      "depends only on the first variable, then sampling the third variable from its\n",
      "univariate conditional distribution, which depends only on the first and sec-\n",
      "ond variables; and so on. Below is an R subroutine that samples 10,000 draws\n",
      "from a five-dimensional normal distribution with mean vector µT = [1 2 3 4 5]\n",
      "and covariance matrix with all covariances equal to .5 and the variances\n",
      "σ2ii = [5 4 3 2 1]. The sampling begins with the last element (x5) and sam-\n",
      "ples from the appropriate conditional distribution moving backward to x1.\n",
      "Thus, as the fourth element is being sampled from its conditional distribu-\n",
      "tion, the relevant covariance matrix is the 2 × 2 matrix for variables x4 and\n",
      "x5. As the third element is being sampled, the relevant covariance matrix is\n",
      "the 3× 3 covariance matrix for variables x3, x4, and x5, etc.:\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 285\n",
      "\n",
      "d=5\n",
      "\n",
      "x<-matrix(NA,10000,d)\n",
      "\n",
      "mu=matrix(c(1,2,3,4,5),d)\n",
      "\n",
      "sig=matrix(.5,d,d)\n",
      "\n",
      "sig[1,1]=5; sig[2,2]=4; sig[3,3]=3; sig[4,4]=2; sig[5,5]=1\n",
      "\n",
      "for(i in 1:10000)\n",
      "\n",
      "{\n",
      "\n",
      "#simulate 5th element\n",
      "\n",
      "x[i,d]=rnorm(1,mu[d],sqrt(sig[d,d]))\n",
      "\n",
      "#simulate 4th, 3rd...\n",
      "\n",
      "for(j in d:2)\n",
      "\n",
      "{\n",
      "\n",
      "m=mu[j-1] +\n",
      "\n",
      "sig[(j-1),(j:d)]%*%solve(sig[j:d,j:d])%*%(x[i,j:d]-mu[j:d])\n",
      "\n",
      "s=sig[j-1,j-1] -\n",
      "\n",
      "sig[(j-1),(j:d)]%*%(solve(sig[j:d,j:d]))%*%sig[(j:d),(j-1)]\n",
      "\n",
      "x[i,j-1]=rnorm(1,m,sqrt(s))\n",
      "\n",
      "}\n",
      "\n",
      "print(c(i,x[i,]))\n",
      "\n",
      "}\n",
      "\n",
      "The resulting mean vector of the 10,000 draws was:\n",
      "\n",
      "x̄T = [1.00 1.97 3.04 4.00 5.00],\n",
      "\n",
      "and the covariance matrix was:\n",
      "\n",
      "\n",
      "4.99\n",
      ".47 4.01\n",
      ".44 .51 3.00\n",
      ".48 .47 .53 2.01\n",
      ".47 .48 .52 .51 1.01\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "Based on these results, it seems that a strategy for sampling from trun-\n",
      "cated multivariate normal distributions might simply involve applying this\n",
      "approach but ensuring that, for each univariate draw, we enforce the uni-\n",
      "variate truncation requirements as we did in Chapter 8. Below is a simple\n",
      "R program for performing such simulation on a truncated standard bivariate\n",
      "normal distribution with corrrelation/covariance .5, with the point of trunca-\n",
      "tion being 0 in both dimensions so that draws must come from above 0 in each\n",
      "dimension. The program simulates 2,000 draws from this distribution, using\n",
      "both naive simulation and using the conditional decomposition approach:\n",
      "\n",
      "covmat=diag(2)\n",
      "\n",
      "covmat[1,2]=covmat[2,1]=.5\n",
      "\n",
      "\n",
      "\n",
      "286 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "z=matrix(0,2000,2)\n",
      "\n",
      "q=matrix(0,2000,2)\n",
      "\n",
      "count=0\n",
      "\n",
      "for(i in 1:2000)\n",
      "\n",
      "{\n",
      "\n",
      "#naive simulation\n",
      "\n",
      "z[i,]=0\n",
      "\n",
      "while(z[i,1]<=0 | z[i,2]<=0)\n",
      "\n",
      "{count=count+1\n",
      "\n",
      "z[i,]=rnorm(2,0,1)%*%(chol(covmat))\n",
      "\n",
      "}\n",
      "\n",
      "#conditional simulation based on decomposition\n",
      "\n",
      "q[i,1]=qnorm(runif(1,min=.5,max=1),0,1)\n",
      "\n",
      "mm=covmat[1,2]*q[i,1]\n",
      "\n",
      "ss=1-.5^2\n",
      "\n",
      "q[i,2]=qnorm(runif(1,min=pnorm(0,mm,sqrt(ss)),max=1),mm,sqrt(ss))\n",
      "\n",
      "}\n",
      "\n",
      "Figure 10.2 shows some results of the naive and conditional/decomposition\n",
      "approach. The upper plots show the contours of the BV N(0, 1, ρ = .5) dis-\n",
      "tribution, with the two sets of simulated values superimposed. The upper\n",
      "left plot shows the results of the naive simulation approach—which required\n",
      "5,854 draws to obtain 2,000 samples in the appropriate region—and the up-\n",
      "per right plot shows the results of the conditional decomposition approach.\n",
      "Although both methods appear to sample thoroughly throughout the desired\n",
      "region, the conditional approach appears not to have sampled as thoroughly\n",
      "from the x1 dimension as from the x2 dimension (notice that the contours are\n",
      "more visible close to the x1 axis under the conditional approach). This con-\n",
      "clusion is substantiated in the bottom figures, as the histograms for x1 are not\n",
      "even approximately identical. The histograms show that the sampled values\n",
      "for x1 obtained using the conditional approach seem to be overly clustered\n",
      "close to 0. This result is not an artifact of simulating only a few thousand\n",
      "samples. I reconducted the simulation, drawing 200,000 samples using both\n",
      "the naive and the conditional/decomposition approaches. The mean for x1 of\n",
      "that simulation was .90 under the naive simulation approach but .80 under\n",
      "the conditional approach, and the variance of x1 was .40 under the naive ap-\n",
      "proach and .36 under the conditional approach. The results for x2 were not\n",
      "as different. The mean for x2 was .90 and .87 under the naive and conditional\n",
      "approaches (respectively), and the variances were .40 and .39, respectively.\n",
      "\n",
      "Why does this conditional decomposition approach appear to work for x2,\n",
      "but not for x1? The answer is that the marginal distribution for x1 ultimately\n",
      "depends on the marginal distribution for x2, and so our first draw—for x1—is\n",
      "using the wrong marginal distribution. Figure 10.3 clarifies the problem. If we\n",
      "simply simulate from a truncated univariate normal distribution for x1 subject\n",
      "only to its truncation constraints, we end up simulating from the entire right\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 287\n",
      "\n",
      "Naive Simulation\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "Conditional Simulation\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x1\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Naive\n",
      "Conditional\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x2\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "Naive\n",
      "Conditional\n",
      "\n",
      "Fig. 10.2. Comparison of truncated bivariate normal simulation using naive simula-\n",
      "tion and conditional/decomposition simulation: Upper plots show the true contours\n",
      "(solid lines) of the bivariate normal distribution with sampled values superimposed\n",
      "(dots); and lower plots show histograms of the values sampled from the two dimen-\n",
      "sions under the two alternative sampling approaches.\n",
      "\n",
      "side of the bivariate distribution. However, given that x2 is also truncated,\n",
      "the lower right “wedge” should be omitted from the mass of the marginal for\n",
      "x1. Leaving this mass in the marginal for x1 yields too much mass close to\n",
      "0, which leads to the oversampling of values close to 0 and, consequently, a\n",
      "mean and variance that are too small for x1.\n",
      "\n",
      "Once we have obtained a value for x1—which at least is in the right region\n",
      "(above 0)—the draw for x2|x1 (∼ TN(ρx1, 1−ρ2)) is from the approximately\n",
      "correct distribution. The result is that the marginal distribution for x2 ends up\n",
      "\n",
      "\n",
      "\n",
      "288 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "being close to the correct distribution, only slightly off due to the oversampling\n",
      "of certain values of x1.\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "f(x1)\n",
      "when x2 truncation is ignored\n",
      "\n",
      "inappropriately\n",
      "included\n",
      "\n",
      "Fig. 10.3. Truncated bivariate normal distribution: Marginal distribution for x1\n",
      "when truncation of x2 is ignored.\n",
      "\n",
      "If the draw from x2|x1 is close to correct, it seems that we could then\n",
      "use this draw to draw a new x1; that is, we could then draw x1|x2 ∼\n",
      "TN(ρx2, 1 − ρ2), to obtain a better draw for x1. Then, we could use this\n",
      "new, better draw for x1 to produce a better draw for x2. Indeed, we can do\n",
      "this, and what we have now done is to complete a pair of iterations of a Gibbs\n",
      "sampler for the truncated multivariate normal distribution! Recall that Gibbs\n",
      "sampling involves iteratively sampling from the conditional distributions for\n",
      "one random variable, given all others. Thus, simulation from a K-dimensional\n",
      "normal distribution would involve:\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 289\n",
      "\n",
      "1. Simulate x1|x2, x3, x4, . . . , xK−1, xK .\n",
      "2. Simulate x2|x1, x3, x4, . . . , xK−1, xK .\n",
      "3. Simulate x3|x1, x2, x4, . . . , xK−1, xK .\n",
      "...\n",
      "\n",
      "...\n",
      "K − 1. Simulate xK−1|x1, x2, x3, . . . , xK−2, xK .\n",
      "\n",
      "K. Simulate xK |x1, x2, x3, . . . , xK−2, xK−1.\n",
      "\n",
      "If we follow this Gibbs sampling strategy, simulating each variable from its\n",
      "conditional distribution subject to its univariate truncation constraints, the\n",
      "Gibbs sampler will produce a sample from the truncated multivariate normal\n",
      "distribution within a few iterations. To perform the Gibbs sampling, each\n",
      "conditional distribution can be derived using Equations 10.14 and 10.15.\n",
      "\n",
      "Figure 10.4 shows the results of 2,000 samples if we follow this Gibbs\n",
      "sampling strategy for only two iterations per sample.4 As the histograms show,\n",
      "the distributions for both x1 and x2 now closely match those obtained from\n",
      "the less efficient naive sampling approach. The means for x1 and x2 under\n",
      "this approach were .89 and .90, respectively—both close to the means of .90\n",
      "and .90 obtained via naive sampling—and the variances were .40 and .40,\n",
      "respectively—both almost indistinguishable from the variances of .40 and .40\n",
      "obtained via naive sampling (see Robert 1995 who presents a more detailed\n",
      "and theoretical discussion of this approach).\n",
      "\n",
      "In conclusion, although simulating from truncated multivariate normal dis-\n",
      "tributions is more complex than simulating from truncated univariate normal\n",
      "distributions, we can perform such simulation using a Gibbs sampler with\n",
      "few iterations, suggesting that we can simulate our latent data in a multi-\n",
      "variate probit model algorithm fairly efficiently—a Gibbs-sampler-within-a-\n",
      "Gibbs-sampler approach!\n",
      "\n",
      "10.2.3 Step 3: Simulation of thresholds in the multivariate probit\n",
      "model\n",
      "\n",
      "As we discussed in Chapter 8, the conditional distributions for the free thresh-\n",
      "olds are straightforward to derive: The conditionals in the univariate probit\n",
      "model are simply uniform on the interval between the largest latent score\n",
      "simulated for a person in the category below the threshold of interest and the\n",
      "smallest latent score simulated for a person in the category above the thresh-\n",
      "old of interest. The thresholds in the multivariate probit are equally easy to\n",
      "derive. For the sake of simplicity of notation, let’s assume a bivariate probit\n",
      "model with uniform prior distributon for all parameters. The latent variable\n",
      "representation of the posterior distribution then is:\n",
      "\n",
      "p(τ, β,Σ, Z|X, Y ) ∝\n",
      "\n",
      "4 The last three lines of the algorithm, in which mm and ss and q[i,?] are derived,\n",
      "are repeated twice: q[i,1] is drawn; then q[i,2] is drawn again.\n",
      "\n",
      "\n",
      "\n",
      "290 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "Naive Simulation\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "Conditional Simulation\n",
      "\n",
      "x1\n",
      "\n",
      "x 2\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "3\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "● ●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "●\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x1\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Naive\n",
      "Conditional\n",
      "\n",
      "0 1 2 3 4 5\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "x2\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "Naive\n",
      "Conditional\n",
      "\n",
      "Fig. 10.4. Comparison of truncated bivariate normal distribution simulation using\n",
      "naive simulation and two iterations of Gibbs sampling: Upper plots show the true\n",
      "contours (solid lines) of the bivariate normal distribution with sampled values su-\n",
      "perimposed (dots); and lower plots show histograms of the values sampled from the\n",
      "two dimensions under the two alternative approaches to sampling.\n",
      "\n",
      "n∏\n",
      "i=1\n",
      "\n",
      "(\n",
      "R∑\n",
      "\n",
      "r=1\n",
      "\n",
      "C∑\n",
      "c=1\n",
      "\n",
      "φ2(Zi −XTi β, Σ)I(τr < zi1 < τr+1)I(τc < zi2 < τc+1)\n",
      "\n",
      ")\n",
      ",\n",
      "\n",
      "where Zi is the two-dimensional latent variable for individual i, with zi1 and\n",
      "zi2 representing the specific elements, φ2(a, b) is the bivariate normal density\n",
      "function with mean a and covariance matrix b, τr is the rth threshold in the\n",
      "first dimension, and τc is the cth threshold in the second dimension. This\n",
      "posterior can be expanded as in Equation 10.11, but as in that example,\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 291\n",
      "\n",
      "each individual only contributes a single term to the posterior, namely the\n",
      "component for which the indicator functions both take a value of 1.\n",
      "\n",
      "If we are considering a particular threshold, say τk, all terms in the poste-\n",
      "rior that do not involve τk can be removed as proportionality constants. Thus,\n",
      "the φ() components, and all other indicator functions that do not include τk,\n",
      "can be eliminated, leaving us with a string of products of indicators like:\n",
      "\n",
      "p(τk|θ) ∝ I(τk−1 < Zk−1 < τk)× I(τk < Zk < τk+1),\n",
      "\n",
      "where the first indicator function is repeated for all Z : τk−1 < Z < τk,\n",
      "and the second indicator function is repeated for all Z : τk < Z < τk+1. We\n",
      "know that all of the indicator functions are “true,” and so, we can ultimately\n",
      "drop the indicator function itself. If τk > Zk−1 ∀Zk−1, then τk > max(Zk−1).\n",
      "Similarly, if τk < Zk ∀Zk, then τk < min(Zk). The shape of the distribution\n",
      "over this interval is proportional to a constant and is therefore uniform. Thus,\n",
      "in the multivariate probit model, the distributions for the free thresholds are\n",
      "uniform just as they were in the univariate probit model. Furthermore, the\n",
      "conditional distributions for the thresholds in one dimension do not depend\n",
      "on the thresholds in the other dimension, suggesting that we can draw the\n",
      "thresholds one at a time from independent uniform distributions within and\n",
      "across equations.\n",
      "\n",
      "Although this finding leads to a simple approach to simulating the thresh-\n",
      "olds, there is a substantial drawback to implementing it. When there are large\n",
      "numbers of individuals in each category, the difference between the maximum\n",
      "latent score in one category and the minimum latent score in the next tends\n",
      "to be small and leads to very slow movement of the thresholds in the Gibbs\n",
      "sampler. In the current example, we have a sample size of well over 30,000\n",
      "individuals, which is guaranteed to produce slow convergence and mixing of\n",
      "the thresholds. In the probit models discussed in Chapter 8, slow convergence\n",
      "and mixing of the thresholds was not particularly problematic, because the\n",
      "sample size was relatively small and the dimensionality of the outcome was\n",
      "one, and we could rapidly run tens of thousands of iterations of the Gibbs\n",
      "sampler. In a multivariate probit model with a large sample size, on the other\n",
      "hand, it may simply be too costly to run the algorithm for an extended num-\n",
      "ber of iterations. For example, I ran the algorithm for the multivariate probit\n",
      "model for 100,000 iterations sampling the threshold parameters from uniform\n",
      "distributions. The algorithm took 10 hours to run, and worse, the threshold\n",
      "parameters did not converge and therefore certainly did not thoroughly mix.\n",
      "Worse still, because of the slow movement of the thresholds, autocorrelation\n",
      "function plots showed unacceptably high autocorrelations even up to 25-30\n",
      "lags. Clearly, we need an alternative approach to sampling the thresholds in\n",
      "the multivariate probit model—at least when the sample size is so large.\n",
      "\n",
      "In the current example, we have two free thresholds (one in each dimen-\n",
      "sion) that must be estimated. Figure 10.5 shows trace plots of the threshold\n",
      "parameters for three runs of the Gibbs sampler for the multivariate probit\n",
      "\n",
      "\n",
      "\n",
      "292 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "model (to be discussed in the next section), as well as autocorrelation func-\n",
      "tion plots for the threshold parameter in the first equation/dimension of the\n",
      "outcome (τ13). The first two plots in the first column of the 3-by-3 figure\n",
      "shows the trace plots for the two threshold parameters from a 10,000 iter-\n",
      "ation run of the Gibbs sampler, with every tenth sample saved. From the\n",
      "figures, it is not clear that the threshold parameters’ samples have mixed\n",
      "well, nor is it clear whether the algorithm has even converged. The bottom\n",
      "plot in the column is the autocorrrelation function plot for τ13. Even after\n",
      "thinning the Gibbs samples to every tenth sample, the threshold parameter\n",
      "shows extremely high autocorrelation. The second column of the figure shows\n",
      "the results of extending the algorithm another 90,000 iterations (still saving\n",
      "every tenth). Once again, it is not clear that the algorithm has mixed well\n",
      "nor converged. Furthermore, the autocorrelation plot at the bottom of the\n",
      "column shows extremely high autocorrelation. The third column, in contrast,\n",
      "shows the results for the thresholds when Cowles’ (1996) algorithm is used to\n",
      "sample them. This algorithm was run for 6,000 iterations, with the samples\n",
      "thinned to every fifth iteration. The upper two plots show rapid convergence\n",
      "and thorough mixing. Furthermore, the autocorrelation function plot shows\n",
      "little autocorrrelation beyond one lag.\n",
      "\n",
      "What is Cowles’ algorithm? Cowles’ algorithm is a Metropolis–Hastings\n",
      "(MH) step that replaces the Gibbs sampling step for simulating the thresh-\n",
      "olds from uniform distributions (Cowles 1996). Rather than simulating the\n",
      "thresholds from narrow uniform distributions, Cowles’ algorithm generates\n",
      "candidate thresholds over the entire interval between adjacent thresholds and\n",
      "then uses the standard MH accept/reject criterion for determining whether to\n",
      "accept the candidate. The result is that the thresholds, even though all candi-\n",
      "dates are not accepted, make larger “jumps” when they move, leading to more\n",
      "rapid convergence, faster mixing, and less autocorrelation between successive\n",
      "samples of the thresholds. I describe Cowles’ algorithm for one dimension—\n",
      "i.e., one vector of thresholds—but the algorithm is extendable to multivariate\n",
      "models, as I show in Section 10.2.5.\n",
      "\n",
      "Consider a vector of thresholds, τ for a one-dimensional ordinal variable\n",
      "with K categories:\n",
      "\n",
      "τ = [(τ1 = −∞) (τ2 = 0) . . . (τK−1) (τK) (τK+1 = ∞)].\n",
      "\n",
      "Cowles’ algorithm begins by simulating candidate parameters for each free\n",
      "element of τ from normal distributions centered over the current value of\n",
      "each τ truncated at the current values of the thresholds below and above the\n",
      "threshold being simulated:\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 293\n",
      "\n",
      "0 200 600 1000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".9\n",
      "4\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "8\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 1\n",
      "3\n",
      "\n",
      "0 4000 8000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".9\n",
      "4\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "8\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 1\n",
      "3\n",
      "\n",
      "0 200 600 1000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".9\n",
      "4\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "8\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 1\n",
      "3\n",
      "\n",
      "0 200 600 1000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "9\n",
      "1\n",
      "\n",
      ".0\n",
      "2\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "5\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 2\n",
      "3\n",
      "\n",
      "0 4000 8000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "9\n",
      "1\n",
      "\n",
      ".0\n",
      "2\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "5\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 2\n",
      "3\n",
      "\n",
      "0 200 600 1000\n",
      "\n",
      "0\n",
      ".9\n",
      "\n",
      "9\n",
      "1\n",
      "\n",
      ".0\n",
      "2\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "5\n",
      "\n",
      "Iteration\n",
      "\n",
      "τ 2\n",
      "3\n",
      "\n",
      "0 5 15 25\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "τ13\n",
      "\n",
      "0 10 20 30 40\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "τ13\n",
      "\n",
      "0 5 15 25\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Lag\n",
      "\n",
      "A\n",
      "C\n",
      "\n",
      "F\n",
      "\n",
      "τ13\n",
      "\n",
      "Fig. 10.5. Gibbs sampling versus Cowles’ algorithm for sampling threshold param-\n",
      "eters.\n",
      "\n",
      "τ c3 ∼ N(τ3 , σ\n",
      "2, τ2 = 0, τ4)\n",
      "\n",
      "τ c4 ∼ N(τ4 , σ\n",
      "2, τ c3 , τ5)\n",
      "\n",
      "τ c5 ∼ N(τ5 , σ\n",
      "2, τ c4 , τ6)\n",
      "\n",
      "...\n",
      "\n",
      "τ cK−1 ∼ N(τK−1 , σ\n",
      "2, τ cK−2, τK)\n",
      "\n",
      "τ cK ∼ N(τK , σ\n",
      "2, τ cK−1, τK+1 = ∞).\n",
      "\n",
      "Here, N(a, b, c, d) is the truncated normal distribution with mean a, variance\n",
      "b, and lower and upper truncation points of c and d, respectively. Thus, the\n",
      "\n",
      "\n",
      "\n",
      "294 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "generation of candidate values for the vector τ is sequential, with the lower\n",
      "truncation point for τk being determined by the candidate threshold below\n",
      "(i.e., τ ck−1), and the upper truncation point being determined by the current\n",
      "value of the threshold above.\n",
      "\n",
      "In our current political orientation/party affiliation example, only one\n",
      "threshold is freely estimated in each dimension, and so, τ c13 ∼ N(τ13, σ, τ12 =\n",
      "0, τ14 = ∞) and τ c23 ∼ N(τ23, σ, τ22 = 0, τ24 = ∞).\n",
      "\n",
      "Once a set of candidates is generated, the next step in the MH algorithm\n",
      "is to compute the ratio R. In this case, given the truncation of the normal pro-\n",
      "posal densities, the proposals are asymmetric, and hence, the full ratio must\n",
      "be computed. Given that the posterior distributions for the threshold param-\n",
      "eters are independent across equations, p(τ |β, Σ, Z, Y,X) ∝ p(τ |β, X, Y ), and\n",
      "so:\n",
      "\n",
      "R =\n",
      "f(τ c|β, X, Y )g(τ |τ c)\n",
      "f(τ |β, X, Y )g(τ c|τ)\n",
      "\n",
      ",\n",
      "\n",
      "where f(a|b) is the posterior density evaluated at τ (or τ c) and g(c|d) is the\n",
      "value of the proposal density at c when the proposal is centered over d. The\n",
      "posterior for the thresholds is:\n",
      "\n",
      "p(τ |β, X, Y ) ∝\n",
      "n∏\n",
      "\n",
      "i=1\n",
      "\n",
      "Φ(XTi β, τyi , τyi+1),\n",
      "\n",
      "where Φ(a, b, c) is the integral of the normal density function with mean a and\n",
      "variance 1 between thresholds b and c. Thus, the first half of the ratio R is:\n",
      "\n",
      "n∏\n",
      "i=1\n",
      "\n",
      "Φ(βT Xi, τ cyi , τ\n",
      "c\n",
      "yi+1\n",
      "\n",
      ")\n",
      "Φ(βT Xi, τyi , τyi+1),\n",
      "\n",
      ".\n",
      "\n",
      "The latter half of the ratio corrects for the asymmetry in the proposals and\n",
      "is:\n",
      "\n",
      "K∏\n",
      "j=3\n",
      "\n",
      "Φ(τj , σ, τ cj−1, τj+1)\n",
      "Φ(τ cj , σ, τj−1, τ\n",
      "\n",
      "c\n",
      "j+1)\n",
      "\n",
      ",\n",
      "\n",
      "where in this case Φ(a, b, c, d) is the integral of the normal distribution with\n",
      "mean a and standard deviation b between the thresholds c and d. The standard\n",
      "deviation b is chosen to produce an acceptance rate around 50%.\n",
      "\n",
      "The product of these components constitutes the full ratio R. Once this\n",
      "ratio is computed, we follow the usual MH steps: We compare this ratio to a\n",
      "u ∼ U(0, 1) random draw, and we accept τ c if R > u.\n",
      "\n",
      "In the example at hand, we have a two-dimensional outcome, each dimen-\n",
      "sion of which only requires simulation of one threshold. As discussed in greater\n",
      "detail in Section 10.2.5, I perform a separate MH step for each dimension of\n",
      "the outcome.\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 295\n",
      "\n",
      "10.2.4 Step 5: Simulating the error covariance matrix\n",
      "\n",
      "Drawing samples of covariance matrices is straightforward, only involving sim-\n",
      "ulation from an appropriate inverse Wishart distribution. Simulation of co-\n",
      "variance matrices subject to constraints, on the other hand, is not so simple.\n",
      "In the multivariate regression model, no constraints were placed on the error\n",
      "covariance matrix. However, in the multivariate probit model, we must impose\n",
      "constraints to identify the model. Specifically, the typical constraint imposed\n",
      "is that the diagonal elements of the error covariance matrix are all 1.5 This\n",
      "set of constraints makes the error covariance matrix a correlation matrix, and\n",
      "correlation matrices have no standard/known distribution.\n",
      "\n",
      "Simulation of the error covariance/correlation matrix in a multivariate\n",
      "probit model, then, requires some alternative to simulating the matrix from\n",
      "an inverse Wishart distribution. One alternative is to use an MH algorithm\n",
      "to update the free elements of the error covariance matrix within a larger\n",
      "Gibbs sampler for the other parameters of the algorithm (Chib and Greenberg\n",
      "1998). A second alternative is to (1) simulate the approximate error covariance\n",
      "matrix, Σ̃, from an inverse Wishart distribution, and (2) convert Σ̃ into a\n",
      "correlation matrix by computing:\n",
      "\n",
      "Σ = (diag(Σ)−1/2)Σ̃(diag(Σ)−1/2).\n",
      "\n",
      "This set of matrix multiplications simply divides each element (σ̃ij) of the\n",
      "original matrix by the square root of the σ̃ii and σ̃jj (see Imai and van Dyk\n",
      "2005 for example, within the context of the multinomial probit model). In\n",
      "other words, each element is divided by the standard deviations of the rele-\n",
      "vant variables, yielding correlations off the main diagonal and ones along the\n",
      "diagonal. This approach is not exact, but in my experience, I find it generally\n",
      "leads to acceptable inference when (1) the sample size is large; and (2) the\n",
      "error correlations are small—both of which are often true with social science\n",
      "data. In fact, when the sample is large (say, above 1,000), sampling from an\n",
      "inverse Wishart distribution and then converting the draw to a correlation\n",
      "matrix generally produces comparable posterior means for the correlations\n",
      "but larger posterior standard deviations for them compared with using an\n",
      "MH algorithm to sample directly from the posterior distribution for the cor-\n",
      "relations. The reason is that sampling the entire matrix allows the diagonal\n",
      "elements to vary, and often, the sample variances will be less than one. Mul-\n",
      "tiplying the off-diagonal elements by the inverse standard deviations of the\n",
      "relevant diagonal elements thus tends to produce a broader distribution for\n",
      "the correlation than using an MH algorithm to sample directly from the dis-\n",
      "tribution for each correlation. All in all, then, this approach may yield more\n",
      "conservative inference than using MH sampling.\n",
      "\n",
      "5 Technically, there can only be d(d− 1)/2 free elements of the covariance matrix,\n",
      "where d is the number of dimensions of the outcome.\n",
      "\n",
      "\n",
      "\n",
      "296 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "If we decide to use an MH algorithm to sample the correlations, such can\n",
      "be done fairly easily. In a multivariate probit with d outcomes, we will have\n",
      "d(d − 1)/2 free correlations to estimate. We can sequentially update these\n",
      "parameters, rather than updating them simultaneously, by drawing candi-\n",
      "date values for each correlation from normal distributions centered over the\n",
      "previous value of the parameter, with some variance chosen to produce an\n",
      "acceptance rate of around 50%. Below are some example MH steps for se-\n",
      "quentially drawing the correlations from a model with a three-dimensional\n",
      "outcome:\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "for(j in 1:3)\n",
      "\n",
      "{\n",
      "\n",
      "cs=s\n",
      "\n",
      "like=-.5*(n+3+1)*log(det(s))-.5*sum(diag(v%*%solve(s)))\n",
      "\n",
      "if(j==1){cs[2,1]=cs[1,2]=cs[2,1]+rnorm(1,mean=0,sd=.007)}\n",
      "\n",
      "if(j==2){cs[3,1]=cs[1,3]=cs[3,1]+rnorm(1,mean=0,sd=.007)}\n",
      "\n",
      "if(j==3){cs[3,2]=cs[2,3]=cs[3,2]+rnorm(1,mean=0,sd=.007)}\n",
      "\n",
      "if((j==1 & abs(cs[2,1])<1) |\n",
      "\n",
      "(j==2 & abs(cs[3,1])<1) |\n",
      "\n",
      "(j==3 & abs(cs[3,2])<1))\n",
      "\n",
      "{\n",
      "\n",
      "clike=-.5*(n+3+1)*log(det(cs))-.5*sum(diag(v%*%solve(cs)))\n",
      "\n",
      "if((clike-like)>log(runif(1,0,1)))\n",
      "\n",
      "{s[i,,]=cs; acc[j]=acc[j]+1}\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "This set of steps is intended to be placed somewhere within a larger al-\n",
      "gorithm for the multivariate probit model, where the variable v has been\n",
      "computed and is the matrix of sums of cross-products of errors and s is the\n",
      "error covariance matrix from the previous iteration, with diagonal elements\n",
      "permanently fixed at 1. The j loop loops over the three free elements of the\n",
      "covariance/correlation matrix. First, the log-posterior density is computed up\n",
      "to a proportionality constant. Then, a candidate is drawn for an element of\n",
      "the covariance matrix (cs[a,b]). If this candidate falls in the interval [−1,\n",
      "1]—the allowable range for a correlation—then the log-posterior is computed\n",
      "using the candidate and the log-ratio R is computed (which is a subtraction in\n",
      "log scale) and compared with the log of a U(0, 1) random draw, u. If the ratio\n",
      "exceeds log(u), then the correlation matrix is updated using the candidate.\n",
      "Otherwise, the matrix s[] remains as is.\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 297\n",
      "\n",
      "10.2.5 Implementing the algorithm\n",
      "\n",
      "We have already discussed the steps involved in the Gibbs sampler for the\n",
      "multivariate probit model, and in the previous sections, we have discussed\n",
      "various issues that must be handled in the algorithm that differentiate it from\n",
      "the algorithm we discussed for the multivariate linear regression model. All\n",
      "that remains is to integrate these. Below is the complete R algorithm for the\n",
      "current party affiliation/political orientation example:\n",
      "\n",
      "#R program for multivariate probit model\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\mvnprob.dat1\")[,1:7])\n",
      "\n",
      "z=as.matrix(read.table(\"c:\\\\mvnprob.dat1\")[,8:9])\n",
      "\n",
      "#create variables and starting values\n",
      "\n",
      "zstar=matrix(0,nrow(z),2)\n",
      "\n",
      "d=2;k=7\n",
      "\n",
      "b<-matrix(0,(d*k))\n",
      "\n",
      "s=cs=diag(d)\n",
      "\n",
      "tz=matrix(0,d,4)\n",
      "\n",
      "tz[,1]=-Inf; tz[,2]=0; tz[,4]=Inf\n",
      "\n",
      "tz[1,3]=qnorm(sum(z[,1]<=2)/nrow(z),\n",
      "\n",
      "mean=-qnorm(sum(z[,1]==1)/nrow(z),mean=0,sd=1),\n",
      "\n",
      "sd=1)\n",
      "\n",
      "tz[2,3]=qnorm(sum(z[,2]<=2)/nrow(z),\n",
      "\n",
      "mean=-qnorm(sum(z[,2]==1)/nrow(z),mean=0,sd=1),\n",
      "\n",
      "sd=1)\n",
      "\n",
      "ctz=tz\n",
      "\n",
      "acc1=acc2=acctot=0;\n",
      "\n",
      "write(c(0,t(b),t(s),tz[1,3],tz[2,3]),\n",
      "\n",
      "file=\"c:\\\\mvprob.res\",ncolumns=(d*k+k*k +3),append=T)\n",
      "\n",
      "#begin Gibbs sampling\n",
      "\n",
      "for(i in 2:6000){\n",
      "\n",
      "#draw latent data: one-iteration gibbs sampler for tmvn simulation\n",
      "\n",
      "bb=matrix(b,k,2)\n",
      "\n",
      "m=x%*%bb\n",
      "\n",
      "for(j in 1:d)\n",
      "\n",
      "{\n",
      "\n",
      "mm=m[,j] + t(s[j,-j])%*%solve(s[-j,-j])%*%(zstar[,-j]-m[,-j])\n",
      "\n",
      "ss=s[j,j] - t(s[j,-j])%*%solve(s[-j,-j])%*%s[j,-j]\n",
      "\n",
      "zstar[,j]=qnorm(runif(nrow(z),\n",
      "\n",
      "min=pnorm(tz[j,z[,j]],mm,sqrt(ss)),\n",
      "\n",
      "\n",
      "\n",
      "298 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "max=pnorm(tz[j,(z[,j]+1)],mm,sqrt(ss))),\n",
      "\n",
      "mean=mm,sd=sqrt(ss))\n",
      "\n",
      "}\n",
      "\n",
      "#draw thresholds using Cowles’ algorithm\n",
      "\n",
      "ctz[1,3]=qnorm(runif(1,min=pnorm(0,mean=tz[1,3],sd=.01),max=1),\n",
      "\n",
      "mean=tz[1,3],sd=.01)\n",
      "\n",
      "r=as.matrix((pnorm(ctz[1,z[,1]+1]-m[,1],0,1)\n",
      "\n",
      "-pnorm(ctz[1,z[,1]]-m[,1],0,1))\n",
      "\n",
      "/\n",
      "\n",
      "(pnorm(tz[1,z[,1]+1]-m[,1],0,1)\n",
      "\n",
      "-pnorm(tz[1,z[,1]]-m[,1],0,1)))\n",
      "\n",
      "r= t(log(r))%*%matrix(1,nrow(z))\n",
      "\n",
      "+log((1-pnorm(-tz[1,3]/.01,0,1))/(1-pnorm(-ctz[1,3]/.01,0,1)))\n",
      "\n",
      "if(r>log(runif(1,0,1))){tz[1,3]=ctz[1,3]; acc1=acc1+1}\n",
      "\n",
      "ctz[2,3]=qnorm(runif(1,min=pnorm(0,mean=tz[2,3],sd=.01),max=1),\n",
      "\n",
      "mean=tz[2,3],sd=.01)\n",
      "\n",
      "r=as.matrix((pnorm(ctz[2,z[,2]+1]-m[,2],0,1)\n",
      "\n",
      "-pnorm(ctz[2,z[,2]]-m[,2],0,1))\n",
      "\n",
      "/\n",
      "\n",
      "(pnorm(tz[2,z[,2]+1]-m[,2],0,1)\n",
      "\n",
      "-pnorm(tz[2,z[,2]]-m[,2],0,1)))\n",
      "\n",
      "r=t(log(r))%*%matrix(1,nrow(z))\n",
      "\n",
      "+log((1-pnorm(-tz[2,3]/.01,0,1))/(1-pnorm(-ctz[2,3]/.01,0,1)))\n",
      "\n",
      "if(r>log(runif(1,0,1))){tz[2,3]=ctz[2,3]; acc2=acc2+1}\n",
      "\n",
      "#draw b from mvn\n",
      "\n",
      "vb=solve(solve(s)%x%(t(x)%*%x))\n",
      "\n",
      "mn=vb%*%(as.vector(t(x)%*%zstar%*%t(solve(s))))\n",
      "\n",
      "b=mn+t(rnorm((d*k),0,1)%*%chol(vb))\n",
      "\n",
      "#use metropolis-hastings sampling to draw sigma\n",
      "\n",
      "e=matrix((as.vector(zstar)-(diag(d)%x%x%*%b)),nrow(z),d)\n",
      "\n",
      "v=t(e)%*%e\n",
      "\n",
      "like=-.5*((d+nrow(z)+1)*log(det(s)) + sum(diag(v%*%solve(s))))\n",
      "\n",
      "cs[1,2]=cs[2,1]=s[1,2]+rnorm(1,mean=0,sd=.01)\n",
      "\n",
      "if(abs(cs[1,2])<1)\n",
      "\n",
      "{\n",
      "\n",
      "cslike=-.5*((d+nrow(z)+1)*log(det(cs)) + sum(diag(v%*%solve(cs))))\n",
      "\n",
      "if((cslike-like)>log(runif(1,0,1)))\n",
      "\n",
      "{\n",
      "\n",
      "s[1,2]=s[2,1]=cs[1,2]; acctot=acctot+1\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 299\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "if(i%%10==0){print(i)}\n",
      "\n",
      "if(i%%5==0){write(c(i,t(b),t(s),tz[1,3],tz[2,3]),\n",
      "\n",
      "file=\"c:\\\\mvprob.res\",ncolumns=(d*k+k*k+3),append=T)}\n",
      "\n",
      "}\n",
      "\n",
      "This algorithm is certainly the longest algorithm in the book thus far, but\n",
      "it is not inherently difficult to follow if it is considered in parts. As with other\n",
      "algorithms in the book, we first read the covariates and outcome variables into\n",
      "two matrices (x and z, respectively). Next, we establish matrices and starting\n",
      "values for the various parameters in the algorithm. Here, I created a variable\n",
      "to store the latent variables in (zstar), started each regression coefficient at\n",
      "0, and started the error covariance matrix as an identity matrix. I created\n",
      "two matrices for the error covariance matrix—s and cs. One (cs) is a storage\n",
      "variable for candidate parameters, and the other (s) holds the previous value.\n",
      "Next, I created two sets of threshold parameters—one for the candidates,\n",
      "and one for the previous values, just as with the covariance matrix—and I\n",
      "initialized them as in Chapter 8 based on the proportion of respondents in\n",
      "each category of the original variable z. Finally, I created three acceptance\n",
      "rate variables in which to monitor the parameters being updated with MH\n",
      "steps.\n",
      "\n",
      "Once the Gibbs sampler begins, the latent data are simulated using the\n",
      "Gibbs sampling approach described in Section 10.2.2. The only difference be-\n",
      "tween the algorithm as written here versus in Section 10.2.2 is that it appears\n",
      "that I am only performing one iteration of the Gibbs sampler to obtain a draw\n",
      "for each zstar. In fact, this algorithm does only iterate the Gibbs sampler one\n",
      "time: It samples each latent trait (for each person) conditional on the values\n",
      "of the latent variable in the other dimensions. Whereas we needed to perform\n",
      "at least two iterations to produce a legitimate draw from the truncated mul-\n",
      "tivariate normal distribution in Section 10.2.2, here, I rely on the value of the\n",
      "latent variable stored from the previous iteration of the overall Gibbs sampler\n",
      "for the model. As the overall Gibbs sampler for the model converges, so should\n",
      "the simulations from the truncated multivariate normal distribution within.\n",
      "\n",
      "After these latent vectors are drawn, the two free thresholds are updated\n",
      "using Cowles’ algorithm, as described in Section 10.2.3. This section of the\n",
      "program is repeated: The first instance is for the threshold in the latent distri-\n",
      "bution for the first dimension/equation; the second is for the the threshold in\n",
      "the latent distribution for the second dimension/equation. Next, the regres-\n",
      "sion parameters are updated as in the original multivariate regression model at\n",
      "the beginning of this chapter. Finally, the free element in the error covariance\n",
      "matrix—the error correlation between the two dimensions of the outcome—is\n",
      "simulated using an MH step as described in the previous section.\n",
      "\n",
      "The algorithm took approximately 45 minutes to run 6,000 iterations on\n",
      "my (average) desktop computer. Although this length of time is long compared\n",
      "\n",
      "\n",
      "\n",
      "300 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "with the time taken by the other algorithms presented in the book, it is\n",
      "considerably shorter than it would take if we had not used Cowles’ algorithm\n",
      "to sample the thresholds, and it is considerably shorter than it would have\n",
      "taken a few years ago when processor speeds were much slower. Additionally,\n",
      "using R under another platform (e.g., UNIX), and/or using an alternative\n",
      "programming language (e.g., C in UNIX), would drastically improve speed.\n",
      "\n",
      "Table 10.3 presents the results—after thinning the samples to every fifth\n",
      "value and dropping the first 200 samples as burn-in—and compares them\n",
      "to the results obtained from SAS’s proc logistic descending procedure\n",
      "(STATA results are equivalent to the SAS results) using independent univari-\n",
      "ate ordinal probit models.\n",
      "\n",
      "Table 10.3. Multivariate probit regression model of political orientation and party\n",
      "affiliation on covariates (GSS data, n = 37, 028).\n",
      "\n",
      "Multivariate Probit Model Univariate Probit Models\n",
      "Bayesian Posterior Means Classical MLE\n",
      "\n",
      "Variable Dem. vs. Rep. Lib. vs. Cons. Dem. vs. Rep. Lib. vs. Cons.\n",
      "\n",
      "Intercept1 −1.55(.07)*** −0.35(.07)*** −2.52(.07)*** −1.37(.07)***\n",
      "Intercept2 NA NA −1.57(.07)*** −0.34(.07)***\n",
      "Year 0.0097(.0007)*** 0.004(.0007)*** 0.0097(.0007)*** 0.004(.0007)***\n",
      "Age −0.002(.0003)*** 0.007(.0003)*** −0.002(.0004)*** 0.007(.0003)***\n",
      "Male 0.094(.012)*** 0.08(.01)*** 0.095(.012)*** 0.08(.01)***\n",
      "White 0.798(.018)*** 0.27(.016)*** 0.801(.017)*** 0.26(.02)***\n",
      "South −0.014(.012) 0.15(.01)*** −0.014(.013) 0.15(.01)***\n",
      "Educ 0.029(.002)*** −0.001(.002) 0.029(.002)*** −0.002(.002)\n",
      "τ1 −∞ −∞ −∞ −∞\n",
      "τ2 0.0 0.0 NA NA\n",
      "τ3 0..96(.007)*** 1.03(.007)*** NA NA\n",
      "τ4 +∞ +∞ +∞ +∞\n",
      "ρe .327(.006)***\n",
      "\n",
      "Note: The Bayesian p-values are based on one-sided tail probabilities that the pa-\n",
      "rameter exceeds 0 truncated to the classical cut-points of *p < .05, **p < .01,\n",
      "***p < .001. For the MLEs, the p-values are based on the usual t-test.\n",
      "\n",
      "The Bayesian results for the effect of year indicate that individuals have\n",
      "become more likely to consider themselves Republicans, and have become\n",
      "more conservative, over time. Older persons are more likely to be Democrats\n",
      "but are also more conservative than younger persons. Males and whites are\n",
      "more likely to be both conservative and Republican. There is no obvious\n",
      "regional difference in party affiliation, but Southerners are more likely to be\n",
      "conservative. Interestingly, years of schooling has a positive influence on the\n",
      "tendency to be Republican but does not affect political orientation.\n",
      "\n",
      "The results of separate classical univariate probit models differ very little\n",
      "from the Bayesian results, with a couple of exceptions. First, as estimated\n",
      "by SAS, the univariate probit models have two intercepts and no estimated\n",
      "thresholds. This is a slightly different parameterization of the model than\n",
      "STATA’s, which estimates no model intercept but multiple thresholds (see\n",
      "\n",
      "\n",
      "\n",
      "10.2 Multivariate probit models 301\n",
      "\n",
      "Chapter 8), and it is clearly a different parameterization than our model\n",
      "with one intercept and one estimated threshold. However, if we consider our\n",
      "intercept as SAS’s “Intercept 2”, we can subtract the estimated threshold\n",
      "from this intercept to obtain SAS’s “Intercept 1.” Given that these threshold\n",
      "parameters are nuissance parameters, the parameterization of the model with\n",
      "respect to them has no substantive implications.\n",
      "\n",
      "Second, the multivariate probit model results include an additional param-\n",
      "eter, ρe, that was not estimated (obviously) in the univariate probit models.\n",
      "Our result for this parameter indicates that the errors are moderately and\n",
      "positively correlated (ρ = .33), meaning that unobserved factors that influ-\n",
      "ence party affiliation tend to be positively correlated with unobserved factors\n",
      "that influence political orientation.\n",
      "\n",
      "If the results of the multivariate probit model are generally indistinct from\n",
      "those of the separate univariate probit models estimated via maximum like-\n",
      "lihood by extant software packages, then what is the advantage of the multi-\n",
      "variate approach specifically and the Bayesian approach more generally? As\n",
      "we discussed at the beginning of the chapter, the main advantages to adopt-\n",
      "ing a multivariate modeling strategy include (1) the elegance of using a single\n",
      "model, rather than multiple models; and (2) the gain in efficiency if the covari-\n",
      "ate vectors differ across equations. Additionally, if interest centers on making\n",
      "inference regarding functions of parameters rather than on the parameters\n",
      "themselves, a multivariate Bayesian approach can provide more information\n",
      "more simply than a classical approach.\n",
      "\n",
      "In assessing this latter claim, using the current example, suppose our in-\n",
      "terest lay in understanding change in the concurrence of party affiliation and\n",
      "political orientation over time, and not on change in each independently. For\n",
      "instance, suppose our question is whether there has been a shift over time\n",
      "in the propensity for individuals to self-identify as conservative Republicans.\n",
      "This question requires us to examine the influence of time on a bivariate\n",
      "propensity—a two-dimensional integral in this model. Refer again to Fig-\n",
      "ure 10.1—this probabiilty is the area above the third threshold in each di-\n",
      "mension. This integral depends on the error correlation between equations as\n",
      "well as the model parameters applied to a set of specified covariate values.\n",
      "A set of univariate models lacks the error correlation and therefore cannot\n",
      "help us address this question. A classically estimated bivariate probit model,\n",
      "on the other hand, can help us obtain a point estimate of the probability a\n",
      "hypothetical individual (with a given set of covariate values) will self-identify\n",
      "as a conservative Republican. However, producing an interval estimate of this\n",
      "probability is not straightforward, because it would involve using some com-\n",
      "bination of the standard errors of the model parameters across the equations,\n",
      "plus incorporating the standard error of the error correlation, plus considering\n",
      "how these standard errors translate through the bivariate normal integral.\n",
      "\n",
      "Under the Bayesian approach, we can easily produce a posterior distri-\n",
      "bution for the probability of self-identifying as a conservative Republican by\n",
      "using posterior predictive simulation. Throughout the book, we have used and\n",
      "\n",
      "\n",
      "\n",
      "302 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "discussed posterior predictive simulation as a tool for evaluating how well a\n",
      "model fits the data at hand; this is still true. However, posterior predictive\n",
      "simulation can also be used to make inference beyond that which can be made\n",
      "directly from the estimated model parameters. Specifically, we can make in-\n",
      "ference about change over time in the probability an individual with given\n",
      "characteristics will self-identify as a conservative Republican. The R program\n",
      "below shows how we can accomplish this:\n",
      "\n",
      "g=as.matrix(read.table(\"c:\\\\mvprob1.out\")[201:1200,])\n",
      "\n",
      "summ=matrix(0,31,4)\n",
      "\n",
      "#loop across all years\n",
      "\n",
      "for(m in 1:31)\n",
      "\n",
      "{\n",
      "\n",
      "x=matrix(c(1,m+73,30,1,1,1,12),7)\n",
      "\n",
      "cellsum=matrix(0,1000)\n",
      "\n",
      "#loop over 1000 post-burnin samples of parameters\n",
      "\n",
      "for(i in 1:1000)\n",
      "\n",
      "{\n",
      "\n",
      "s=matrix(c(1,g[i,17],g[i,17],1),2,2)\n",
      "\n",
      "b=matrix(g[i,2:15],7,2)\n",
      "\n",
      "t1=g[i,20]; t2=g[i,21]\n",
      "\n",
      "xb=t(b)%*%x\n",
      "\n",
      "#generate 1000 ppd samples to compute probabilities\n",
      "\n",
      "#this step is equivalent to integration over desired cell\n",
      "\n",
      "for(j in 1:1000)\n",
      "\n",
      "{\n",
      "\n",
      "zz=xb+t(rnorm(2,0,1)%*%chol(s))\n",
      "\n",
      "if(zz[1]>t1 & zz[2]>t2){cellsum[i]=cellsum[i]+1}\n",
      "\n",
      "}\n",
      "\n",
      "if(i%%5==0){print(c(m,i))}\n",
      "\n",
      "}\n",
      "\n",
      "summ[m,1]=mean(cellsum/1000)\n",
      "\n",
      "summ[m,2]=sd(cellsum/1000)\n",
      "\n",
      "summ[m,3]=sort(cellsum)[25]\n",
      "\n",
      "summ[m,4]=sort(cellsum)[975]\n",
      "\n",
      "}\n",
      "\n",
      "The R program first reads in the 1,000 post-burn-in parameter samples\n",
      "from the multivariate probit output file and creates a matrix to store summary\n",
      "statistics about the posterior predictive distributions for 31 individuals. The\n",
      "first loop (over m) is across the years of observation (1974 to 2004, a 31-\n",
      "year period). The vector x is then defined so that the posterior predictive\n",
      "simulation will be for a 30-year-old white male from the south with 12 years\n",
      "of education, for each year from 1974 through 2004.\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 303\n",
      "\n",
      "Once the error correlation matrix (s), regression parameter matrix (b), and\n",
      "thresholds (t1 and t2) have been defined, a predicted score (xb) is computed.\n",
      "Next, 1,000 samples from a bivariate normal distribution with this predicted\n",
      "mean and given error covariance matrix are generated, and a tally of the\n",
      "number of these samples that fall above the appropriate threshold in each\n",
      "dimension is kept (cellsum). This step is equivalent to computing the integral\n",
      "of the bivariate normal distribution with the given mean and covariance over\n",
      "its upper right tail.\n",
      "\n",
      "Once we have computed this tally for the current sample of the model\n",
      "parameters, we repeat the process for all 1,000 post-burn-in values of the\n",
      "parameters. The end result of this process is that, for each year, we obtain\n",
      "summaries of the distribution for the probability of self-identifying as a con-\n",
      "servative Republican.\n",
      "\n",
      "Figure 10.6 is a plot of the 95% probability intervals for being a conser-\n",
      "vative Republican from 1974 to 2004. As the figure shows, this probability\n",
      "has increased substantially over the 31-year period. In 1974, the probability a\n",
      "30-year-old white Southern male with a high-school diploma would consider\n",
      "himself a conservative Republican was about .136 (95% interval of [.11, .16]).\n",
      "By 2004, this probability had increased to .197 (95% interval of [.17, .22]),\n",
      "which is a 45% increase in the probability.\n",
      "\n",
      "In this example, I have shown how to answer only one question using pos-\n",
      "terior predictive simulation from the multivariate model results. Nonetheless,\n",
      "it should be clear that the approach is flexible and can be used for making any\n",
      "number of additional inferences. In the next section, I provide a more detailed\n",
      "and realistic use of the Bayesian approach to making inference to parameters\n",
      "not directly estimated in a multivariate setting.\n",
      "\n",
      "10.3 A multivariate probit model for generating\n",
      "distributions of multistate life tables\n",
      "\n",
      "The life table is a basic tool of demography that has been used for centuries.\n",
      "The basic measure produced by a life table—life expectancy—is the expected\n",
      "number of years of life remaining for individuals at given ages and is pro-\n",
      "duced using age-specific probabilities (or rates) of mortality in a specific year\n",
      "(see Preston, Heuveline, and Guillot 2001; Schoen 1988). In the early 1970s,\n",
      "researchers began using hazard regression models (e.g., the discrete time pro-\n",
      "bit model discussed in Chapter 8) to produce smoothed (parametric and pre-\n",
      "dicted) mortality probabilities for life table estimation (see Cox 1972; Menken\n",
      "et al. 1981). Using hazard models allowed researchers to produce life tables\n",
      "for specific subpopulations, because estimated age-specific mortality rates for\n",
      "a subpopulation could be obtained from the hazard model by (1) applying\n",
      "the estimated model parameters to a given covariate profile (e.g., a specific\n",
      "value for age, sex, race, etc.) to obtain a predicted score and (2) transforming\n",
      "this value into a probability by inverting the link function used in the hazard\n",
      "\n",
      "\n",
      "\n",
      "304 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "Year\n",
      "\n",
      "P\n",
      "ro\n",
      "\n",
      "b\n",
      "a\n",
      "\n",
      "b\n",
      "ili\n",
      "\n",
      "ty\n",
      " o\n",
      "\n",
      "f \n",
      "B\n",
      "\n",
      "e\n",
      "in\n",
      "\n",
      "g\n",
      " a\n",
      "\n",
      " C\n",
      "o\n",
      "\n",
      "n\n",
      "se\n",
      "\n",
      "rv\n",
      "a\n",
      "\n",
      "tiv\n",
      "e\n",
      "\n",
      " R\n",
      "e\n",
      "\n",
      "p\n",
      "u\n",
      "\n",
      "b\n",
      "lic\n",
      "\n",
      "a\n",
      "n\n",
      "\n",
      "1974 1978 1982 1986 1990 1994 1998 2002\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "5\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".2\n",
      "0\n",
      "\n",
      ".2\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "5\n",
      "\n",
      "Fig. 10.6. Posterior predictive distributions for the probability a 30-year-old white\n",
      "male from the South self-identifies as a conservative Republican across time.\n",
      "\n",
      "model. Once a complete set of age-specific estimated mortality probabilities is\n",
      "produced—by incrementing age and repeating the calculations, as we did in\n",
      "the previous section with the year variable—basic life table calculations can\n",
      "then be applied to produce a life table.\n",
      "\n",
      "The multistate life table is an important extension of the basic life ta-\n",
      "ble that allows one to decompose total life expectancy into estimates of life\n",
      "remaining to be lived in different states (see Schoen 1988). For example, an\n",
      "important current measure derived from multistate life tables is healthy life ex-\n",
      "pectancy (HLE), which is the number of remaining years that can be expected\n",
      "to be lived healthy (or some similar health-based state). Its complement, un-\n",
      "healthy life expectancy (ULE), is the number of remaining years that can be\n",
      "expected to be lived unhealthy, and the sum of HLE and ULE is total life\n",
      "expectancy (TLE) (e.g., see Crimmins, Saito, and Ingegneri 1997).\n",
      "\n",
      "Whereas the basic life table requires age-specific mortality probabilities\n",
      "as input, the multistate life table requires age-specific transition probability\n",
      "matrices as input, where these matrices contain the probabilities of transition-\n",
      "ing between the various states being considered. Figure 10.7 shows the “state\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 305\n",
      "\n",
      "space” for a simple three-state model in which individuals can be healthy\n",
      "(state 1), unhealthy (state 2), or dead (state 3), and the allowable transitions\n",
      "are from the healthy state to healthy, unhealthy, or dead states, and from the\n",
      "unhealthy state to either the healthy, unhealthy, or dead states. Thus, there\n",
      "are six required transition probabilities (at each age).\n",
      "\n",
      "●\n",
      "\n",
      "Healthy\n",
      "(Stay=p11)\n",
      "\n",
      "Unhealthy\n",
      "(Stay=p22)\n",
      "\n",
      "Dead\n",
      "\n",
      "p21\n",
      "\n",
      "p12\n",
      "\n",
      "p13 p23\n",
      "\n",
      "Fig. 10.7. Representation of state space for a three state model.\n",
      "\n",
      "Figure 10.8 shows that the necessary age-specific transition probabilities\n",
      "may be estimated with a bivariate dichotomous probit model, with healthy\n",
      "versus unhealthy being one outcome variable, and alive versus dead being the\n",
      "other. In order to actually capture transition probabilities, we need two-wave\n",
      "panel data, so that we may know both the state in which individuals begin a\n",
      "time interval and the state in which individuals end the interval. If we have\n",
      "such data, then age and the starting state can be included as covariates, along\n",
      "with any other variables desired.\n",
      "\n",
      "Age-specific transition probabilities can then be produced from the model\n",
      "parameters by computing the linear combination of parameters and desired\n",
      "covariate values. To obtain the complete age range, this linear combination\n",
      "\n",
      "\n",
      "\n",
      "306 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "●\n",
      "\n",
      "Alive Dead\n",
      "\n",
      "Healthy\n",
      "\n",
      "Unhealthy\n",
      "\n",
      "τ2\n",
      "\n",
      "τ1\n",
      "\n",
      "Start H: p11\n",
      "\n",
      "Start U: p21\n",
      "\n",
      "Start H: p12\n",
      "\n",
      "Start U: p22\n",
      "\n",
      "Start H: p13\n",
      "\n",
      "Start U: p23\n",
      "\n",
      "Start H: p13\n",
      "\n",
      "Start U: p23\n",
      "\n",
      "Fig. 10.8. Two-dimensional outcome for capturing a three-state state space.\n",
      "\n",
      "should be repeated, incrementing age from the youngest to oldest ages. To\n",
      "obtain all transition probabilities, the covariate value for the starting state\n",
      "(i.e., healthy vs. unhealthy) must be set to each of its possible values. The\n",
      "transition probabilities can then be obtained as in the univariate hazard model\n",
      "discussed above by inverting the link function applied to the predicted values.\n",
      "However, in the multistate case, the link function is multivariate, and so the\n",
      "transition probabilities are obtained via bivariate integration. I illustrate this\n",
      "process in the example.\n",
      "\n",
      "The process of using a multivariate model with covariates to produce tran-\n",
      "sition probabilities for input into multistate life table calculations is not new\n",
      "(see Land, Guralnik, and Blazer 1994). However, the classical approach to es-\n",
      "timation using maximum likelihood methods does not allow a straightforward\n",
      "method for constructing interval estimates of the state expectancies (HLE,\n",
      "ULE, and TLE) in these tables (but see Laditka and Wolf 1998 for a method\n",
      "to do so via simulation). It is unclear how the estimated standard errors for\n",
      "the parameters translate into standard errors for state expectancies, as we\n",
      "discussed regarding the party affiliation/political orientation example in the\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 307\n",
      "\n",
      "previous section. Thus, to date, most researchers using multistate life tables\n",
      "produced from hazard model results have simply reported point estimates for\n",
      "life table quantities. Yet, given that the data used for model estimation typ-\n",
      "ically come from samples, and are not population data, we need to be able\n",
      "to quantify at a minimum the uncertainty inherent in using sample data to\n",
      "make inference to populations. The Bayesian approach offers a straightforward\n",
      "way of doing so (see Lynch and Brown 2005 for a more in-depth discussion\n",
      "than what is presented below for single decrement, multiple decrement, and\n",
      "multistate life tables using Gibbs samplers and MH algorithms).\n",
      "\n",
      "10.3.1 Model specification and simulation\n",
      "\n",
      "The data for this example are from the 1987 and 1992 NHEFS. As described\n",
      "earlier, the NHEFS is a series of at least four surveys, begun in 1971 and\n",
      "continuing in 1982, 1987, and 1992. The original sample size was 14,407 per-\n",
      "sons. After elminating persons who died or dropped out of the survey by\n",
      "1987, restricting the age range to persons 45 years of age or older in 1987,\n",
      "and eliminating individuals who were missing on one or more variables of in-\n",
      "terest, I obtained a sample of n = 3, 495 persons. I include age in 1987, sex,\n",
      "race, region of residence, marital status (both measured in 1987), education,\n",
      "and self-rated health in 1987 and 1992 in the analyses. Self-rated health is\n",
      "dichotomized as “Excellent” or “Good” health versus “Fair” or “Poor” health\n",
      "in both 1987 and 1992, and the 1987 measure is included as a covariate. Age\n",
      "is measured in five-year intervals (i.e., 45-49 = 0, 50-54 = 1, . . ., 85+ = 8).\n",
      "Sex, race, region, and marital status are measured as dummy variables (male\n",
      "= 1, nonwhite = 1, South = 1, married = 1), and education is measured as\n",
      "years of schooling.\n",
      "\n",
      "A bivariate probit model for predicting health status (unhealthy vs.\n",
      "healthy) and survival status (dead vs. alive) in 1992 is constructed, with\n",
      "health status in 1987, and an interaction between age and health status in\n",
      "1987, included as predictors, along with sex, race, region, marital status, and\n",
      "education. The bivariate probit model for predicting health and survival status\n",
      "in 1992 does not differ substantially from the bivariate probit model discussed\n",
      "in the previous section, and so I do not repeat the general model here (refer\n",
      "to the previous section).\n",
      "\n",
      "One feature of the model, however, does differ and requires some discus-\n",
      "sion. Given the outcome space shown in Figure 10.8, it is clear that we can\n",
      "observe health status among survivors but not among decedents. For indi-\n",
      "viduals who died between the 1987 and 1992 waves, health status in 1992 is\n",
      "not observed, because the respondent, of course, could not be interviewed in\n",
      "1992. This dilemma constitutes a missing data problem of sorts and is easily\n",
      "handled via Gibbs sampling. The only change in the model—and really, in\n",
      "the Gibbs sampler—is that latent data for the health outcome for decedents\n",
      "must be simulated without the truncation constraint. Below is the R program\n",
      "for the model after making this adjustment:\n",
      "\n",
      "\n",
      "\n",
      "308 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "#read data\n",
      "\n",
      "x=as.matrix(read.table(\"c:\\\\mvnprob.dat2\")[,1:9])\n",
      "\n",
      "z=as.matrix(read.table(\"c:\\\\mvnprob.dat2\")[,10:11])\n",
      "\n",
      "#estabish variables\n",
      "\n",
      "zstar=matrix(0,nrow(z),2)\n",
      "\n",
      "b<-matrix(0,18)\n",
      "\n",
      "s<-diag(d); cs<-diag(d)\n",
      "\n",
      "acctot=0\n",
      "\n",
      "#define thresholds--note ’trick’ for t3 and t4\n",
      "\n",
      "tz=matrix(0,d,4);ctz=matrix(0,d,4)\n",
      "\n",
      "tz[1]=-Inf; tz[2]=0; tz[3]=tz[4]=Inf\n",
      "\n",
      "write(c(0,t(b),t(s)),file=\"c:\\\\mshaz.out\", ncolumns=23,append=T)\n",
      "\n",
      "#begin Gibbs sampler\n",
      "\n",
      "for(i in 2:10000){\n",
      "\n",
      "#draw latent data\n",
      "\n",
      "bb=matrix(b,9,2)\n",
      "\n",
      "m=x%*%bb\n",
      "\n",
      "#mini mvn gibbs sampler--KEY PART\n",
      "\n",
      "mm=m[,2] + s[1,2]*(zstar[,1]-m[,1])\n",
      "\n",
      "ss=1-s[1,2]^2\n",
      "\n",
      "zstar[,2]=qnorm(runif(nrow(z),\n",
      "\n",
      "min=pnorm(tz[z[,2]+1],mm,sqrt(ss)),\n",
      "\n",
      "max=pnorm(tz[z[,2]+2],mm,sqrt(ss))),mean=mm,sd=sqrt(ss))\n",
      "\n",
      "mm=m[,1] + s[1,2]*(zstar[,2]-m[,2])\n",
      "\n",
      "ss=1-s[1,2]^2\n",
      "\n",
      "zstar[,1]=qnorm(runif(nrow(z),\n",
      "\n",
      "min=pnorm(tz[z[,1]-z[,2]+1],mm,sqrt(ss)),\n",
      "\n",
      "max=pnorm(tz[z[,1]+z[,2]+2],mm,sqrt(ss))),mean=mm,sd=sqrt(ss))\n",
      "\n",
      "#draw b from mvn\n",
      "\n",
      "vb=solve(solve(s)%x%(t(x)%*%x))\n",
      "\n",
      "mn=vb%*%(as.vector(t(x)%*%zstar%*%t(solve(s))))\n",
      "\n",
      "b=mn+t(rnorm(18,0,1)%*%chol(vb))\n",
      "\n",
      "#simulate s using MH sampling\n",
      "\n",
      "e=matrix((as.vector(zstar)-(diag(2)%x%x%*%b)),nrow(z),2)\n",
      "\n",
      "v=t(e)%*%e\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 309\n",
      "\n",
      "like=-.5*(2+nrow(z)+1)*log(det(s))-.5*sum(diag(v%*%solve(s)))\n",
      "\n",
      "cs[1,2]=cs[2,1]=s[1,2]+rnorm(1,mean=0,sd=.03)\n",
      "\n",
      "if(abs(cs[1,2])<1)\n",
      "\n",
      "{\n",
      "\n",
      "cslike=-.5*(2+nrow(z)+1)*log(det(cs))-.5*sum(diag(v%*%solve(cs)))\n",
      "\n",
      "if((cslike-like)>log(runif(1,0,1)))\n",
      "\n",
      "{s[1,2]=s[2,1]=cs[1,2]; acctot=acctot+1}\n",
      "\n",
      "}\n",
      "\n",
      "if(i%%10==0){print(c(i,b[1],b[1+k],s[1,2],acctot/i),digits=5)}\n",
      "\n",
      "if(i%%5==0)\n",
      "\n",
      "{write(c(i,t(b),t(s)),file=\"c:\\\\mshaz.out\",ncolumns=23,append=T)}\n",
      "\n",
      "}\n",
      "\n",
      "This algorithm is remarkably similar to the longer algorithm presented in\n",
      "the previous section with only a few exceptions. First, there is no estimation\n",
      "of thresholds. Given that both dimensions of the outcome are dichotomous,\n",
      "there are only three thresholds in each dimension, and these are fixed at −∞,\n",
      "0, and ∞ in each dimension to identify the model. Second, I have added a\n",
      "fourth threshold in each dimension; these thresholds are set equal to ∞—\n",
      "like the third threshold in each dimension—and are used as a computing\n",
      "convenience.\n",
      "\n",
      "Third, and most importantly, the simulation of the latent data imposes\n",
      "slightly different truncation requirements (see the lines after the KEY PART\n",
      "comment). The outcome variables are coded so that the mortality outcome\n",
      "takes a 1 or 0 value, and the health outcome takes a 1 or 0 value for sur-\n",
      "vivors and a 1 for decedents. With this coding, simulation of the latent data\n",
      "underlying the observed dichotomous outcomes is as follows:\n",
      "\n",
      "Observed outcome Latent health trait Latent mortality trait\n",
      "alive and healthy below 0 below 0\n",
      "alive and unhealthy above 0 below 0\n",
      "dead no truncation above 0\n",
      "\n",
      "The specifc snippets of code that perform the last simulation are:\n",
      "\n",
      "...min=pnorm(tz[z[,1]-z[,2]+1]...\n",
      "\n",
      "max=pnorm(tz[z[,1]+z[,2]+2]...\n",
      "\n",
      "For individuals who die, the first snippet reduces to tz[1], and the second\n",
      "reduces to tz[4]. Thus, the latent trait for health is simulated from −∞ to\n",
      "∞. For persons who remain alive and are healthy in 1992, in contrast, these\n",
      "snippets reduce to t[1] and t[2](−∞ to 0). Finally, for persons who remain\n",
      "alive but are unhealthy in 1992, these snippets reduce to t[2] and t[3] (0\n",
      "to ∞).\n",
      "\n",
      "\n",
      "\n",
      "310 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "It is important to realize that, although these are minor changes to the\n",
      "model/program, handling this missing data could not be easily done without\n",
      "a multivariate model. If we were using separate univariate probit models for\n",
      "health and mortality, decedents would be treated as missing observations and\n",
      "would, by default, be listwise deleted by most software packages. Even if a\n",
      "univariate model were employed that used a FIML estimator or some other\n",
      "method to compensate for the missing outcome data, it would be less efficient\n",
      "than the multivariate approach, which can use the error correlation between\n",
      "equations, as well as the covariate effects in the mortality dimension, to inform\n",
      "inference regarding the missing data.\n",
      "\n",
      "10.3.2 Life table generation and other posterior inferences\n",
      "\n",
      "As the program indicates, I ran this Gibbs sampler for 10,000 iterations, saving\n",
      "every fifth sample. All parameters converged very rapidly and mixed very thor-\n",
      "oughly, and I retained the last 1,000 samples for posterior inference. Table 10.4\n",
      "shows the posterior means and posterior standard deviations for model param-\n",
      "eters. Age and poor health in 1987 had expected positive effects on both poor\n",
      "health and mortality. Also, as expected, males, nonwhites, and Southerners\n",
      "had generally poorer health and greater mortality risk than females, whites,\n",
      "and nonsoutherners, while education and marriage had protective effects.\n",
      "\n",
      "Table 10.4. Results of multivariate probit regression of health and mortality on\n",
      "covariates.\n",
      "\n",
      "Outcome\n",
      "\n",
      "Variable Poor Health Mortality\n",
      "\n",
      "Intercept −0.65(.15)*** −2.50(.18)***\n",
      "Age 0.069(.02)*** 0.34(.02)***\n",
      "Unhealthy in ’87 1.44(.12)*** 0.77(.16)***\n",
      "Age×Unhealthy in ’87 −0.03(.03) −0.04(.03)#\n",
      "Male 0.13(.06)* 0.38(.07)***\n",
      "Nonwhite 0.35(.09)*** 0.05(.10)\n",
      "South 0.04(.06) 0.09(.07)#\n",
      "Married −0.20(.07)** −0.16(.07)*\n",
      "Yrs. Schooling −0.05(.01)*** −0.02(.01)*\n",
      "σ12 −0.10(.17)\n",
      "\n",
      "Note: The Bayesian estimates are posterior means. The Bayesian p-values are based\n",
      "on one-sided tail probabilities that the parameter exceeds 0 truncated to the classical\n",
      "cut-points of #p < .1, *p < .05, **p < .01.\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 311\n",
      "\n",
      "While these parameters themselves may be of interest, we can also use\n",
      "them to construct multistate life tables for specific subpopulations. In the\n",
      "previous section’s example, we considered the probability an individual would\n",
      "self-identify as a conservative Republican. This probability was computed by\n",
      "(1) producing a predicted value based on an established set of covariates and\n",
      "each sample of model parameters, (2) using posterior predictive simulation to\n",
      "simulate observations with the given characteristics, and (3) tallying the num-\n",
      "ber of simulated individuals who fell in the appropriate cell of the contingency\n",
      "table, based on their scores and the values of the thresholds that divide the\n",
      "continuous latent variable into the ordinal ones. In the example, the propor-\n",
      "tion of individuals that fell above the third threshold in both dimensions was\n",
      "the probability of being a conservative Republican. In the current example,\n",
      "we need to follow a similar procedure, but with a few differences.\n",
      "\n",
      "First, we must compute more than one proportion—we need a number of\n",
      "probabilities to construct the entire transition probability matrix needed as\n",
      "input for multistate life table generation. Second, rather than using posterior\n",
      "predictive simulation and computing the proportion of our draws that fall in\n",
      "various regions of the latent distribution, we will compute these probabilities\n",
      "directly using bivariate normal integration. Third, one of our covariates—\n",
      "whether an individual is healthy or unhealthy in 1987—is important for com-\n",
      "puting transition probabilities appropriately, and so, we will need to change\n",
      "this covariate’s value in order to obtain all the needed transition probabilities\n",
      "for a given age.\n",
      "\n",
      "The age-specific transition probability matrices for this three-state model\n",
      "(healthy, unhealthy, dead) are 3 × 3. The first row contains the conditional\n",
      "probabilities of transitioning from the healthy state to the healthy state (re-\n",
      "tention probability), from the healthy state to the unhealthy state, and from\n",
      "the healthy state to the deceased state. These probabilities can be obtained\n",
      "from the model parameter samples by setting the starting state covariate to\n",
      "0 and all other covariates to predetermined, desired values, and then comput-\n",
      "ing predicted values for each age [Xβ(1) in the health dimension; Xβ(2) in\n",
      "the mortality dimension]. We can then integrate a 0-mean bivariate normal\n",
      "distribution with appropriate correlation from the Gibbs sampler over the\n",
      "appropriate regions to obtain the desired transition probabilities for the first\n",
      "row of the transition probability matrix. These integrals are:\n",
      "\n",
      "p12 =\n",
      "∫ Xβ(1)\n",
      "−∞\n",
      "\n",
      "∫ ∞\n",
      "Xβ(2)\n",
      "\n",
      "BV N(0, ρ)\n",
      "\n",
      "p13 =\n",
      "∫ ∞\n",
      "−∞\n",
      "\n",
      "∫ Xβ(2)\n",
      "−∞\n",
      "\n",
      "BV N(0, ρ)\n",
      "\n",
      "p11 = 1− (p12 + p13).\n",
      "\n",
      "\n",
      "\n",
      "312 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "To obtain the transition probabilities for the unhealthy starting state,\n",
      "we simply need to change the starting state covariate to 1 (to represent an\n",
      "unhealthy starting state) and recompute these integrals as follows:\n",
      "\n",
      "p22 =\n",
      "∫ Xβ(1)\n",
      "−∞\n",
      "\n",
      "∫ ∞\n",
      "Xβ(2)\n",
      "\n",
      "BV N(0, ρ)\n",
      "\n",
      "p23 =\n",
      "∫ ∞\n",
      "−∞\n",
      "\n",
      "∫ Xβ(2)\n",
      "−∞\n",
      "\n",
      "BV N(0, ρ)\n",
      "\n",
      "p21 = 1− (p22 + p23).\n",
      "\n",
      "The final row of each transition probability matrix is fixed to [0 0 1] to\n",
      "represent that no transitions out of the deceased state can occur.\n",
      "\n",
      "Below is an R program that performs these computations and then con-\n",
      "verts the transition probability matrices into multistate life tables:\n",
      "\n",
      "ageints=9; n=5\n",
      "\n",
      "cv=matrix(c(0,1,1,1,16),ageints,5,byrow=T)\n",
      "\n",
      "x=matrix(1,ageints,9); x[,2]=seq(0,ageints); x[,5:9]=cv\n",
      "\n",
      "g<-as.matrix(read.table(\"c:\\\\mshaz.out\"))\n",
      "\n",
      "b=matrix(0,9,2)\n",
      "\n",
      "radix=c(.848,.152,0)\n",
      "\n",
      "mpower<-function(mat,power)\n",
      "\n",
      "{ma<-diag(3);for(i in 1:power){ma=ma%*%mat};return(ma)}\n",
      "\n",
      "for(m in 1001:2000){\n",
      "\n",
      "#read in parameter sample\n",
      "\n",
      "b[(1:9),1]=g[m,(2:10)]; b[(1:9),2]=g[m,(11:19)]\n",
      "\n",
      "rho=g[m,21]\n",
      "\n",
      "sig=matrix(c(1,rho,rho,1),2,2)\n",
      "\n",
      "#compute predicted values for transitions: start h\n",
      "\n",
      "x[,3]=0; x[,4]=0; hfb=x%*%b\n",
      "\n",
      "#compute predicted values for transitions: start u\n",
      "\n",
      "x[,3]=1; x[,4]=x[,2]; ufb=x%*%b\n",
      "\n",
      "#establish life table variables\n",
      "\n",
      "l<-array(0,c(ageints,3,3)); l[1,,]=diag(3)*radix\n",
      "\n",
      "bl<-matrix(0,ageints,2); tl<-matrix(0,ageints,2)\n",
      "\n",
      "#compute transition probabilities matrices\n",
      "\n",
      "for(a in 1:ageints){\n",
      "\n",
      "pmat[1,2]=pmvnorm(lower=c(-Inf,hfb[a,2]),upper=c(hfb[a,1],+Inf),\n",
      "\n",
      "mean=c(0,0),corr=sig)\n",
      "\n",
      "pmat[1,3]=pmvnorm(lower=c(-Inf,-Inf),upper=c(+Inf,hfb[a,2]),\n",
      "\n",
      "\n",
      "\n",
      "10.3 A multivariate probit model for generating distributions of multistate life tables 313\n",
      "\n",
      "mean=c(0,0),corr=sig)\n",
      "\n",
      "pmat[2,2]=pmvnorm(lower=c(-Inf,ufb[a,2]),upper=c(ufb[a,1],+Inf),\n",
      "\n",
      "mean=c(0,0),corr=sig)\n",
      "\n",
      "pmat[2,3]=pmvnorm(lower=c(-Inf,-Inf),upper=c(+Inf,ufb[a,2]),\n",
      "\n",
      "mean=c(0,0),corr=sig)\n",
      "\n",
      "pmat[1,1]=1-(pmat[1,2]+pmat[1,3])\n",
      "\n",
      "pmat[2,1]=1-(pmat[2,2]+pmat[2,3])\n",
      "\n",
      "#convert tp to m via Sylvester’s formula\n",
      "\n",
      "mmat=0\n",
      "\n",
      "lam2=(pmat[2,2]+pmat[1,1]+\n",
      "\n",
      "sqrt((pmat[2,2]+pmat[1,1])^2-\n",
      "\n",
      "4*(pmat[1,1]*pmat[2,2]-pmat[1,2]*pmat[2,1])))/2\n",
      "\n",
      "lam3=(pmat[2,2]+pmat[1,1]-\n",
      "\n",
      "sqrt((pmat[2,2]+pmat[1,1])^2-\n",
      "\n",
      "4*(pmat[1,1]*pmat[2,2]-pmat[1,2]*pmat[2,1])))/2\n",
      "\n",
      "mmat= (log(lam2)/((lam2-1)*(lam2-lam3)))*\n",
      "\n",
      "((pmat-diag(3))%*%(pmat-lam3*diag(3)))+\n",
      "\n",
      "(log(lam3)/((lam3-1)*(lam3-lam2)))*\n",
      "\n",
      "((pmat-diag(3))%*%(pmat-lam2*diag(3)))\n",
      "\n",
      "mmat=-mmat\n",
      "\n",
      "#compute lx and Lx for next age group\n",
      "\n",
      "if(a<ageints)\n",
      "\n",
      "{\n",
      "\n",
      "expm=diag(3);pyr=diag(3)\n",
      "\n",
      "for(j in 1:20)\n",
      "\n",
      "{\n",
      "\n",
      "expm=expm + ((-1)^j)*mpower(mmat,j)/factorial(j);\n",
      "\n",
      "pyr=pyr + ((-1)^j)*mpower(mmat,j)/factorial(j+1);\n",
      "\n",
      "}\n",
      "\n",
      "lx=l[a,,]%*%(expm)\n",
      "\n",
      "l[a+1,1,1]=sum(lx[,1]); l[a+1,2,2]=sum(lx[,2]); l[a+1,3,3]=0;\n",
      "\n",
      "blx=n*(l[a,,]%*%pyr)\n",
      "\n",
      "bl[a,1]=sum(blx[,1]); bl[a,2]=sum(blx[,2])\n",
      "\n",
      "}\n",
      "\n",
      "if(a==ageints)\n",
      "\n",
      "{\n",
      "\n",
      "blx=l[a,1:2,1:2]%*%solve(mmat[1:2,1:2])\n",
      "\n",
      "bl[a,1]=sum(blx[,1]); bl[a,2]=sum(blx[,2])\n",
      "\n",
      "}\n",
      "\n",
      "}\n",
      "\n",
      "le<-matrix(NA,ageints,2)\n",
      "\n",
      "for(a in 1:ageints){\n",
      "\n",
      "tl[a,1]=sum(bl[a:ageints,1]); tl[a,2]=sum(bl[a:ageints,2])\n",
      "\n",
      "le[a,]=tl[a,]/sum(l[a,,])\n",
      "\n",
      "\n",
      "\n",
      "314 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "}\n",
      "\n",
      "write(c(t(le)),file=\"c:\\\\lifetab.out\",append=T,ncolumns=(2*ageints))\n",
      "\n",
      "print(c(m,le[1,1],le[1,2]))\n",
      "\n",
      "}\n",
      "\n",
      "This program is fairly lengthy, and I will not discuss it in great depth.\n",
      "A number of variables are defined at the beginning, including the number of\n",
      "age intervals (9), the number of years in each age interval (5), and the values\n",
      "of the five fixed covariates (male, nonwhite, South, married, and years of\n",
      "schooling). These covariate values are repeated for nine lines in the x matrix,\n",
      "which contains incremented values of age. Next, the parameter file is read.\n",
      "Finally, the radix is established. The radix is the number or proportion of\n",
      "individuals in each state at the beginning of the first age in the life table.\n",
      "\n",
      "After defining a function that computes a matrix raised to a specified\n",
      "power, the loop begins to generate a life table for each post-burn-in sample\n",
      "from the Gibbs sampler. The parameters are read in one sample at a time and\n",
      "are combined with the covariate matrix, first with the starting state set to 0\n",
      "(to produce hfb) and then with the starting state set to 1 (to produce ufb).\n",
      "\n",
      "After obtaining these two 9 × 2 matrices of predicted values, several life\n",
      "table variables are established before the computation of the life tables begins.\n",
      "A loop across age is established to produce the life table. First, the age-specific\n",
      "transition probability matrix is computed. Second, this matrix is transformed\n",
      "into the hazard rate matrix using Sylvester’s formula to compute its log (see\n",
      "Singer and Spilerman 1976). The remainder of the program proceeds with\n",
      "typical steps in computing the number of individuals transitioning between\n",
      "states over the age interval and the number of person-years lived in each state\n",
      "during the interval (see Schoen 1988).\n",
      "\n",
      "As an example, I ran this program to produce life tables for married non-\n",
      "white females from the South with 16 years of education. The radix was es-\n",
      "tablished by predicting the starting state (healthy/unhealthy) using age, sex,\n",
      "race, region, marital status, and education, and then computing the predicted\n",
      "score, based on the covariate profile just described. Figure 10.9 shows trace\n",
      "plots of HLE, ULE, TLE, and the proportion of remaining life that can be\n",
      "expected to be lived healthy (HLE/TLE). These trace plots suggest that the\n",
      "life table quantities converged and mixed thoroughly, just as the parameters\n",
      "in the original Gibbs sampler did. Figure 10.10 shows histograms of these\n",
      "same quantities; all appear approximately normal.\n",
      "\n",
      "The samples of life table quantities can be summarized just as we would\n",
      "summarize other sample data. Mean TLE at age 45 for a person with the\n",
      "given covariate profile is 33.1 years, with a standard deviation of 1.5 years;\n",
      "an empirical interval estimate is [30.0, 35.9] years. The expected number of\n",
      "years remaining health was 25.4 years, with a standard deviation of 1.9 years;\n",
      "an empirical interval estimate is [21.7, 29.2] years. The remaining proportion\n",
      "of life to be spent healthy, based on these results, is .77, with an empirical\n",
      "interval estimate of [.68, .84]. If we wished to compare these results with those\n",
      "for a person with a different covariate profile, we would simply rerun the R\n",
      "\n",
      "\n",
      "\n",
      "10.4 Conclusions 315\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "2\n",
      "0\n",
      "\n",
      "2\n",
      "4\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "Sample\n",
      "\n",
      "H\n",
      "L\n",
      "\n",
      "E\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "Sample\n",
      "\n",
      "U\n",
      "L\n",
      "\n",
      "E\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "2\n",
      "8\n",
      "\n",
      "3\n",
      "2\n",
      "\n",
      "3\n",
      "6\n",
      "\n",
      "Sample\n",
      "\n",
      "T\n",
      "L\n",
      "\n",
      "E\n",
      "\n",
      "0 200 400 600 800\n",
      "\n",
      "0\n",
      ".6\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      ".7\n",
      "5\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "5\n",
      "\n",
      "Sample\n",
      "\n",
      "H\n",
      "L\n",
      "\n",
      "E\n",
      "/T\n",
      "\n",
      "L\n",
      "E\n",
      "\n",
      "Fig. 10.9. Trace plots of life table quantities computed from Gibbs samples of\n",
      "bivariate probit model parameters.\n",
      "\n",
      "program for a different covariate profile and then perform whatever sort of\n",
      "comparison we would like, treating the samples of life table quantities as we\n",
      "would a sample of parameters or a sample of data.\n",
      "\n",
      "10.4 Conclusions\n",
      "\n",
      "In this chapter, I have described the multivariate linear regression model and\n",
      "the multivariate probit model, perhaps two of the most important multivariate\n",
      "models that may be used in social science. Multivariate models are mathe-\n",
      "matically and computationally more complex than univariate models, and so\n",
      "I have paid particular attention to the reasons for using such models. As I\n",
      "\n",
      "\n",
      "\n",
      "316 10 Introduction to Multivariate Regression Models\n",
      "\n",
      "18 22 26 30\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "\n",
      "HLE\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "4 6 8 10 12\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "\n",
      "ULE\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "26 28 30 32 34 36 38\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "0\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "\n",
      "TLE\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "0.60 0.70 0.80 0.90\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "HLE/TLE\n",
      "\n",
      "F\n",
      "re\n",
      "\n",
      "q\n",
      "u\n",
      "\n",
      "e\n",
      "n\n",
      "\n",
      "cy\n",
      "\n",
      "Fig. 10.10. Histograms of life table quantitities computed from Gibbs samples of\n",
      "bivariate probit model parameters.\n",
      "\n",
      "have shown, the Bayesian approach to these particular models offers several\n",
      "benefits over a classical approach, especially when inference for auxilliary pa-\n",
      "rameters is of interest.\n",
      "\n",
      "This chapter should be considered simply an introduction to multivariate\n",
      "regression models, however, because we have not discussed many additional\n",
      "common and useful multivariate models like the multinomial probit model and\n",
      "simultaneous/structural equation models. Although simultaneous/structural\n",
      "equation models are substantially more difficult than the models presented\n",
      "here, the multinomial probit model is not inherently more difficult than the\n",
      "multivariate probit model (see Exercises). I recommend reading Bollen (1989),\n",
      "Hayduk (1987), Kaplan (2000) and Maruyama (1998) for an introduction to\n",
      "\n",
      "\n",
      "\n",
      "10.5 Exercises 317\n",
      "\n",
      "these models from a classical perspective. I recommend Lee and Zhu (2000)\n",
      "and Song and Lee (2002) for a Bayesian approach.\n",
      "\n",
      "10.5 Exercises\n",
      "\n",
      "1. Develop a simple rejection sampling scheme for sampling from selected\n",
      "regions of the three-dimensional (trivariate) normal distribution.\n",
      "\n",
      "2. Develop a Gibbs sampler for a multivariate model in which some of the\n",
      "outcomes are continuous and some are ordinal. How difficult is this process\n",
      "relative to constructing a full multivariate probit sampling algorithm?\n",
      "\n",
      "3. A polychoric correlation matrix is the correlation matrix between the la-\n",
      "tent variables in a multivariate probit model with no covariates. This type\n",
      "of matrix is commonly used as input for structural equation models when\n",
      "ordinal data are present. How could the Gibbs sampler for the multivari-\n",
      "ate probit model be modified to obtain the polychoric correlation matrix\n",
      "rather than regression coefficients and an error correlation matrix?\n",
      "\n",
      "4. The multinomial probit model differs only slightly from the multivari-\n",
      "ate probit model. In the multinomial probit model, the outcomes in each\n",
      "dimension are all dichotomous and are mutually exclusive; that is, an in-\n",
      "dividual can only take a 1 on (at most) a single outcome variable. The\n",
      "model is generally used in social sciences for predicting a nominal level\n",
      "outcome—the outcome variable is broken into a series of dummy variables\n",
      "with one omitted as the reference. The mutual exclusivity constraint leads\n",
      "to only two differences between the multinomial probit and the multivari-\n",
      "ate probit we have already discussed. First, a slightly different approach\n",
      "to sampling the latent data thought to underlie the observed response\n",
      "must be undertaken. Second, only one of the diagonal elements of the\n",
      "error covariance matrix must be constrained to 1 to identify the model.\n",
      "Regarding the simulation of the latent data, individuals are assumed to\n",
      "have latent traits, the maximum of which is the one for which the respon-\n",
      "dent’s observed outcome is “1.” That latent trait must be above 0. Latent\n",
      "traits for the other outcomes may also be above 0 but cannot be larger\n",
      "than that one. If an individual does not take a “1” value on any of the\n",
      "outcomes (i.e., his/her response is the reference outcome), all latent traits\n",
      "must be sampled from below 0. Develop a multinomial probit model al-\n",
      "gorithm and compare the results with those obtained using a multinomial\n",
      "logit procedure in another software package (see Imai and van Dyk 2005\n",
      "for an in-depth consideration of various extant MCMC approaches to this\n",
      "model).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "11\n",
      "\n",
      "Conclusion\n",
      "\n",
      "The goal of this book has been to describe a Bayesian approach to statis-\n",
      "tics, including the process of estimating parameters via Markov chain Monte\n",
      "Carlo (MCMC) methods, applied to models that are relatively common in so-\n",
      "cial science research. Throughout the text, I have referenced a small portion\n",
      "of a much larger body of literature on Bayesian theory, Bayesian modeling,\n",
      "and MCMC methodology that has emerged over the last two decades. I have\n",
      "largely limited these references to sources that I have found to be invaluable\n",
      "in my studies and that were the most important and directly relevant sources\n",
      "to the topic being addressed. Yet, I have barely scratched the surface of po-\n",
      "tential sources. In this concluding chapter, I present a number of books and\n",
      "articles that I recommend reading to gain a more in-depth understanding of\n",
      "the topics I have discussed throughout the book, as well as some topics that\n",
      "I have not addressed. Some of these references I have presented in previous\n",
      "chapters; some I have not. Here, I provide a condensed summary.\n",
      "\n",
      "As we have seen, the Bayesian approach to statistics involves (1) setting\n",
      "up a research question; (2) establishing a probability model for the data we\n",
      "intend to use to answer the question; (3) incorporating prior knowledge via the\n",
      "development of a prior distribution; (4) deriving the posterior distribution for\n",
      "the parameters, given the data and prior knowledge; (5) simulating samples of\n",
      "model parameters using MCMC methods; (6) evaluating the fit of the model\n",
      "and perhaps choosing a “best” model from among a variety of models; and\n",
      "(7) summarizing the parameter samples—including some that were derived\n",
      "from the original parameters—using basic descriptive statistics.\n",
      "\n",
      "From these steps it is clear that understanding the Bayesian approach\n",
      "and implementing it requires a solid understanding of probability theory, and\n",
      "so the second chapter of this book covered the foundations of probability\n",
      "theory and the mathematical approach to statistics required to complete a\n",
      "Bayesian analysis. As I stated in Chapter 2, my coverage of this material\n",
      "was sufficient for our purposes, but I recommend DeGroot (1986) and Rudas\n",
      "(2004) for a more in-depth presentation. Both of these texts are thorough, yet\n",
      "\n",
      "\n",
      "\n",
      "320 11 Conclusion\n",
      "\n",
      "very readable, and present a much broader view of probability theory than\n",
      "covered in this text.\n",
      "\n",
      "The latter part of Chapter 2 reviewed the classical approach to statistics,\n",
      "from a probability-theory standpoint, involving maximum likelihood estima-\n",
      "tion. Most graduate statistics courses and books present the basics of this\n",
      "estimation method, but I recommend Edwards (1992) for a more theoretical\n",
      "discussion of this approach (and a rejection of the Bayesian one!) and Eliason\n",
      "(1993) for greater detail on the actual mechanics involved in producing ML\n",
      "estimates.\n",
      "\n",
      "Chapter 3 provided a detailed presentation of Bayes’ Theorem for both\n",
      "point probabilities and probability distributions. In this process, I covered\n",
      "some of the key arguments that have been presented historically against the\n",
      "Bayesian approach to statistics. I am personally convinced that many of these\n",
      "arguments have been adquately resolved over the last few decades, if not the\n",
      "last century. Nonetheless, I recommend reading Jeffreys (1961) and, again, Ed-\n",
      "wards (1992) for the theoretical bases for and against the Bayesian paradigm.\n",
      "One of the key arguments against the Bayesian approach has centered on the\n",
      "use of prior distributions. I strongly recommend Gelman et al. (1995) and\n",
      "Gill (2002) for lengthier discussions of prior distributions, including how to\n",
      "make them noninformative so as to avoid the argument that priors corrupt\n",
      "results by introducing too much subjectivity into the model. In this book, I\n",
      "have largely avoided using highly informative priors. As I said in Chapter 3,\n",
      "most Bayesian analyses to date, especially in social science research, have used\n",
      "such noninformative priors to avoid the subjectivity criticism. I think this is\n",
      "an appropriate strategy, but I also believe that the use of informative priors\n",
      "will be useful in the future in formalizing the current state of knowledge and\n",
      "incorporating it into the model so as to make social science research truly cu-\n",
      "mulative. From my view, social science research currently incorporates prior\n",
      "knowledge informally via the literature review in the hypothesis construction\n",
      "and model selection process, but we often see the same hypotheses tested\n",
      "again and again, leading to a broadening of research questions and findings\n",
      "rather than an accumulation of them.\n",
      "\n",
      "Once one has decided to undertake a Bayesian analysis and has obtained\n",
      "a posterior distribution, the next step in the analysis is to summarize it. The\n",
      "summarization process is the primary area in which tremendous advances\n",
      "have been made in Bayesian statistics in the last few decades. Prior to 1990,\n",
      "Bayesian analyses often required extensive knowledge of numerical methods\n",
      "to approximate integrals necessary for adequately summarizing posterior den-\n",
      "sities. The emergence of the use of MCMC methods in statistics replaced the\n",
      "need for a stong background in numerical methods with a need for a back-\n",
      "ground in simulation methods. The goal of Chapters 4 through 6 was to pro-\n",
      "vide an introduction to these methods, specifically the Gibbs sampler and the\n",
      "random walk Metropolis algorithm. For an introduction to general simula-\n",
      "tion methods, I first recommend Ripley (1987). Next, I strongly recommend\n",
      "Gilks, Richardson, and Spiegelhalter (1996), the first major book introducing\n",
      "\n",
      "\n",
      "\n",
      "11 Conclusion 321\n",
      "\n",
      "MCMC methods in great detail. A number of additional books have appeared\n",
      "since then providing much more technical and theoretical detail specifically\n",
      "regarding the implementation of MCMC (and related) methods. Along these\n",
      "lines, I recommend Chen, Shao, and Ibrahim (2000), Liu (2001), and Robert\n",
      "and Casella (1999). These books are extremely detailed and provide advanced\n",
      "discussion of simulation approaches for various models. I also recommend\n",
      "Casella and George (1992) and Smith and Gelfand (1992) for concise, read-\n",
      "able discussions of MCMC methods.\n",
      "\n",
      "The actual summarization of the posterior distribution for the model pa-\n",
      "rameters, once a sample from it has been obtained or it has been determined\n",
      "that the density is a known form from which integrals can be analytically\n",
      "derived, is a straightforward process. In this book, we began describing the\n",
      "summarization of the posterior distribution in Chapter 3, and we continued\n",
      "throughout the remainder of the book as we covered some of the most common\n",
      "classes of models used in social science research.\n",
      "\n",
      "There is a large and growing market of introductory books on Bayesian\n",
      "statistics, and every one of them covers the process of Bayesian analysis from\n",
      "Bayesian model development through the process of summarization of the\n",
      "posterior distribution. Each introductory book, however, tends to have a dif-\n",
      "ferent selection of models covered as well as a slightly different emphasis. In\n",
      "this book, I have attempted to cover models that are most likely to be used\n",
      "by social science researchers using observational data on humans collected\n",
      "through social surveys. Nonetheless, such are not the only data analyzed by\n",
      "social scientists, and the models I discussed do not constitute an exhaustive\n",
      "set. Additionally, this book does not exhaust the possible approaches to in-\n",
      "troducing Bayesian statistics, and therefore, I suggest a number of additional\n",
      "introductory books.\n",
      "\n",
      "First, I recommend Gill (2002), which, to my knowledge is the only other\n",
      "extant book written for social scientists. Second, I cannot recommend Gelman\n",
      "et al. (1995) strongly enough. That book is now a classic text on Bayesian\n",
      "statistics and covers much more ground than I covered here, albeit at a more\n",
      "advanced level. I recommend Box and Tiao (1973) as a classic introductory\n",
      "book written at an advanced level focused primarily on Bayesian theory.\n",
      "Along those lines, I recommend Lee (1989), and Robert (1994), which are\n",
      "also somewhat theoretical but written at a less advanced level. In addition to\n",
      "these books, I suggest Congdon (2001 and 2003) for highly applied introduc-\n",
      "tions written at an advanced level. Finally, I recommend Leonard and Hsu\n",
      "(1999) and Sivia (1996), both of which are very readable introductions to the\n",
      "Bayesian approach to statistics.\n",
      "\n",
      "Noticeably absent from this book is an introduction to time series analysis,\n",
      "despite the fact that several of my examples suggested (if not begged for) a\n",
      "time series modeling approach. With the growth in the availability of longi-\n",
      "tudinal data, both in terms of panel data and repeated cross-sectional data,\n",
      "time series analyses have become increasingly important in social science over\n",
      "the last several decades. I recommend Hamilton (1994) for an exposition of\n",
      "\n",
      "\n",
      "\n",
      "322 11 Conclusion\n",
      "\n",
      "the classical approach to time series analysis. Additionally, I recommend Alho\n",
      "and Spencer (2005) for a statistical demographic approach to time series, and\n",
      "I suggest Pole, West, and Harrison (1994) for a Bayesian appproach to time\n",
      "series models.\n",
      "\n",
      "Although this book has been written as an introduction to Bayesian statis-\n",
      "tics, many of the models presented can be—and generally are!—estimated via\n",
      "maximum likelihood. In many cases I have compared Bayesian results with\n",
      "those that would be obtained via maximum likelihood approaches. The point\n",
      "of the book has been to show the flexibility and advantages of the Bayesian\n",
      "approach using models that are familiar to social scientists. As we have seen,\n",
      "the primary advantages of a Bayesian approach to modeling include the flexi-\n",
      "bility of altering the model to suit your needs, the ease of estimating quantities\n",
      "that are not directly estimated in the model, the ease of conducting inference\n",
      "and interpreting the results, and the ability to diagnose problems in models\n",
      "in which maximum likelihood solutions are neither satisfactory nor sufficient.\n",
      "I hope that now, after having read this book, you will agree with these con-\n",
      "clusions and begin to explore the vast and growing literature on Bayesian\n",
      "statistics and consider using a Bayesian approach in your own research.\n",
      "\n",
      "\n",
      "\n",
      "A\n",
      "\n",
      "Background Mathematics\n",
      "\n",
      "Although mathematics and statistics are different disciplines, a general back-\n",
      "ground in mathematics and mathematical concepts is important for a solid\n",
      "understanding of statistics. In this appendix, I summarize the basic concepts\n",
      "from calculus and matrix algebra that are necessary to understand mathemat-\n",
      "ical statistics. In addition to the basic summary of calculus presented below,\n",
      "I recommend Thompson 1946, a classic, condensed calculus text. In addition\n",
      "to my summary of matrix algebra, I recommend Harville 1997. For additional\n",
      "math commonly used throughout the book, I recommend Hagle 1996.\n",
      "\n",
      "A.1 Summary of calculus\n",
      "\n",
      "Calculus can be divided into two halves: differential and integral calculus. In\n",
      "a nutshell, differential calculus is concerned with slopes of curves and rates\n",
      "of change. Integral calculus, on the other hand, is concerned with area under\n",
      "and between curves.\n",
      "\n",
      "A.1.1 Limits\n",
      "\n",
      "Central to both branches of calculus is the notion of “limits.” A limit is the\n",
      "result a function returns as some quantity in the function approaches some\n",
      "value. For example, the limit of the function f(x) = 1/x as x approaches\n",
      "infinity is 0. In calculus notation we write this expression as:\n",
      "\n",
      "lim\n",
      "x→∞\n",
      "\n",
      "1\n",
      "x\n",
      "\n",
      "= 0. (A.1)\n",
      "\n",
      "The concept of a limit is fairly intuitive. If we imagine in this example that\n",
      "x = 1, then the function value is 1. If x = 2, then the function value is smaller,\n",
      "at 1/2. As x gets larger, the fraction gets smaller, so that, as x approaches\n",
      "the largest value possible (∞), the fraction’s value approaches 0. Evaluating\n",
      "limits can be quite difficult, and a good part of a first course in calculus is\n",
      "\n",
      "\n",
      "\n",
      "324 A Background Mathematics\n",
      "\n",
      "spent learning how to determine limits when expressions are more complex.\n",
      "For example, imagine if the limit in Equation A.1 were taken as x → 0.\n",
      "\n",
      "A.1.2 Differential calculus\n",
      "\n",
      "With the basic concept of a limit defined, the two main branches of calculus\n",
      "can be developed. Differential calculus is essentially concerned with slopes of\n",
      "curves. When one considers the slope of a line, one is referring to a constant:\n",
      "A line has a given slope, and that slope does not change regardless of the\n",
      "location on the line at which the slope is evaluated. In contrast, the slope of a\n",
      "curve varies depending on the point on the curve where the slope is evaluated.\n",
      "In defining the slope of a curve, it is thus necessary to define a tangent line. A\n",
      "tangent line is a line that intersects a curve at a single point (see Figure A.1).\n",
      "\n",
      "−4 −2 0 2 4\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "0\n",
      "−\n",
      "\n",
      "5\n",
      "0\n",
      "\n",
      "5\n",
      "1\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "5\n",
      "2\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "5\n",
      "\n",
      "x\n",
      "\n",
      "y\n",
      "\n",
      "Tangent Line at x=1\n",
      "\n",
      "x=1\n",
      "\n",
      "dy/dx=2\n",
      "\n",
      "Fig. A.1. Generic depiction of a curve and a tangent line at an arbitrary point.\n",
      "\n",
      "When we want to determine the slope of a curve, we are referring to the\n",
      "slope of the tangent line at that point. Recall from basic algebra that a line\n",
      "(and, consequently, its slope) is determined by two points. Imagine, then, that\n",
      "\n",
      "\n",
      "\n",
      "A.1 Summary of calculus 325\n",
      "\n",
      "we want to evaluate the slope of a curve, say the curve y = x2, at the location\n",
      "x = 1 (thus, y = 1). The curve y = x2 is a parabola, centered at the origin\n",
      "and increasing as x moves away from 0. In order to determine the slope of\n",
      "the curve, we could begin by taking two points on the curve, say where x = 0\n",
      "and x = 2, and then evaluating the slope of the line determined by these\n",
      "points. This approach would give an approximation to the slope of the curve\n",
      "at x = 1. Where x = 0, y = 0, and where x = 2, y = 4. The slope of a\n",
      "line can be computed by taking the ratio of the change in y to the change in\n",
      "x. Such changes can be denoted as ∆y and ∆x, respectively, with the ratio\n",
      "being expressed as dy/dx in calculus. Thus, the slope of this line is dy/dx = 2.\n",
      "However, what two coordinates should we use to determine the slope of the\n",
      "curve at a point? If we change coordinates, we will most likely change the\n",
      "estimate of the slope of the tangent line at the point of interest. It makes\n",
      "sense that the further we get away from the x coordinate we are examining,\n",
      "the worse the estimate will be; hence, perhaps we should get as close to the\n",
      "desired x coordinate as possible (i.e., move from x = 0 and x = 2 to x = .99\n",
      "and x = 1.01). In calculus, using limits, we can define this “closing in on x”\n",
      "as:\n",
      "\n",
      "slope of curve at x ≡\n",
      "dy\n",
      "\n",
      "dx\n",
      "= lim\n",
      "\n",
      "∆x→0\n",
      "\n",
      "f(x + ∆x)− f(x)\n",
      "∆x\n",
      "\n",
      ". (A.2)\n",
      "\n",
      "Equation A.2 says that the slope of the curve—called a derivative—evaluated\n",
      "at x is the limit of the slope of the line created by taking two points on either\n",
      "side of x, as those points get infinitely close to x. This equation is intuitively\n",
      "simple—it is simply a generic slope formula—but it may be analytically com-\n",
      "plex. In this equation, for example, without some additional contortions, if\n",
      "∆x goes to 0, the equation explodes, given the division by 0 problem. Thus,\n",
      "a fair amount of differential calculus is spent learning “tricks” to manipulate\n",
      "equations so that this limit can be evaluated. Fortunately, for many com-\n",
      "mon functions, there are simple rules for taking derivatives. For example, the\n",
      "derivative of a constant C is 0 (which makes sense, because y = C is a flat\n",
      "line with slope 0). Another common rule is that the derivative of the function\n",
      "f(x) = xn is nxn−1 (see the end of this section for the proof). Thus, using\n",
      "calculus notation, the equation for the slope of our example curve y = x2 is:\n",
      "\n",
      "dy\n",
      "\n",
      "dx\n",
      "≡\n",
      "\n",
      "df(x)\n",
      "dx\n",
      "\n",
      "= 2x.\n",
      "\n",
      "The expressions on the left of the equal sign are interchangeable and are\n",
      "simply a slope formula (change in y over change in x). Notice how the deriva-\n",
      "tive here was taken by placing the exponent in front of x and reducing the\n",
      "exponent by 1 (x1 = x). Notice also that this equation for the slope is general\n",
      "to this curve: All one must do to determine the slope of this curve is to sub-\n",
      "stitute an x value. The derivative function will return the slope of the curve\n",
      "at that point. If our original equation were for a line, y = mx + b, then the\n",
      "derivative would be m, which is the slope of the line (recall the slope-intercept\n",
      "\n",
      "\n",
      "\n",
      "326 A Background Mathematics\n",
      "\n",
      "form from algebra), and the m is constant across all values of x. This result\n",
      "should be intuitive: As stated above, the slope of a line is constant regardless\n",
      "of where it is evaluated.\n",
      "\n",
      "This “first” derivative of a curve can not only be considered a slope of\n",
      "the curve, but it can also be considered a rate of change, since the slope\n",
      "tells the rate of change in y per unit change in x. There are higher order\n",
      "rates of change (i.e., change in the rate of change—also called acceleration).\n",
      "Differential calculus involves not only first derivatives, but also higher order\n",
      "derivatives, which are computed by taking the derivative of a derivative. In\n",
      "theory, there is virtually no limit to the number of derivatives of a function\n",
      "that can be taken, except often second, third, and higher order derivatives are\n",
      "constant.\n",
      "\n",
      "Differential calculus in dimensions greater than two is also concerned with\n",
      "“partial” derivatives, or the slope of a curve in one of the multiple dimen-\n",
      "sions. Partial derivatives tend to be fairly simple to compute, because when\n",
      "taking a partial derivative, one treats the variables representing other dimen-\n",
      "sions as constants. Whereas full derivatives are denoted using dy/dx, partial\n",
      "derivatives are denoted using the ∂ symbol: ∂y/∂x.\n",
      "\n",
      "Derivatives are a fundamental part of likelihood analysis in statistics. Max-\n",
      "imizing a likelihood function requires finding the point at which the likelihood\n",
      "curve’s derivative is 0, because a tangent line with slope 0 indicates a flat line,\n",
      "implying a point on the curve that is either a maximum or a minimum. In the\n",
      "parabola example, the minimum is reached where x = 0.\n",
      "\n",
      "Second derivatives are also useful in classical statistics. The second deriva-\n",
      "tive represents the rate of curvature in a curve. Given that the density func-\n",
      "tion for a probability distribution is simply a mathematical curve, the second\n",
      "derivative reveals whether the curve has steep or shallow curvature, implying\n",
      "either smaller or greater variance (see Chapter 2).\n",
      "\n",
      "A.1.3 Integral calculus\n",
      "\n",
      "The other branch of calculus, integral calculus, is concerned in general with\n",
      "finding area under and between curves. In probability modeling, finding the\n",
      "area under curves is crucial. Indeed, the key requirement for a mathematical\n",
      "curve to constitute a probability density function is that the area under the\n",
      "entire curve totals 1.\n",
      "\n",
      "Finding the area under a line segment is easy, because there are simple\n",
      "formulas for computing areas of rectangles and triangles. However, finding\n",
      "the area under a curve is more difficult, because there are no simple algebraic\n",
      "formulas for areas involving most curves. Imagine, for example, that one is\n",
      "presented with the curve y = −x2 + 3 and wishes to find the area under it.\n",
      "This curve is an inverted parabola, with its vertex located at (0,3). The curve\n",
      "intersects the x axis in two places (where y = 0, x = ±\n",
      "\n",
      "√\n",
      "3 ≈ 1.73).\n",
      "\n",
      "We could divide the curve into two halves, create rectangles that closely\n",
      "fit the area under the curve, and approximate the area under the curve using\n",
      "\n",
      "\n",
      "\n",
      "A.1 Summary of calculus 327\n",
      "\n",
      "the area of the rectangles (see Figure A.2 for a depiction of this curve). This\n",
      "estimated area would approximate the area under the curve, but there would\n",
      "be considerable error. The rectangles, if they joined at x = 0 with a height of\n",
      "3, would include a considerable amount of area that is above the curve where\n",
      "x < 0 and x > 0. Thus, an alternative might be to consider breaking the\n",
      "curve into more than two sections and adjusting the height of each rectangle\n",
      "to more closely match the height of the curve at the rectangle’s position on\n",
      "the x axis. However, unless the rectangles are infinitely small, there would\n",
      "always been some overestimation or underestimation of the area. Again, refer\n",
      "to Figure A.2. The upper plot shows this inverted parabola with its area under\n",
      "the curve being approximated using two rectangles. With two rectangles, there\n",
      "is considerable overestimation of the area under the curve. The middle plot\n",
      "shows the approximation with four rectangles; here there is substantially less\n",
      "overestimation. Finally, the bottom plot shows the approximation with eight\n",
      "rectangles. There is substantially less overestimation of the area with eight\n",
      "rectangles than with four.\n",
      "\n",
      "Once again, the notion of a limit is crucial. If we let the rectangles get\n",
      "infinitely small, then we can reduce the error in approximation to 0. Hence, a\n",
      "solution to finding the area under the curve is to use the following formula:\n",
      "\n",
      "lim\n",
      "n→∞\n",
      "\n",
      "n∑\n",
      "i=1\n",
      "\n",
      "Ai,\n",
      "\n",
      "where Ai is the area of the ith rectangle under the curve.1 In this expression,\n",
      "as n approaches infinity, the sum becomes called an “integral,” and is denoted\n",
      "with the symbol\n",
      "\n",
      "∫\n",
      ". Thus, integrals can be considered the continuous analog\n",
      "\n",
      "of discrete summation using\n",
      "∑\n",
      "\n",
      ". Area under curves can be computed for the\n",
      "entire curve, or they can be computed for only sections of a curve. For example,\n",
      "the expression:\n",
      "\n",
      "∫ √3\n",
      "0\n",
      "\n",
      "(−x2 + 3)dx\n",
      "\n",
      "implies that we are interested in the area under the curve between the limits of\n",
      "x = 0 and x =\n",
      "\n",
      "√\n",
      "3. Notice the “dx” placed at the end of the expression—this\n",
      "\n",
      "quantity is the width of the rectangles being summed, and the remainder of\n",
      "the expression is the height.\n",
      "\n",
      "The fundamental theorem of calculus relates integrals and derivatives,\n",
      "showing that they are inverse functions of one another. In brief, the theo-\n",
      "rem says:\n",
      "\n",
      "d\n",
      "∫\n",
      "\n",
      "f(x)\n",
      "dx\n",
      "\n",
      "= f(x). (A.3)\n",
      "\n",
      "1 In fact, one can achieve closer approximation with a finite n if one uses trapezoids\n",
      "or other shapes that more closely match the shape of the curve at x.\n",
      "\n",
      "\n",
      "\n",
      "328 A Background Mathematics\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "2 rectangles\n",
      "\n",
      "y\n",
      "error\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "4 rectangles\n",
      "\n",
      "y\n",
      "\n",
      "reduction in error\n",
      "\n",
      "−3 −2 −1 0 1 2 3\n",
      "\n",
      "−\n",
      "1\n",
      "\n",
      "1\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "8 rectangles\n",
      "\n",
      "y\n",
      "\n",
      "even more reduction in error\n",
      "\n",
      "Fig. A.2. Finding successive approximations to the area under a curve using rect-\n",
      "angles.\n",
      "\n",
      "In other words, the derivative of an integral of a function is simply the\n",
      "function itself. Thus, if we know the derivative of some function, we also\n",
      "know the integral of some other function. For example, above we saw that the\n",
      "derivative of xn was nxn−1. Using the fundamental theorem, we can invert\n",
      "this process and find that the integral of xn is xn+1/(n + 1). Technically, the\n",
      "integral also involves adding a constant C, because, given that the derivative of\n",
      "a constant is 0, we never know what constant should be added when inverting\n",
      "the derivative (that is, taking an integral).\n",
      "\n",
      "\n",
      "\n",
      "A.1 Summary of calculus 329\n",
      "\n",
      "Without going into all the details of integration, we evaluate integrals\n",
      "by (1) taking the integral and (2) evaluating the integral at the limits of\n",
      "integration. In our current example:\n",
      "\n",
      "∫ √3\n",
      "0\n",
      "\n",
      "(\n",
      "−x2 + 3\n",
      "\n",
      ")\n",
      "dx =\n",
      "\n",
      "−x3\n",
      "\n",
      "3\n",
      "+ 3x\n",
      "\n",
      "∣∣∣∣\n",
      "√\n",
      "\n",
      "3\n",
      "\n",
      "0\n",
      "\n",
      "=\n",
      "−\n",
      "(√\n",
      "\n",
      "3\n",
      ")3\n",
      "\n",
      "3\n",
      "+ 3\n",
      "\n",
      "(√\n",
      "3\n",
      ")\n",
      "− 0\n",
      "\n",
      "= 2\n",
      "√\n",
      "\n",
      "3.\n",
      "\n",
      "The first line shows the integral of the function. The bar to the right\n",
      "indicates that these are the limits at which to evaluate this integral. The\n",
      "third expression shows the value of the function after evaluating it at these\n",
      "limits, and the final expression is the area under the curve between 0 and\n",
      "the\n",
      "\n",
      "√\n",
      "3. Notice how the function value at the lower limit of integration is\n",
      "\n",
      "subtracted from the value of the function at the upper limit of integration.\n",
      "This result should make sense: If you imagine taking all the area under the\n",
      "curve to the left of\n",
      "\n",
      "√\n",
      "3 and then subtracting all the area under the curve to\n",
      "\n",
      "the left of 0, what remains is the area between the two points. This result\n",
      "should be somewhat familiar, if you have taken a basic statistics course and\n",
      "worked with finding the area (or probability) under the standard normal (z)\n",
      "curve between two points via subtraction.\n",
      "\n",
      "Integrals can become very complex, especially when dealing with curves\n",
      "of high dimension (e.g., as are used in multivariate probability distributions).\n",
      "Just as there are partial derivatives, there are partial integrals. Integrals can\n",
      "also become complex when there is not a simple expression for the integral.\n",
      "Indeed, some curves have no analytically tractable integrals (for example, the\n",
      "normal distribution curve has no closed form solution for its integral). In these\n",
      "cases, other methods must be used to evaluate the integral, including using\n",
      "sums of the area of tiny rectangles (or trapezoids).\n",
      "\n",
      "A.1.4 Finding a general rule for a derivative\n",
      "\n",
      "Above, I said that the derivative of y = xn is nxn−1. We know this result is\n",
      "true generally by applying Equation A.2. Suppose our equation is y = xn +C,\n",
      "where C is an arbitrary constant. Derivatives of sums are equal to the sum\n",
      "of the derivatives, so we can take the derivative of xn separately from that of\n",
      "C. Since the derivative of a constant is 0, we can disregard it and focus only\n",
      "on xn. Following Equation A.2, we have:\n",
      "\n",
      "dy\n",
      "\n",
      "dx\n",
      "= lim\n",
      "\n",
      "∆x→0\n",
      "\n",
      "(x + ∆x)n − xn\n",
      "\n",
      "∆x\n",
      ". (A.4)\n",
      "\n",
      "Using the rule for binomial expansion, (x + ∆x)n can be expanded as:\n",
      "\n",
      "\n",
      "\n",
      "330 A Background Mathematics\n",
      "\n",
      "(\n",
      "n\n",
      "0\n",
      "\n",
      ")\n",
      "xn∆x0 +\n",
      "\n",
      "(\n",
      "n\n",
      "1\n",
      "\n",
      ")\n",
      "xn−1∆x1 + . . . +\n",
      "\n",
      "(\n",
      "n\n",
      "\n",
      "n− 1\n",
      "\n",
      ")\n",
      "x1∆xn−1 +\n",
      "\n",
      "(\n",
      "n\n",
      "n\n",
      "\n",
      ")\n",
      "x0∆xn.\n",
      "\n",
      "Notice that the first term of this expression reduces to xn, and thus, when\n",
      "it is included in Equation A.4, it cancels the xn that is subtracted at the end\n",
      "in the numerator. So, we are left with a series of terms containing powers of\n",
      "∆x:\n",
      "\n",
      "dy\n",
      "\n",
      "dx\n",
      "= lim\n",
      "\n",
      "∆x→0\n",
      "\n",
      "(\n",
      "n\n",
      "1\n",
      "\n",
      ")\n",
      "xn−1∆x1 + . . . +\n",
      "\n",
      "(\n",
      "n\n",
      "n\n",
      "\n",
      ")\n",
      "x0∆xn\n",
      "\n",
      "∆x\n",
      ".\n",
      "\n",
      "Because all the terms in the numerator contain at least one ∆x, we can factor\n",
      "out one from each part of the numerator and cancel it with the denominator,\n",
      "and we are left with:\n",
      "\n",
      "dy\n",
      "\n",
      "dx\n",
      "= lim\n",
      "\n",
      "∆x→0\n",
      "\n",
      "(\n",
      "n\n",
      "1\n",
      "\n",
      ")\n",
      "xn−1 + . . . +\n",
      "\n",
      "(\n",
      "n\n",
      "n\n",
      "\n",
      ")\n",
      "x0∆xn−1. (A.5)\n",
      "\n",
      "Allowing ∆x to go to 0 in Equation A.5 eliminates all the terms in the\n",
      "equation except for the first term, which is nxn−1, the rule presented above.\n",
      "Other general rules for derivatives are derived in a similar fashion and are\n",
      "usually presented in a table in a calculus text.\n",
      "\n",
      "A.2 Summary of matrix algebra\n",
      "\n",
      "Matrix algebra is a form of mathematics that allows compact notation for, and\n",
      "mathematical manipulation of, high-dimensional expressions and equations.\n",
      "For the purposes of this book, only a relatively simple exposition is required in\n",
      "order to understand the notation for multivariate equations and calculations.\n",
      "\n",
      "A.2.1 Matrix notation\n",
      "\n",
      "The basic unit in matrix algebra is a matrix, generally expressed as:\n",
      "\n",
      "A =\n",
      "\n",
      "\n",
      "a11 a12 a13a21 a22 a23\n",
      "\n",
      "a31 a32 a33\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "Here, the matrix A is denoted as a matrix by the boldfaced type. Through-\n",
      "out the text, however, I use capitalized letters to denote matrices, rather than\n",
      "boldface, for the sake of appearance. Matrices can be of any dimension; in\n",
      "this example, the matrix is a “3-by-3” or “3× 3” matrix. The number of rows\n",
      "is listed first; the number of columns is listed second. The subscripts of the\n",
      "matrix elements (a’s) clarify this: The third item in the second row is element\n",
      "\n",
      "\n",
      "\n",
      "A.2 Summary of matrix algebra 331\n",
      "\n",
      "a23. A matrix with only one element (i.e., 1×1 dimension) is called a “scalar.”\n",
      "A matrix with only a single column is called a column vector; a matrix with\n",
      "only a single row is called a row vector. The term “vector” also has meaning\n",
      "in analytic geometry, referring to a line segment that originates at the origin\n",
      "(0, 0, . . . , 0) and terminates at the coordinates listed in the k dimensions. For\n",
      "example, you are already familiar with the Cartesian coordinate (4, 5), which\n",
      "is located 4 units from 0 in the x dimension and 5 units from 0 in the y dimen-\n",
      "sion. The vector [4, 5], then, is the line segment formed by taking a straight\n",
      "line from (0, 0) to (4, 5).\n",
      "\n",
      "A.2.2 Matrix operations\n",
      "\n",
      "The first important operation that can be performed on a matrix (or vector) is\n",
      "the transpose function, which is denoted as: A′ or AT . The transpose function\n",
      "reverses the rows and columns of a matrix so that if B = AT :\n",
      "\n",
      "bij = aji,∀ i, j. (A.6)\n",
      "\n",
      "This equation says that the ijth element of the transposed matrix is the jith\n",
      "\n",
      "element of the original matrix for all i = 1 . . . I and j = 1 . . . J elements.\n",
      "The dimensionality of a transposed matrix, therefore, is the opposite of the\n",
      "original matrix. For example, if matrix B is 3× 2, then matrix BT will be of\n",
      "dimension 2× 3. The transpose of the multiple of two matrices, say (AB)T , is\n",
      "simply the multiple of the transposes of the original matrices, only in reverse.\n",
      "Thus, (AB)T = BT AT .\n",
      "\n",
      "With this basic function developed, we can now discuss other matrix oper-\n",
      "ations, including matrix addition, subtraction, and multiplication (including\n",
      "division). Matrix addition and subtraction are relatively simple. Provided two\n",
      "matrices have the same dimensionality (i.e., they “conform” for addition),\n",
      "adding or subtracting two matrices is performed by simply adding and sub-\n",
      "tracting corresponding elements in the two matrices:\n",
      "\n",
      "A + B =\n",
      "[\n",
      "\n",
      "a11 a12\n",
      "a21 a22\n",
      "\n",
      "]\n",
      "+\n",
      "[\n",
      "\n",
      "b11 b12\n",
      "b21 b22\n",
      "\n",
      "]\n",
      "=\n",
      "[\n",
      "\n",
      "(a11 + b11) (a12 + b12)\n",
      "(a21 + b21) (a22 + b22)\n",
      "\n",
      "]\n",
      ". (A.7)\n",
      "\n",
      "The commutative property of addition and subtraction that holds in scalar\n",
      "algebra also holds in matrix algebra: The order in which matrices are added (or\n",
      "subtracted) makes no difference to the outcome, so that A+B+C = C+B+A.\n",
      "\n",
      "Matrix multiplication is slightly more difficult than addition and subtrac-\n",
      "tion, unless one is multiplying a matrix by a scalar. In that case, the scalar\n",
      "is distributed to each element in the matrix, and multiplication is carried out\n",
      "element by element:\n",
      "\n",
      "k\n",
      "\n",
      "[\n",
      "a11 a12\n",
      "a21 a22\n",
      "\n",
      "]\n",
      "=\n",
      "[\n",
      "\n",
      "ka11 ka12\n",
      "ka21 ka22\n",
      "\n",
      "]\n",
      ". (A.8)\n",
      "\n",
      "\n",
      "\n",
      "332 A Background Mathematics\n",
      "\n",
      "In the event two matrices are being multiplied, before multiplying, one\n",
      "must make sure the matrices conform for multiplication, where “conform”\n",
      "means that the number of columns in the first matrix must equal the number\n",
      "of rows in the second matrix. For example, one cannot post-multiply a 2× 3\n",
      "matrix A by another 2 × 3 matrix B, because the number of columns in A\n",
      "is 3, while the number of rows in B is 2. One could, however, multiply A by\n",
      "a 3 × 2 matrix C. The matrix that results from multiplying A and C would\n",
      "have dimension 2 × 2 (the same number of rows as the first matrix and the\n",
      "same number of columns as the second matrix).\n",
      "\n",
      "The general rule for matrix multiplication is as follows: If one is multiplying\n",
      "A× C to obtain matrix D, then:\n",
      "\n",
      "dij =\n",
      "K∑\n",
      "\n",
      "k=1\n",
      "\n",
      "aikckj , ∀ i, j. (A.9)\n",
      "\n",
      "That is, the ijth element of matrix D is equal to the sum of the multiple\n",
      "of the elements in row i of A and the column j of C. Matrix multiplication is\n",
      "therefore a fairly tedious process. As an example, assume A is 2× 3 and C is\n",
      "3× 2, with the following elements:\n",
      "\n",
      "A =\n",
      "[\n",
      "\n",
      "1 2 3\n",
      "4 5 6\n",
      "\n",
      "]\n",
      ", C =\n",
      "\n",
      "\n",
      "1 23 4\n",
      "\n",
      "5 6\n",
      "\n",
      "\n",
      " .\n",
      "\n",
      "Then, element d11 = (1 × 1) + (2 × 3) + (3 × 5) = 22, and the entire D\n",
      "matrix is (solve this yourself):\n",
      "\n",
      "D =\n",
      "[\n",
      "\n",
      "22 28\n",
      "49 64\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "Notice that matrix D is 2× 2.\n",
      "Unlike matrix addition and subtraction, in which the order of the matrices\n",
      "\n",
      "is irrelevant, order matters for multiplication. Obviously, given the conforma-\n",
      "bility requirement, reversing the order of matrices may make multiplication\n",
      "impossible (e.g., although a 3 × 2 matrix can be post-multiplied by a 2 × 4\n",
      "matrix, the 2×4 matrix cannot be post-multiplied by the 3×2 matrix). How-\n",
      "ever, even if matrices are conformable for multiplication after reversing their\n",
      "order, the resulting matrices will not generally be identical. For example, a\n",
      "1×k row vector multiplied by a k×1 column vector will yield a scalar (1×1),\n",
      "but if we reverse the order of multiplication, we will obtain a k × k matrix.\n",
      "\n",
      "Some additional functions that apply to matrices and are commonly used\n",
      "in statistics include the trace operator [the trace of A is denoted as tr(A)],\n",
      "the determinant, and the inverse. The trace of a matrix is simply the sum\n",
      "of the diagonal elements of the matrix. The determinant is more difficult.\n",
      "Technically, the determinant is the sum of the signed multiples of all the\n",
      "permutations of a matrix, where the term “permutations” refers to the unique\n",
      "\n",
      "\n",
      "\n",
      "A.2 Summary of matrix algebra 333\n",
      "\n",
      "combinations of a single element from each row and column, for all rows\n",
      "and columns. If d denotes the dimensionality of a matrix, then there are d!\n",
      "permutations for the matrix. For example, in a 3 × 3 matrix, there are a\n",
      "total of 6 permutations (3! = 3 × 2 × 1 = 6): (a11, a22, a33), (a12, a23, a31),\n",
      "(a13, a21, a32), (a13, a22, a31), (a11, a23, a32), (a12, a21, a33). Notice how\n",
      "for each combination, there is one element from each row and column. The\n",
      "signing of each permutation is determined by the column position of each\n",
      "element in all the pairs that can be constructed using the elements of the\n",
      "permutation, and the subscript of element at each position in each pair. For\n",
      "example, the permutation (a11, a22, a33) has elements from columns 1, 2,\n",
      "and 3. The possible ordered (i, j) pairs that can come from this permutation\n",
      "include (1, 2), (1, 3), and (2, 3) (based on the column position). If there are an\n",
      "even number of (i, j) pairs in which i > j, then the permutation is considered\n",
      "“even” and takes a positive sign. Otherwise, the permutation is considered\n",
      "“odd” and takes a negative sign. In this example, there are 0 pairs in which\n",
      "i > j, so the permutation is even (0 is even). However, in the permutation\n",
      "(a13, a22, a31), the pairs are (3, 2), (3, 1), and (2, 1). In this set, all three pairs\n",
      "are such that i > j; hence this permutation is odd and takes a negative sign.\n",
      "The determinant is denoted using absolute value bars on either side of the\n",
      "matrix name: for instance, the determinant of A is denoted as |A|, or often,\n",
      "det(A).\n",
      "\n",
      "For 2× 2 and 3× 3 matrices, determinants can be calculated fairly easily.\n",
      "For example, the determinant of a 2 × 2 matrix A is: det(A) = a11a22 −\n",
      "a12a21. However, for larger matrices, the number of permutations becomes\n",
      "large rapidly. Fortunately, several rules simplify the process. First, if any row\n",
      "or column in a matrix is a vector of 0, then the determinant is 0. In that\n",
      "case, the matrix is said not to be of full rank. Second, the same is true if\n",
      "any two rows or columns is identical. Third, for a diagonal matrix (i.e., there\n",
      "are 0’s everywhere but the main diagonal—the 11, 22, 33,... positions), the\n",
      "determinant is only the multiple of the diagonal elements. There are additional\n",
      "rules, but they are not necessary for this brief introduction. I note that the\n",
      "determinant is essentially a measure of the area/volume/hypervolume spanned\n",
      "by the vectors of the matrix. This helps, I think, to clarify why matrices with\n",
      "0 vectors in them have determinant 0: Just as in two dimensions a line has\n",
      "no area, when we have a 0 vector in a matrix, the dimensionality of the figure\n",
      "spanned by the vectors of a matrix is reduced by a dimension (because one\n",
      "vector does not pass the origin), and hence the hypervolume is necessarily 0.\n",
      "\n",
      "Finally, a very important function in matrix algebra is the inverse function.\n",
      "The inverse function allows the matrix equivalent of division. In a sense, just\n",
      "as 5 times its inverse 1/5 = 1, a matrix A times its inverse—denoted A−1—\n",
      "equals I, where I is the “identity matrix.” An identity matrix is a diagonal\n",
      "matrix with ones along the diagonal, and it is the matrix equivalent of “1.”\n",
      "Some simple algebraic rules follow from the discussion of inverses and the\n",
      "identity matrix:\n",
      "\n",
      "\n",
      "\n",
      "334 A Background Mathematics\n",
      "\n",
      "AA−1 = A−1A = I. (A.10)\n",
      "\n",
      "Furthermore,\n",
      "\n",
      "AI = IA = A. (A.11)\n",
      "\n",
      "Given the commutability implicit in the above rules, it stands that in-\n",
      "verses only exist for square matrices, and that all identity matrices are square\n",
      "matrices. For that matter, the determinant function can only apply to square\n",
      "matrices also.\n",
      "\n",
      "Computing the inverse of matrices is a difficult task, and there are several\n",
      "methods by which to derive them. The simplest method to compute an inverse\n",
      "is to use the following formula:\n",
      "\n",
      "A−1 =\n",
      "1\n",
      "|A|\n",
      "\n",
      "adj A (A.12)\n",
      "\n",
      "The only new element in this formula is “adj A,” which means “adjoint of\n",
      "A.” The adjoint of a matrix is the transpose of its matrix of cofactors, where\n",
      "a cofactor is the signed determinant of the “minor” of an element of a matrix.\n",
      "The minor of element i, j is the matrix the remains after deleting the ith row\n",
      "and jth column of the original matrix. For example, the minor of element a11\n",
      "of the matrix A shown at the beginning of this matrix algebra summary is:[\n",
      "\n",
      "a22 a23\n",
      "a32 a33\n",
      "\n",
      "]\n",
      ".\n",
      "\n",
      "Taking its determinant leaves one with a scalar that is then multiplied\n",
      "by −1i+j . This latter process is called “signing.” In this case, we obtain\n",
      "(−1)2(a22a33 − a23a32) as the cofactor for element a11. If one replaces ev-\n",
      "ery element in matrix A with its cofactor (called “expansion by cofactors,”)\n",
      "then transposes the result, one will obtain the adjoint of A. Multiplying this\n",
      "by 1/|A| (a scalar) will yield the inverse of A.\n",
      "\n",
      "Cofactors can also be used to find determinants. Simply take each element\n",
      "in any row or column and multiply it by its cofactor, and then sum these\n",
      "results together for all the elements in the row/column. Obviously, effort is\n",
      "saved if one chooses a row or column with several 0’s, because the determinants\n",
      "of those elements’ minors do not need to be calculated. (They will ulitmately\n",
      "be multiplied by 0 and will drop from the calculation).\n",
      "\n",
      "Even using cofactors, the process of finding determinants and inverses is\n",
      "tedious. Fortunately, computer packages tend to have determinant and inver-\n",
      "sion routines built into them, and there are plenty of inversion algorithms\n",
      "available if you are designing your own software, so that we generally need\n",
      "not worry. It is worth mentioning that, if a matrix has a 0 determinant, it\n",
      "does not have an inverse. There are many additional matrix algebra rules and\n",
      "tricks that one may need to know; however, they are also beyond the scope of\n",
      "this introduction.\n",
      "\n",
      "\n",
      "\n",
      "A.3 Exercises 335\n",
      "\n",
      "A.3 Exercises\n",
      "\n",
      "A.3.1 Calculus exercises\n",
      "\n",
      "Evaluate the following limits. If a limit does not exist, indicate such. You\n",
      "should graph the expression to see whether the limit exists.\n",
      "\n",
      "1. limx→∞ 1x .\n",
      "2. limx→0 1x .\n",
      "3. limx→∞ x\n",
      "\n",
      "2+5\n",
      "3\n",
      "\n",
      ".\n",
      "4. limx→0 x\n",
      "\n",
      "2−1\n",
      "5\n",
      "\n",
      ".\n",
      "5. limx→∞ exp(−x).\n",
      "6. limx→0 exp(−x).\n",
      "7. limx→∞ x2x .\n",
      "\n",
      "8. limσ→∞ 1σ\n",
      "√\n",
      "\n",
      "2π\n",
      "exp\n",
      "\n",
      "{\n",
      "−(x−µ)2\n",
      "\n",
      "2σ2\n",
      "\n",
      "}\n",
      ".\n",
      "\n",
      "Using the following rules for derivatives, find the slope of the curve below at\n",
      "x = 3.\n",
      "\n",
      "(power rule) (constant rule) (addition rule)\n",
      "\n",
      "d\n",
      "dx\n",
      "\n",
      "[un] = nun−1du d\n",
      "dx\n",
      "\n",
      "[cu] = c× d\n",
      "dx\n",
      "\n",
      "[u]du d\n",
      "dx\n",
      "\n",
      "[u + v] = d\n",
      "dx\n",
      "\n",
      "[u]du + d\n",
      "dx\n",
      "\n",
      "[v]dv\n",
      "\n",
      "9. y = 3x3 + 5x2 + 2x + 10.\n",
      "10. Using the fundamental theorem of calculus and the rules above, find the\n",
      "area under the curve (the integral) of the curve above between 0 and 5.\n",
      "\n",
      "A.3.2 Matrix algebra exercises\n",
      "\n",
      "Perform the various calculations on the matrices below. If an operation is\n",
      "not possible for an item, indicate this and explain why. Don’t forget steps:\n",
      "Expansion by cofactors involves a sign function that depends on i and j, where\n",
      "i and j index the rows and columns. Also, the adjoint matrix is the transpose\n",
      "of the matrix of cofactors.\n",
      "\n",
      "A B C D E\n",
      "\n",
      "[\n",
      "3 2\n",
      "−1 3\n",
      "\n",
      "]  3 1 −14 3 5\n",
      "10 4 1\n",
      "\n",
      "\n",
      " [1 7 4]\n",
      "\n",
      "\n",
      "29\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4 1 37 1 8\n",
      "\n",
      "6 8 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "336 A Background Mathematics\n",
      "\n",
      "1. Find B + E.\n",
      "2. Find E −B.\n",
      "3. Find AB.\n",
      "4. Find CD.\n",
      "5. Find DC.\n",
      "6. Find DE.\n",
      "7. Find ET .\n",
      "8. Find det(A).\n",
      "9. Find det(B).\n",
      "\n",
      "10. Find E−1 (Hint: Use expansion by cofactors).\n",
      "11. Find tr(B).\n",
      "12. If the determinant represents area, volume, or hypervolume from a ge-\n",
      "\n",
      "ometric perspective, then what is the hypervolume of a six-dimensional\n",
      "identity matrix?\n",
      "\n",
      "\n",
      "\n",
      "B\n",
      "\n",
      "The Central Limit Theorem, Confidence\n",
      "Intervals, and Hypothesis Tests\n",
      "\n",
      "The classical approach to statistical inference relies heavily on the Central\n",
      "Limit Theorem (CLT), a key result of which is:\n",
      "\n",
      "f(x̄|µx, σx, n)\n",
      "asy∼ N\n",
      "\n",
      "(\n",
      "µx,\n",
      "\n",
      "σx√\n",
      "n\n",
      "\n",
      ")\n",
      ". (B.1)\n",
      "\n",
      "In English, this expression says that, as sample sizes get larger, the sampling\n",
      "distribution f() for a statistic, here x̄, approaches a normal distribution with a\n",
      "mean equal to the population mean of x (µx) and a standard deviation equal\n",
      "to the population standard deviation of x (σx) divided by the square root\n",
      "of the sample size (n) used to compute x̄. It is important to note that the\n",
      "theorem does not require the distribution of the original variable x from which\n",
      "the statistic x̄ was generated to be normal: Regardless of the distribution of\n",
      "the data, the asymptotic sampling distribution of a statistic will be normal.\n",
      "\n",
      "What is a sampling distribution? Consider that, when we take a random\n",
      "sample of size n individuals, this sample is only one of many possible samples\n",
      "of size n that could be drawn from the population. Certainly, if we took a\n",
      "second random sample of the same size, we would not end up with the same\n",
      "collection of respondents, and consequently, the value of any statistic (like\n",
      "the mean) we calculate from one sample will most likely differ from the value\n",
      "obtained in another. The CLT, however, says that, if we were to take all\n",
      "possible random samples of a given size from the population and compute the\n",
      "value of the statistic of interest (e.g., the mean) for each one, the distribution\n",
      "of these statistics—the sampling distribution—would be normal, assuming the\n",
      "sample size is large enough.\n",
      "\n",
      "B.1 A simulation study\n",
      "\n",
      "To demonstrate the theorem, I display the results of a brief simulation that\n",
      "followed the following structure:\n",
      "\n",
      "\n",
      "\n",
      "338 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "1. I drew 1,000 samples each of sizes n = 1, 2, 30, and 100 from a U(0, 1)\n",
      "distribution. The mean and standard deviation of a U(a, b) distribution\n",
      "\n",
      "are b+a\n",
      "2\n",
      "\n",
      "and\n",
      "√\n",
      "\n",
      "(b−a)2\n",
      "12\n",
      "\n",
      ", respectively. Here, those values are .5 and .2887\n",
      "(variance = .0833).\n",
      "\n",
      "2. I computed the mean for each of the 1,000 samples for each sample size and\n",
      "computed the standard deviation for each sample also. The collection of\n",
      "the means from each of the 1,000 samples of a given sample size represents\n",
      "the sampling distribution of the mean for that sample size.\n",
      "\n",
      "3. I computed the mean of all the sample means computed from Step 2 for\n",
      "each sample size, and I computed the standard deviation (and variances)\n",
      "of these distributions of means.\n",
      "\n",
      "Figure B.1 shows the distribution of these 1, 000 sample means (the sam-\n",
      "pling distribution) for all four sample sizes. The figure shows that the means\n",
      "are distributed across the [0, 1] interval for samples size n = 1, basically re-\n",
      "flecting that samples of this size are nothing more than draws from the original\n",
      "distribution. As the sample size increases, the means become more clustered\n",
      "around the true mean of .5.\n",
      "\n",
      "Figure B.2 shows what happens to the standard deviation of the sampling\n",
      "distribution of the mean as the sample sizes get larger. The figure depicts\n",
      "the empirical standard deviation of the sampling distribution for each of the\n",
      "sample sizes (the standard deviation of the 1,000 sample means), as well as the\n",
      "theoretical standard deviation claimed by the CLT. Notice how the empirical\n",
      "and theoretical standard deviations almost perfectly coincide, illustrating the\n",
      "theorem.\n",
      "\n",
      "Figure B.3 shows the distribution of the variances for samples size n = 2,\n",
      "n = 30, and n = 100. (Note: The variances for the samples of size n = 1\n",
      "are 0, which is why they are not plotted.) The sampling distribution for the\n",
      "variances at n = 2 is truncated at 0 and highly right-skewed (in fact, the\n",
      "distribution of variances is an inverse gamma distribution; see Chapter 3). The\n",
      "distribution is also very broad. As the sample sizes increase, the distribution\n",
      "becomes more and more symmetric in shape and narrower. The mean of each\n",
      "of these distributions of the variance is equal to approximately .08, the correct\n",
      "variance (\n",
      "\n",
      "√\n",
      "1/12\n",
      "\n",
      "2\n",
      "). These results suggest that larger samples are also better\n",
      "\n",
      "at pinpointing the true population variance/standard deviation.\n",
      "\n",
      "B.2 Classical inference\n",
      "\n",
      "Classical statistical inference relies on the CLT as exemplified by the simula-\n",
      "tion results. When we draw a random sample of size n from a population, we\n",
      "can imagine that we are really sampling a sample mean, variance, regression\n",
      "parameter, or some other statistic from its sampling distribution. From that\n",
      "perspective, if we know that large samples have a normal sampling distribu-\n",
      "tion for a statistic (θ̂), and we let the sample statistic (e.g., mean, variance,\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 339\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "1\n",
      ".2\n",
      "\n",
      "x, n=1\n",
      "\n",
      "f(x\n",
      ")\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "1\n",
      ".0\n",
      "\n",
      "2\n",
      ".0\n",
      "\n",
      "x, n=2\n",
      "\n",
      "f(x\n",
      ")\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "\n",
      "x, n=30\n",
      "\n",
      "f(x\n",
      ")\n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "4\n",
      "6\n",
      "\n",
      "8\n",
      "1\n",
      "\n",
      "2\n",
      "\n",
      "x, n=100\n",
      "\n",
      "f(x\n",
      ")\n",
      "\n",
      "Fig. B.1. Distributions of sample means for four sample sizes (n = 1, 2, 30, and\n",
      "100).\n",
      "\n",
      "regression parameter) be our best guess for the true population parameter\n",
      "(θ), then we can use what we know about normal distributions to make infer-\n",
      "ence about the likely value of the true population parameter. Two strategies\n",
      "are commonly used in classical inference: hypothesis testing and confidence\n",
      "interval construction.\n",
      "\n",
      "B.2.1 Hypothesis testing\n",
      "\n",
      "Under the hypothesis testing approach, we hypothesize some value for θ (H0;\n",
      "the “null hypothesis”), and we compare our sample statistic to our hypoth-\n",
      "esized value of the parameter and decide whether our observed estimate is\n",
      "sufficiently different from the hypothesized value that we should reject the\n",
      "hypothesized value.\n",
      "\n",
      "How do we determine whether the sample statistic is “sufficiently different”\n",
      "from H0? By the CLT, we know that the sampling distribution for θ̂ is normal,\n",
      "and so we can standardize θ̂ by subtracting off H0 and dividing by the standard\n",
      "deviation of the sampling distribution (σ/\n",
      "\n",
      "√\n",
      "n; called the “standard error”).\n",
      "\n",
      "\n",
      "\n",
      "340 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "0 20 40 60 80 100\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".0\n",
      "5\n",
      "\n",
      "0\n",
      ".1\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".1\n",
      "5\n",
      "\n",
      "0\n",
      ".2\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      ".2\n",
      "5\n",
      "\n",
      "0\n",
      ".3\n",
      "\n",
      "0\n",
      "\n",
      "Sample Size\n",
      "\n",
      "S\n",
      "ta\n",
      "\n",
      "n\n",
      "d\n",
      "\n",
      "a\n",
      "rd\n",
      "\n",
      " E\n",
      "rr\n",
      "\n",
      "o\n",
      "r\n",
      "\n",
      "*\n",
      "_\n",
      "\n",
      "Empirical Standard Error\n",
      "Theoretical Standard Error\n",
      "\n",
      "Fig. B.2. Empirical versus theoretical standard deviations of sample means under\n",
      "the CLT.\n",
      "\n",
      "Given the normality of the sampling distribution, with σ known, the result is\n",
      "a z score:\n",
      "\n",
      "z =\n",
      "θ̂ −H0\n",
      "σ/\n",
      "√\n",
      "\n",
      "n\n",
      ".\n",
      "\n",
      "We can therefore use this z score to assess the probability of observing θ̂ if H0\n",
      "were true—that is, if the true sampling distribution for θ̂ were centered over\n",
      "our hypothesized value for the parameter. If observing an estimate as extreme\n",
      "as θ̂ or moreso would be a rare event if H0 were true, we conclude that H0 is not\n",
      "true. In social science research, we generally consider an estimate that would\n",
      "occur with probability .05 or less under H0 to be sufficient evidence to reject\n",
      "H0. We call this level the “critical level” (denoted α), and we call our observed\n",
      "probability of obtaining the estimate we did under the null hypothesis a “p-\n",
      "value.”\n",
      "\n",
      "Although the sampling distribution for a statistic is asymptotically normal\n",
      "according to the CLT, we generally do not know σ (and therefore σ/\n",
      "\n",
      "√\n",
      "n) with\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 341\n",
      "\n",
      "0.0 0.1 0.2 0.3 0.4\n",
      "0\n",
      "\n",
      "4\n",
      "8\n",
      "\n",
      "1\n",
      "2\n",
      "\n",
      "σ, n=2\n",
      "\n",
      "f(σ\n",
      ")\n",
      "\n",
      "0.0 0.1 0.2 0.3 0.4\n",
      "\n",
      "0\n",
      "5\n",
      "\n",
      "1\n",
      "5\n",
      "\n",
      "σ, n=30\n",
      "\n",
      "f(σ\n",
      ")\n",
      "\n",
      "0.0 0.1 0.2 0.3 0.4\n",
      "\n",
      "0\n",
      "1\n",
      "\n",
      "0\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "σ, n=100\n",
      "\n",
      "f(σ\n",
      ")\n",
      "\n",
      "Fig. B.3. Sampling distributions of variances in the simulation.\n",
      "\n",
      "certainty. Instead, we usually estimate σ with the sample estimate s (σ̂).\n",
      "Recall, however, from the previous figure that the variance (hence, standard\n",
      "deviation) also has a distribution and that this distribution is fairly broad\n",
      "for small n. This means that our sample estimate of the population standard\n",
      "deviation has some uncertainty built into it. As a result, we must compensate\n",
      "for this uncertainty by using the t distribution, rather than the z distribution\n",
      "in constructing our standardized score:\n",
      "\n",
      "t =\n",
      "θ̂ −H0\n",
      "σ̂/\n",
      "√\n",
      "\n",
      "n\n",
      ".\n",
      "\n",
      "Technically, the t distribution is the marginal distribution for the mean when\n",
      "σ is unknown, and so this calculation compensates for the uncertainty inherent\n",
      "in using σ̂ to estimate σ.\n",
      "\n",
      "Recall also from the figure that, as n gets larger, our uncertainty about the\n",
      "true value of that population standard deviation shrinks, allowing us to use\n",
      "distributions that look more and more normal. Thus, when n is large enough,\n",
      "\n",
      "\n",
      "\n",
      "342 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "the t distribution converges on the normal distribution, and we often simply\n",
      "use the normal distribution for inference.\n",
      "\n",
      "B.2.2 Confidence intervals\n",
      "\n",
      "An alternative method of performing statistical inference is to construct a\n",
      "“confidence interval” around a sample estimate in order to state, with some\n",
      "level of “confidence,” where we suspect the parameter θ falls. When σ is\n",
      "known, we compute intervals using the z distribution; when σ is not known,\n",
      "we compute intervals using the t distribution, for the same reason discussed\n",
      "in the previous section. These intervals are computed as:\n",
      "\n",
      "(1− α)%C.I. = θ̂ ± zα\n",
      "2\n",
      "\n",
      "(\n",
      "σ\n",
      "√\n",
      "\n",
      "n\n",
      "\n",
      ")\n",
      ",\n",
      "\n",
      "and\n",
      "\n",
      "(1− α)%C.I. = θ̂ ± tα\n",
      "2\n",
      "\n",
      "(\n",
      "σ̂\n",
      "√\n",
      "\n",
      "n\n",
      "\n",
      ")\n",
      ",\n",
      "\n",
      "respectively, where (1 − α)% is the level of confidence, zα\n",
      "2\n",
      "\n",
      "(or tα\n",
      "2\n",
      ") is the\n",
      "\n",
      "number of standard errors we must add/subtract from our sample estimate θ̂\n",
      "to obtain the desired level of confidence, and σ/\n",
      "\n",
      "√\n",
      "n (or σ̂/\n",
      "\n",
      "√\n",
      "n) is the size of\n",
      "\n",
      "the standard error. Just as we commonly use a critical level of α = .05 under\n",
      "the hypothesis testing approach to inference, we often construct 95% (1-.05)\n",
      "confidence intervals, and so I will discuss such intervals here.\n",
      "\n",
      "The correct interpretation of a 95% confidence interval is that 95% of the\n",
      "confidence intervals that could be drawn from the sampling distribution for\n",
      "θ̂ would capture θ. In order to understand why, consider, once again, that\n",
      "the sampling distribution for θ̂ is normal and centered over θ with a standard\n",
      "deviation of σ/\n",
      "\n",
      "√\n",
      "n. Then (assuming σ is known), 95% of all possible values\n",
      "\n",
      "of θ̂ will fall within 1.96 standard errors of θ. If this is true, then if we add\n",
      "(and subtract) 1.96 standard errors to (from) all possible estimates θ̂, 95%\n",
      "of the resulting intervals would contain θ. The ones that do not will be the\n",
      "intervals constructed around values of θ̂ that fall in the tails of the sampling\n",
      "distribution. As before, if σ is not known and is estimated with σ̂, our interval\n",
      "should be constructed using the t distribution.\n",
      "\n",
      "Figure B.4 shows a collection of 95% confidence intervals based on the\n",
      "z distribution for the simulated samples of size n = 2, n = 30, and n =\n",
      "100. The intervals are vertical, with the true population parameter value (.5)\n",
      "represented by a horizontal line. Notice how these intervals are much more\n",
      "consistent in width and generally narrower for the larger size samples than the\n",
      "n = 2 sized samples. Also notice that some of the confidence intervals for the\n",
      "samples sized n = 2 are extremely narrow. This result reflects the uncertainty\n",
      "in σ2 (and thus σ2/n) inherent in having small samples.\n",
      "\n",
      "Table B.1 shows the percent of the confidence intervals (out of 1,000 for\n",
      "each sample size) that, in fact, capture the true mean. In the table, I have\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 343\n",
      "\n",
      "5 10 15 20\n",
      "0\n",
      "\n",
      ".0\n",
      "0\n",
      "\n",
      ".4\n",
      "0\n",
      "\n",
      ".8\n",
      "Sample\n",
      "\n",
      "9\n",
      "5\n",
      "\n",
      "%\n",
      " C\n",
      "\n",
      "o\n",
      "n\n",
      "\n",
      "fid\n",
      "e\n",
      "\n",
      "n\n",
      "ce\n",
      "\n",
      " I\n",
      "n\n",
      "\n",
      "te\n",
      "rv\n",
      "\n",
      "a\n",
      "l\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
      "\n",
      "5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Sample\n",
      "\n",
      "9\n",
      "5\n",
      "\n",
      "%\n",
      " C\n",
      "\n",
      "o\n",
      "n\n",
      "\n",
      "fid\n",
      "e\n",
      "\n",
      "n\n",
      "ce\n",
      "\n",
      " I\n",
      "n\n",
      "\n",
      "te\n",
      "rv\n",
      "\n",
      "a\n",
      "l\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
      "\n",
      "5 10 15 20\n",
      "\n",
      "0\n",
      ".0\n",
      "\n",
      "0\n",
      ".4\n",
      "\n",
      "0\n",
      ".8\n",
      "\n",
      "Sample\n",
      "\n",
      "9\n",
      "5\n",
      "\n",
      "%\n",
      " C\n",
      "\n",
      "o\n",
      "n\n",
      "\n",
      "fid\n",
      "e\n",
      "\n",
      "n\n",
      "ce\n",
      "\n",
      " I\n",
      "n\n",
      "\n",
      "te\n",
      "rv\n",
      "\n",
      "a\n",
      "l\n",
      "\n",
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n",
      "\n",
      "Fig. B.4. z-based confidence intervals for the mean.\n",
      "\n",
      "computed these percentages using both 1.96 (the critical value for the z distri-\n",
      "bution) and using the appropriate t critical value, given the degrees of freedom\n",
      "(n− 1) for each size sample. Notice how the z distribution works well for the\n",
      "larger samples, but not for the small n samples. Also observe how well the\n",
      "t-based intervals perform.\n",
      "\n",
      "Table B.1. Percentage of confidence intervals capturing the true mean in simula-\n",
      "tion.\n",
      "\n",
      "Sample Size z-based intervals t-based intervals\n",
      "\n",
      "n = 2 .639 .915\n",
      "n = 30 .933 .948\n",
      "n = 100 .957 .960\n",
      "\n",
      "\n",
      "\n",
      "344 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "B.2.3 Some final notes\n",
      "\n",
      "With classical statistics and inference, there are a few things to keep in mind.\n",
      "First, the interpretation of both the confidence intervals and the test statistics\n",
      "is tedious. We cannot say that our confidence intervals in Table B.1 suggest\n",
      "that the true mean falls within those limits with probability .95. The param-\n",
      "eter, from a classical standpoint, is fixed. Only the interval itself is random.\n",
      "Similarly, if we reject a null hypothesis we cannot say that there is a 95%\n",
      "probability the null is wrong, and we cannot say that we are 95% confident\n",
      "that some alternate hypothesis is true, either. We can only say that, under\n",
      "the assumption the null hypothesis were true, the data would be extremely\n",
      "rare. This leads us to believe that the null is not true, but it does not disprove\n",
      "the null nor prove an alternative.\n",
      "\n",
      "Second, as stated above, the justification of the classical approach to infer-\n",
      "ence rests on the CLT. The CLT, however, claims that sampling distributions\n",
      "for a statistic are only asymptotically normal, meaning that sample sizes must\n",
      "be large in order to invoke the CLT as a justification for classical confidence\n",
      "intervals and hypothesis tests. When sample sizes are small, the theorem does\n",
      "not hold, and hence, confidence intervals and hypothesis tests are suspect. We\n",
      "can see this result in Table B.1. For samples of size n = 2 the supposed 95%\n",
      "z-based intervals only capture the true mean in 63.9% of the cases. Even after\n",
      "adjusting for uncertainty in the estimate of σ by constructing intervals based\n",
      "on the t distribution, the supposed 95% intervals only capture the true mean\n",
      "91.5% of the time.\n",
      "\n",
      "For this appendix, I do not provide exercises. Instead, please see Chapter 2\n",
      "for relevant exercises involving the classical statistical approach.\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 345\n",
      "\n",
      "*********************************************************\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "References\n",
      "\n",
      "Albert, J.H. (1992). “Bayesian Estimation of the Polychoric Correlation Co-\n",
      "efficient.” Journal of Computation and Simulation 44:47-61.\n",
      "\n",
      "Albert, J.H. and S. Chib. (1993). “Bayesian Analysis of Binary and Polychoto-\n",
      "mous Response Data.” Journal of the American Statistical Association\n",
      "88(422):669-679.\n",
      "\n",
      "Alho, J.M. and B.D. Spencer. (2005). Statistical Demography and Forecasting.\n",
      "New York: Springer.\n",
      "\n",
      "Allison, P.D. (1984). Event History Analysis: Regression for Longitudinal\n",
      "Event Data. Sage University Paper Series on Quantitative Applications\n",
      "in the Social Sciences, 07-46. Thousand Oaks, CA: Sage.\n",
      "\n",
      "Allison, P.D. (2003). Missing Data. Sage University Paper Series on Quanti-\n",
      "tative Applications in the Social Sciences, 07-136. Thousand Oaks, CA:\n",
      "Sage.\n",
      "\n",
      "Billingsley, P. (1995). Probability and Measure. 3rd ed. New York: Wiley.\n",
      "Bollen, K.A. (1989). Structural Equations with Latent Variables. New York:\n",
      "\n",
      "Wiley.\n",
      "Bollen, K.A. and P.J. Curran. (2006). Latent Curve Models: A Structural\n",
      "\n",
      "Equation Perspective. Hoboken, NJ: Wiley.\n",
      "Box, G.E.P. and G.C. Tiao. (1973). Bayesian Inference in Statistical Analysis.\n",
      "\n",
      "Reading, MA: Addison-Wesley.\n",
      "Bremaud, P. (1999). Markov Chains, Gibbs Fields, Monte Carlo Simulation,\n",
      "\n",
      "and Queues. New York: Springer-Verlag.\n",
      "Carlin, B.P. and T.A. Louis. (2000). Bayes and Empirical Bayes Methods for\n",
      "\n",
      "Data Analysis. 2nd ed. Boca Raton, FL: Chapman & Hall/CRC.\n",
      "Casella, G. and E.I. George. (1992). “Explaining the Gibbs Sampler.” The\n",
      "\n",
      "American Statistician 46(3):167-174.\n",
      "Chen, M-H., Q-M. Shao, and J.G. Ibrahim. (2000) Monte Carlo Methods in\n",
      "\n",
      "Bayesian Computation. New York: Springer-Verlag.\n",
      "\n",
      "\n",
      "\n",
      "348 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "Chib, S. and E. Greenberg. (1998). “Analysis of Multivariate Probit Models.”\n",
      "Biometrika 85(2):347-361.\n",
      "\n",
      "Chung, K.L. and F. AitSahlia. (2003). Elementary Probability Theory: With\n",
      "Stochastic Processes and an Introduction to Mathematical Finance. 4th\n",
      "ed. New York: Springer-Verlag.\n",
      "\n",
      "Congdon, P. (2003). Applied Bayesian Modelling. West Sussex, England: Wi-\n",
      "ley.\n",
      "\n",
      "Congdon, P. (2001). Bayesian Statistical Modeling. West Sussex, England:\n",
      "Wiley.\n",
      "\n",
      "Cowles, M.K. (1996). “Accelerating Monte Carlo Markov Chain Convergence\n",
      "for Cumulative-Link Generalized Linear Models.” Statistics and Com-\n",
      "puting 6:101-110.\n",
      "\n",
      "Cox, D.R. (1972). “Regression Models and Life Tables.” Journal of the Royal\n",
      "Statistical Society, Series B34:187-220.\n",
      "\n",
      "Crimmins, E.M., Y. Saito, and D. Ingegneri. (1997). “Trends in Disability-\n",
      "Free Life Expectancy in the United States, 1970-90.” Population and\n",
      "Development Review 23(3):555-572.\n",
      "\n",
      "DeGroot, M.H. (1986). Probability and Statistics. (Second Ed.) Reading, MA:\n",
      "Addison-Wesley.\n",
      "\n",
      "DiMaggio, P., J. Evans, and B. Bryson. (1996). “Have Americans’ Social\n",
      "Attitudes Become More Polarized?” American Journal of Sociology\n",
      "102(3):690-755.\n",
      "\n",
      "Drezner, Z. (1992). “Computation of the Multivariate Normal Integral.” ACM\n",
      "Transactions on Mathematical Software 18(4):470-480.\n",
      "\n",
      "Durkheim, E. (1984). The Division of Labor in Society. New York: The Free\n",
      "Press.\n",
      "\n",
      "Edwards, A.W.F. (1992). Likelihood. Expanded Edition. Baltimore: Johns\n",
      "Hopkins University University Press.\n",
      "\n",
      "Eliason, S.R. (1993). Maximum Likelihood Estimation: Logic and Practice.\n",
      "Sage University Paper Series on Quantitative Applications in the Social\n",
      "Sciences, 07-96. Thousand Oaks, CA: Sage.\n",
      "\n",
      "Elo, I.T. and S.H. Preston. (1994). “Estimating African-American Mortality\n",
      "from Inaccurate Data.” Demography 31(3):427-458.\n",
      "\n",
      "Evans, M., N. Hastings, and B. Peacock. (2000). Statistical Distributions. 3rd\n",
      "ed. New York: Wiley.\n",
      "\n",
      "Ezell, M.E., K.C. Land, and L.E. Cohen. (2003). “Modeling Multiple Fail-\n",
      "ure Time Data: A Survey of Variance-Corrected Proportional Haz-\n",
      "ards Models with Empirical Applications to Arrest Data.” Sociological\n",
      "Methodology 33:111-167.\n",
      "\n",
      "Fox, J. (1997). Applied Regression Analysis, Linear Models, and Related Meth-\n",
      "ods. Thousand Oaks, CA: Sage.\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 349\n",
      "\n",
      "Gelfand, A.E. and A.F.M. Smith. (1990). “Sampling-Based Approaches to\n",
      "Calculating Marginal Densities.” Journal of the American Statistical\n",
      "Association 85:398-409.\n",
      "\n",
      "Gelman, A. (1996). “Inference and Monitoring Convergence.” Pp. 131-144 in\n",
      "Gilks, W.R., S. Richardson, and D.J. Spiegelhalter (eds.) Markov Chain\n",
      "Monte Carlo in Practice. London: Chapman and Hall.\n",
      "\n",
      "Gelman, A., J.B. Carlin, H.S. Stern, and D.B. Rubin. (1995). Bayesian Data\n",
      "Analysis. London: Chapman and Hall.\n",
      "\n",
      "Gelman, A. and D.B. Rubin. (1995). “Avoiding Model Selection in Bayesian\n",
      "Social Research.” Sociological Methodology 25:165–74.\n",
      "\n",
      "Gelman, A., X-L. Meng, and H.S. Stern. (1996). “Posterior Predictive Assess-\n",
      "ment of Model Fitness via Realized Discrepancies.” Statistica Sinica\n",
      "6:733–807.\n",
      "\n",
      "Gilks, W.R. (1996). “Full Conditional Distributions.” Pp. 59-88 in Gilks,\n",
      "W.R., S. Richardson, and D.J. Spiegelhalter (eds.). Markov Chain\n",
      "Monte Carlo in Practice. London: Chapman and Hall.\n",
      "\n",
      "Gilks, W.R., S. Richardson, and D.J. Spiegelhalter (eds.). (1996). Markov\n",
      "Chain Monte Carlo in Practice. London: Chapman and Hall.\n",
      "\n",
      "Gilks, W.R. and G.O. Roberts. (1996). “Strategies for Improving MCMC.”\n",
      "Pp. 89-110 in Gilks, W.R., S. Richardson, and D.J. Spiegelhalter (eds.).\n",
      "Markov Chain Monte Carlo in Practice. London: Chapman and Hall.\n",
      "\n",
      "Gill, J. (2002). Bayesian Methods: A Social and Behavioral Sciences Approach.\n",
      "Boca Raton, FL: Chapman & Hall/CRC.\n",
      "\n",
      "Hagle, T.M. (1996) Basic Math for Social Scientists: Problems and Solutions.\n",
      "Sage University Paper Series on Quantitative Applications in the Social\n",
      "Sciences, 07-108. Thousand Oaks, CA: Sage.\n",
      "\n",
      "Hamilton, J.D. (1994). Time Series Analysis. Princeton: Princeton University\n",
      "Press.\n",
      "\n",
      "Harville, D.A. (1997). Matrix Algebra from a Statisticians Perspective. New\n",
      "York: Springer-Verlag.\n",
      "\n",
      "Hastings, W.K. (1970). “Monte Carlo Sampling Methods Using Markov\n",
      "Chains and Their Applications.” Biometrika 57:97-109.\n",
      "\n",
      "Hayduk, L.A. (1987). Structural Equation Modeling with LISREL. Baltimore,\n",
      "MD: The Johns Hopkins University Press.\n",
      "\n",
      "Heckman, J.J. (1979). “Sample Selection Bias as a Specification Error.”\n",
      "Econometrica 47(1):153-161.\n",
      "\n",
      "Hoeting, J.A., D. Madigan, A.E. Raftery, and C.T. Volinsky. (1999). “Bayesian\n",
      "Model Averaging: A Tutorial.” Statistical Science 14(4):382-417.\n",
      "\n",
      "Hosmer, D.W. and S. Lemeshow. (1989). Applied Logistic Regression. New\n",
      "York: Wiley.\n",
      "\n",
      "Hosmer, D.W. and S. Lemeshow. (1999). Applied Survival Analysis: Regres-\n",
      "sion Modeling of Time to Event Data. New York: Wiley.\n",
      "\n",
      "\n",
      "\n",
      "350 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "House, J.S., J.M. Lepkowski, A.M. Kinney, R.P. Mero, R.C. Kessler, and\n",
      "A.R. Herzog. (1994). “The Social Stratification of Aging and Health.”\n",
      "Journal of Health and Social Behavior 35(September):213-234.\n",
      "\n",
      "Hubbard, R. and M.J. Bayarri. (2003). “Confusion Over Measures of Evi-\n",
      "dence (p’s) Versus Errors (α’s) in Classical Stastistical Testing.” The\n",
      "American Statistician 57(3):171-182.\n",
      "\n",
      "Imai, K. and D.A. van Dyk. (2005). “A Bayesian Analysis of the Multino-\n",
      "mial Probit Model Using Marginal Data Augmentation.” Journal of\n",
      "Econometrics 124:311-334.\n",
      "\n",
      "Jeffreys, Sir H.. (1961). Theory of Probability. 3rd ed. New York: Oxford Uni-\n",
      "versity Press.\n",
      "\n",
      "Johnson, V.E. and J.H. Albert. (1999). Ordinal Data Modeling. New York:\n",
      "Springer-Verlag.\n",
      "\n",
      "Judge, G.G., W.E. Griffiths, R.C. Hill, H. Lutkepoohl, and T-C. Lee. (1985).\n",
      "The Theory and Practice of Econometrics. 2nd ed. New York: Wiley.\n",
      "\n",
      "Kaplan, D. (2000). Structural Equation Modeling: Foundations and Exten-\n",
      "sions. Thousands Oaks, CA: Sage.\n",
      "\n",
      "Kass, R.E. and A.E. Raftery. (1995). “Bayes Factors.” Journal of the Ameri-\n",
      "can Statistical Association 90(430):773-795.\n",
      "\n",
      "Laditka, S.B. and D.A. Wolf. (1998). “New Methods for Analyzing Active Life\n",
      "Expectancy.” Journal of Aging and Health 10(2):214-241.\n",
      "\n",
      "Land, K.C., J.M. Guralnik, and D.G. Blazer. (1994). “Estimating Increment-\n",
      "Decrement Life Tables with Multiple Covariates from Panel Data: The\n",
      "Case of Active Life Expectancy.” Demography 31(2):297-319.\n",
      "\n",
      "Land, K.C., P.L. McCall, and D.S. Nagin. (1996). “A Comparison of Pois-\n",
      "son, Negative Binomial, and Semiparametric Mixed Poisson Regression\n",
      "Models with Empirical Applications to Criminal Careers Research.” So-\n",
      "ciological Methods & Research 24:387-442.\n",
      "\n",
      "Lee, P.M. (1989). Bayesian Statistics: An Introduction. New York: Oxford\n",
      "University Press.\n",
      "\n",
      "Lee, S-Y. and H-T. Zhu. (2000). “Statistical Analysis of Nonlinear Struc-\n",
      "tural Equation Models with Continuous and Polytomous Data.” British\n",
      "Journal of Mathematical and Statistical Psychology 53:209-232.\n",
      "\n",
      "Leonard, T. and J.S.J. Hsu. (1999). Bayesian Methods: An Analysis for Statis-\n",
      "ticians and Interdisciplinary Researchers. Cambridge, UK: Cambridge\n",
      "University Press.\n",
      "\n",
      "Liao, T.F. (1994). Interpreting Probability Models: Logit, Probit, and Other\n",
      "Generalized Linear Models. Sage University Paper Series on Quantita-\n",
      "tive Applications in the Social Sciences, 07-101. Thousand Oaks, CA:\n",
      "Sage.\n",
      "\n",
      "Little, R.J.A. and D.B. Rubin. (1987). Statistical Analysis with Missing Data.\n",
      "New York: Wiley.\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 351\n",
      "\n",
      "Liu, J.S. (2001). Monte Carlo Strategies in Scientific Computing. New York:\n",
      "Springer-Verlag.\n",
      "\n",
      "Long, J.S. (1997). Regression Models for Categorical and Limited Dependent\n",
      "Variables. Thousand Oaks, CA: Sage.\n",
      "\n",
      "Long, J.S. and J. Freese. (2003). Regression Models for Categorical Dependent\n",
      "Variables Using Stata. College Station, TX: Stata Press.\n",
      "\n",
      "Lynch, S.M. (2003). “Cohort and Life Course Patterns in the Relationship Be-\n",
      "tween Education and Health: A Hierarchical Approach.” Demography\n",
      "40(2):309-331.\n",
      "\n",
      "Lynch, S.M. and J.S. Brown. (2005). “A New Approach to Estimating Life Ta-\n",
      "bles with Covariates and Constructing Interval Estimates of Life Table\n",
      "Quantities.” Sociological Methodology 35:177-225.\n",
      "\n",
      "Lynch, S.M., J.S. Brown, and K.G. Harmsen. (2003). “Black-White Differ-\n",
      "ences in Mortality Deceleration and Compression and the Mortality\n",
      "Crossover Reconsidered.” Research on Aging 25(5):456-483.\n",
      "\n",
      "Lynch, S.M. and B. Western. (2004). “Bayesian Posterior Predictive Checks\n",
      "for Complex Models.” Sociological Methods & Research 32(3):301-335.\n",
      "\n",
      "Markides, K.S. and S.A. Black. (1996). “Race, Ethnicity and Aging: The Im-\n",
      "pact of Inequality.” Pp. 153-170 in R.H. Binstock and L.K. George\n",
      "(eds.). Handbook of Aging and the Social Sciences 4th ed. San Diego,\n",
      "CA: San Diego Academic Press.\n",
      "\n",
      "Maruyama, G.M. (1998). Basics of Structural Equation Modeling. Thousand\n",
      "Oaks, CA: Sage.\n",
      "\n",
      "McArdle, J.J. and D. Epstein. (1987). “Latent Growth Curves within Devel-\n",
      "opmental Structural Equation Models.” Child Development 58:110-133.\n",
      "\n",
      "Mehta, P.D. and S.G. West. (2000). “Putting the Individual Back into Indi-\n",
      "vidual Growth Curves.” Psychological Methods 5(1):23-43.\n",
      "\n",
      "Menken, J., J. Trussell, D. Stempel, and O. Babakol. (1981). “Propor-\n",
      "tional Hazards Life Table Models: An Illustrative Analysis of Socio-\n",
      "Demographic Influences on Marriage Dissolution in the United States.”\n",
      "Demography 18(2):181-200.\n",
      "\n",
      "Meredith, W. and J. Tisak. (1990). “Latent Curve Analysis.” Psychometrika\n",
      "31:59-72.\n",
      "\n",
      "Neter, J., M.H. Kutner, C.J. Nachtsheim, and W. Wasserman. (1996). Applied\n",
      "Linear Regression Models. 3rd ed. Chicago: Irwin.\n",
      "\n",
      "Olsson, U. (1979). “Maximum Likelihood Estimation of the Polychoric Cor-\n",
      "relation Coefficient.” Psychometrika 44(4):443-458.\n",
      "\n",
      "Olsson, U., F. Drasgow, and N. J. Dorans. (1982). “The Polyserial Correlation\n",
      "Coefficient.” Psychometrika 47(3):337-347.\n",
      "\n",
      "Pole, A., M. West, and J. Harrison. (1994). Applied Bayesian Forecasting and\n",
      "Time Series Analysis. Boca Raton, FL: Chapman & Hall/CRC.\n",
      "\n",
      "\n",
      "\n",
      "352 B The Central Limit Theorem, Confidence Intervals, and Hypothesis Tests\n",
      "\n",
      "Poon, W-Y. and S-Y. Lee. (1987). “Maximum Likelihood Estimation of Multi-\n",
      "variate Polyserial and Polychoric Correlation Coefficients.” Psychome-\n",
      "trika 52(3):409-430.\n",
      "\n",
      "Powers, D.A. and Y. Xie. (2000). Statistical Models for Categorical Data Anal-\n",
      "ysis. San Diego, CA: Academic Press.\n",
      "\n",
      "Press, W.H., S.A. Teukolsky, W.T. Vetterling, and B.P. Flannery. (2002).\n",
      "Numerical Recipes in C: The Art of Scientific Computing. 2nd ed. New\n",
      "York: Cambridge University Press.\n",
      "\n",
      "Preston, S.H., I.T. Elo, I. Rosenwaike, and M. Hill. (1996). “African-American\n",
      "Mortality at Older Ages: Results of a Matching Study.” Demography\n",
      "33:193-209.\n",
      "\n",
      "Preston, S.H., P. Heuveline, and M. Guillot. (2001). Demography: Measuring\n",
      "and Modeling Population Processes. Oxford: Blackwell.\n",
      "\n",
      "Raftery, A.E. (1995). “Bayesian Model Selection in Social Research.” Socio-\n",
      "logical Methodology 25:111-164.\n",
      "\n",
      "Raudenbush, S.W. and A.S. Bryk. (2002). Hierarchical Linear Models: Ap-\n",
      "plications and Data Analysis Methods. 2nd ed. Thousand Oaks, CA:\n",
      "Sage.\n",
      "\n",
      "Ripley, B.D. (1987). Stochastic Simulation. New York: Wiley.\n",
      "Robert, C.P. (1994). The Bayesian Choice: A Decision-Theoretic Motivation.\n",
      "\n",
      "New York: Springer.\n",
      "Robert, C.P. (1995). “Simulation of Truncated Normal Variables.” Statistics\n",
      "\n",
      "and Computing 5(2):121-125.\n",
      "Robert, C.P. and G. Casella. (1999). Monte Carlo Statistical Methods. New\n",
      "\n",
      "York: Springer-Verlag.\n",
      "Rogosa, D.R. and J.B. Willett. (1985). “Understanding Correlates of Change\n",
      "\n",
      "by Modeling Individual Differences in Growth.” Psychometrika 50:203-\n",
      "228.\n",
      "\n",
      "Rubin, D.B. 1984. “ Bayesianly Justifiable and Relevant Frequency Calcula-\n",
      "tions for the Applied Statistician.” Annals of Statistics 12:1151–1172.\n",
      "\n",
      "Rudas, T. (2004). Probability Theory: A Primer. Sage University Paper Series\n",
      "on Quantitative Applications in the Social Sciences, 07-142. Thousand\n",
      "Oaks, CA: Sage.\n",
      "\n",
      "Schervish, M.J. (1984). “Multivariate Normal Probabilities with Error Bound.”\n",
      "Applied Statistics 33:81-94.\n",
      "\n",
      "Schoen, R. (1988). Modeling Multigroup Populations. New York: Plenum.\n",
      "Singer, B. and Spilerman, S. (1976), “The Representation of Social Processes\n",
      "\n",
      "by Markov Models.” American Journal of Sociology 82:1-54.\n",
      "Sivia, D.S. (1996). Data Analysis: A Bayesian Tutorial. New York: Oxford\n",
      "\n",
      "University Press.\n",
      "\n",
      "\n",
      "\n",
      "B.2 Classical inference 353\n",
      "\n",
      "Smith, A.F.M. and A.E. Gelfand. (1992). “Bayesian Statistics without Tears:\n",
      "A Sampling-Resampling Perspective.” The American Statistician 46(2):84-\n",
      "88.\n",
      "\n",
      "Snedecor, G.W. and W.G. Cochran. (1980). Statistical Methods 7th ed. Ames,\n",
      "IA: The Iowa State University Press.\n",
      "\n",
      "Snijders, T. and R. Bosker. (1999). Multilevel Analysis: An Introduction to\n",
      "Basic and Advanced Multilevel Modeling. London: Sage.\n",
      "\n",
      "Song, X-Y. and S-Y. Lee. (2002). “Bayesian Estimation and Model Selection of\n",
      "Multivariate Linear Models with Polytomous Variables.” Multivariate\n",
      "Behavioral Research 37(4):453-477.\n",
      "\n",
      "Spiegelhalter, D.J., N.G. Best, W.R. Gilks, and H. Inskip. (1996). “Hepatitis\n",
      "B: A Case Study in MCMC Methods.” Pp. 21-43 in W.R. Gilks, S.\n",
      "Richardson, and D.J. Spiegelhalter (eds.). Markov Chain Monte Carlo\n",
      "in Practice. Boca Raton, FL: Chapman and Hall/CRC.\n",
      "\n",
      "Thompson, S.P. (1946). Calculus Made Easy. 3rd ed. Hong Kong: MacMillan\n",
      "Press Ltd.\n",
      "\n",
      "Tierney, L. 1996. “Introduction to General State-Space Markov Chain The-\n",
      "ory.” Pp. 59-74 in W.R. Gilks, S. Richardson, and D.J. Spiegelhalter\n",
      "(eds.). Markov Chain Monte Carlo in Practice. Boca Raton, FL: Chap-\n",
      "man and Hall/CRC.\n",
      "\n",
      "Venables, W.N. and B.D. Ripley. (1999). Modern Applied Statistics with S-\n",
      "Plus. 3rd ed. New York: Springer-Verlag.\n",
      "\n",
      "Venables, W.N. and B.D. Ripley. (2000). S Programming. New York: Springer-\n",
      "Verlag.\n",
      "\n",
      "Willett, J.B. and A.G. Sayer. (1994). “Using Covariance Structure Analysis\n",
      "to Detect Correlates and Predictors of Individual Change Over Time.”\n",
      "Psychological Bulletin 16:363-381.\n",
      "\n",
      "Yamaguchi, K. (1991). Even History Analysis.. Newbury Park, CA: Sage.\n",
      "Zellner, A. (1962). “An Efficient Method of Estimating Seemingly Unrelated\n",
      "\n",
      "Regressions and Tests for Aggregation Bias.” Journal of the American\n",
      "Statistical Association 57(298):348-368.\n",
      "\n",
      "Zhang, T. (1997). Sams Teach Yourself C in 24 Hours. Indianapolis, IN: Sams\n",
      "Publishing.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Index\n",
      "\n",
      "acceptance rate, MH, 141–146\n",
      "autocorrelation function, 147\n",
      "\n",
      "batch means, 146\n",
      "Bayes factors, 159–161\n",
      "Bayes’ Theorem, 47–48, 50–51\n",
      "Bayesian p-value, 155\n",
      "Bayesian model averaging, 161\n",
      "Bayesian shrinkage, 57\n",
      "Bayesian updating, 49–50\n",
      "Bayesian vs. classical statistics, 1\n",
      "Bernoulli distribution, see probability\n",
      "\n",
      "distributions\n",
      "beta distribution, see probability\n",
      "\n",
      "distributions\n",
      "beta prior, 54–57\n",
      "binomial distribution, see probability\n",
      "\n",
      "distributions\n",
      "normal approximation to, 26\n",
      "\n",
      "black-white crossover, 206, 208,\n",
      "213–217, 227\n",
      "\n",
      "borrowing strength, 239\n",
      "burn-in, 91, 139\n",
      "Bush, defined as failure, 36\n",
      "\n",
      "candidate, 109\n",
      "Central Limit Theorem, 43, 337–338\n",
      "\n",
      "MCMC sampling error, 150\n",
      "change score, 256\n",
      "chi-square distribution, see probability\n",
      "\n",
      "distributions\n",
      "Cholesky decomposition, 127\n",
      "classical statistics, 9, 33–44\n",
      "\n",
      "classical vs. Bayesian statistics, 1, 50,\n",
      "51\n",
      "\n",
      "CNN/USA Today/Gallup polls, 33, 34,\n",
      "56\n",
      "\n",
      "completing the square, 64, 67\n",
      "composition method\n",
      "\n",
      "normal distribution model, 68\n",
      "OLS regression, 172\n",
      "\n",
      "conditional distribution, 23–25\n",
      "conditional probability, 11\n",
      "confidence interval, 342\n",
      "\n",
      "election polling example, 41\n",
      "conjugate prior, 57, 60\n",
      "continuous distribution, 12\n",
      "convergence, MCMC, 132–135\n",
      "Cowles’ algorithm, 289–294\n",
      "credible interval, 58\n",
      "cumulative distribution function, 13\n",
      "\n",
      "as GLM link function, 194\n",
      "\n",
      "data augmentation, 198\n",
      "deduction vs. induction, 1\n",
      "density, see probability distributions\n",
      "Dirichlet distribution, see probability\n",
      "\n",
      "distributions\n",
      "discrete distribution, 12\n",
      "discrete time probit model, 206\n",
      "distribution, see probability distribu-\n",
      "\n",
      "tions\n",
      "distribution function, 13\n",
      "\n",
      "education and health, 257\n",
      "educational attainment, U.S., 102\n",
      "empirical interval, 103\n",
      "\n",
      "\n",
      "\n",
      "356 Index\n",
      "\n",
      "envelope function, 84\n",
      "exchangeability, 36\n",
      "expectation, 17\n",
      "exponential distribution, see probability\n",
      "\n",
      "distributions\n",
      "\n",
      "Fisher’s z transformation (of correla-\n",
      "tion), 121\n",
      "\n",
      "fixed effects, 264\n",
      "full information maximum likelihood\n",
      "\n",
      "(FIML), 186\n",
      "\n",
      "gamma distribution, see probability\n",
      "distributions\n",
      "\n",
      "gamma function, 54\n",
      "gamma prior, 60–62\n",
      "Gelman-Rubin convergence diagnostic,\n",
      "\n",
      "151\n",
      "General Social Survey data\n",
      "\n",
      "free speech, 15–16\n",
      "free speech/political participation,\n",
      "\n",
      "20–21\n",
      "niceness example, 174–178, 276–277\n",
      "political affiliation and orientation\n",
      "\n",
      "example, 278–283, 300–303\n",
      "generalized linear models, defined, 193\n",
      "Gibbs sampling\n",
      "\n",
      "defined, 88\n",
      "rationale, 93–95\n",
      "relationship with MH, 113–115\n",
      "\n",
      "Gibbs within Metropolis, 114\n",
      "growth model, 257–264\n",
      "\n",
      "Healthy Life Expectancy, 304, 314–315\n",
      "Hessian matrix, 40\n",
      "hierarchical model, purposes of, 231\n",
      "hyperparameter, 232, 234\n",
      "hyperprior, 234, 236, 267\n",
      "\n",
      "identification\n",
      "in multivariate probit model, 278,\n",
      "\n",
      "295, 309\n",
      "ignorability, 183\n",
      "importance ratio, 112\n",
      "improper density, 13\n",
      "imputation, 186\n",
      "independence sampler, 111\n",
      "induction vs. deduction, 1\n",
      "information matrix, 40\n",
      "\n",
      "Internet use example, 251–257\n",
      "inverse chi-square distribution, see\n",
      "\n",
      "probability distributions\n",
      "inverse gamma distribution, see\n",
      "\n",
      "probability distributions\n",
      "inverse probability, 71\n",
      "inverse Wishart distribution, see\n",
      "\n",
      "probability distributions\n",
      "inversion sampling, 81–83\n",
      "\n",
      "limitations, 83\n",
      "item nonresponse, 184\n",
      "iteration, defined, 89\n",
      "\n",
      "Jacobian, 66, 134\n",
      "Jeffreys prior (normal model), 118\n",
      "joint distribution, 19\n",
      "joint probability, 10\n",
      "\n",
      "kernel (of a density), 29\n",
      "Kronecker product, 274\n",
      "\n",
      "latent data augmentation, 198\n",
      "latent growth model, see growth model\n",
      "latent propensities, 209–210\n",
      "latent residuals, 210\n",
      "latent traits, see latent propensities\n",
      "latent variable, 196\n",
      "life tables, 303\n",
      "likelihood function, 36–37\n",
      "\n",
      "Bernoulli, 36\n",
      "binomial, 36\n",
      "multinomial (multivariate probit\n",
      "\n",
      "model), 279\n",
      "multinomial (ordinal probit model),\n",
      "\n",
      "218\n",
      "normal, 63\n",
      "Poisson, 60\n",
      "\n",
      "linear distribution, see probability\n",
      "distributions\n",
      "\n",
      "link function, 194\n",
      "log odds, 197\n",
      "log posterior, use of in MH, 119, 125\n",
      "log-likelihood function, 38\n",
      "logistic regression, 197\n",
      "\n",
      "marginal distribution, 23–25\n",
      "marginal likelihood, 51, 161\n",
      "Markov chain Monte Carlo, defined, 88\n",
      "Markov property, 146\n",
      "\n",
      "\n",
      "\n",
      "Index 357\n",
      "\n",
      "mass function, see probability\n",
      "distributions\n",
      "\n",
      "mathematical notation, 6\n",
      "\n",
      "maximum a posteriori (MAP) estimate,\n",
      "72\n",
      "\n",
      "maximum likelihood estimate, 38–39\n",
      "\n",
      "normal distribution, 42\n",
      "\n",
      "mean, formal definition, 17\n",
      "\n",
      "median\n",
      "\n",
      "formal definition, 18\n",
      "\n",
      "Metropolis algorithm, 112\n",
      "\n",
      "Metropolis within Gibbs, 114\n",
      "\n",
      "Metropolis-Hastings algorithm\n",
      "\n",
      "defined, 108–113\n",
      "\n",
      "relationship with Gibbs, 113–115\n",
      "\n",
      "missing at random (MAR), 182\n",
      "\n",
      "missing completely at random (MCAR),\n",
      "182\n",
      "\n",
      "missing data, 182–191\n",
      "\n",
      "mixed models, 268\n",
      "\n",
      "mixing, MCMC, 132–135\n",
      "\n",
      "model averaging, 161\n",
      "\n",
      "model selection (via Bayes factors), 159\n",
      "\n",
      "multilevel model, 240\n",
      "\n",
      "multinomial distribution, see probabil-\n",
      "ity distributions\n",
      "\n",
      "multinomial likelihood function\n",
      "\n",
      "multivariate probit model, 279\n",
      "\n",
      "ordinal probit model, 218\n",
      "\n",
      "multiple imputation, 186\n",
      "\n",
      "multistate life table example, 303–315\n",
      "\n",
      "multivariate t distribution, see\n",
      "probability distributions\n",
      "\n",
      "multivariate model, defined, 271\n",
      "\n",
      "multivariate normal distribution, see\n",
      "probability distributions\n",
      "\n",
      "multivariate normal integration, 281\n",
      "\n",
      "nonignorability, 183\n",
      "\n",
      "noninformative prior, 55, 72\n",
      "\n",
      "normal distribution, see probability\n",
      "distributions\n",
      "\n",
      "normal likelihood example, 41–44,\n",
      "62–68\n",
      "\n",
      "normal prior, 62–68\n",
      "\n",
      "normalizing constant, 13, 51–53\n",
      "\n",
      "notation, see mathematical notation\n",
      "\n",
      "nuissance parameter, 121\n",
      "\n",
      "observed at random (OAR), 182\n",
      "odds ratio, 197\n",
      "ordinal probit likelihood, 219\n",
      "\n",
      "p-value, 340\n",
      "parameters vs. variables in distributions,\n",
      "\n",
      "68\n",
      "path analysis, 277\n",
      "planar distribution, see probability\n",
      "\n",
      "distributions\n",
      "Poisson distribution, see probability\n",
      "\n",
      "distributions\n",
      "Poisson likelihood function, 60\n",
      "polychoric correlation, 279\n",
      "posterior, 48–51\n",
      "posterior predictive distribution,\n",
      "\n",
      "155–156\n",
      "OLS regression example, 179–182\n",
      "\n",
      "precision parameter, 69, 247\n",
      "pregnancy example\n",
      "\n",
      "Bayes’ rule for point estimates, 47–49\n",
      "presidential election example, 33–34,\n",
      "\n",
      "41, 53–62, 233–240\n",
      "presidential poll data, 33–34, 56\n",
      "prior, x, 48–51\n",
      "\n",
      "dichotomous probit, 197\n",
      "nonformative, see noninformative\n",
      "\n",
      "prior\n",
      "probability density function, 12\n",
      "probability distributions\n",
      "\n",
      "t, 33\n",
      "Bernoulli, 26\n",
      "beta, 54–55\n",
      "binomial, 25–26\n",
      "bivariate normal, 30, 96–103\n",
      "chi-square, 69–70\n",
      "Dirichlet, 69\n",
      "exponential, 52–53, 69–70\n",
      "gamma, 69–70\n",
      "in general, 12–17\n",
      "inverse chi-square, 69–70\n",
      "inverse gamma, 69–70\n",
      "inverse Wishart, 70, 127, 295–296\n",
      "linear, 14–18, 115\n",
      "multinomial, 27–28\n",
      "multivariate, 19–22\n",
      "multivariate t, 33\n",
      "multivariate normal, 30–33\n",
      "normal, 29–30\n",
      "\n",
      "\n",
      "\n",
      "358 Index\n",
      "\n",
      "planar, 19–20\n",
      "Poisson, 28–29\n",
      "standard normal, 30\n",
      "t, 33\n",
      "uniform, 13–14, 18\n",
      "Wishart, 70\n",
      "z, 30\n",
      "\n",
      "probability interval, 58\n",
      "probability mass function, 12\n",
      "probability rules, 9–12\n",
      "programming languages, 5–6\n",
      "\n",
      "R, 5\n",
      "WinBugs, 5, 246–247\n",
      "\n",
      "proper density, 13\n",
      "proportional odds model, 219\n",
      "proportionality, 51–53\n",
      "proposal density, 109–112\n",
      "pseudo-R2, 225, 226\n",
      "\n",
      "quantiles, 18\n",
      "median, 18\n",
      "\n",
      "R programs\n",
      "election data, hierarchical model, 237\n",
      "Gibbs sampler for OLS model, 171,\n",
      "\n",
      "173\n",
      "Gibbs sampler using inversion, planar\n",
      "\n",
      "density, 90\n",
      "Gibbs sampler using rejection, planar\n",
      "\n",
      "density, 95\n",
      "Gibbs sampler, bivariate normal\n",
      "\n",
      "density, 99\n",
      "Gibbs sampler, bivariate normal\n",
      "\n",
      "density, matrix approach, 127\n",
      "Gibbs sampler, dichotomous probit\n",
      "\n",
      "model, 199\n",
      "Gibbs sampler, mean and variance of\n",
      "\n",
      "education, 102, 103\n",
      "Gibbs sampler, ordinal probit model,\n",
      "\n",
      "222\n",
      "Gibbs sampler, simple random effects\n",
      "\n",
      "model, 245\n",
      "Gibbs/MH for BVN model, 124\n",
      "inversion sampling, linear density, 83\n",
      "MH algorithm for OLS model, 168\n",
      "MH for ρ in BVN density, 119\n",
      "MH for linear density, 115\n",
      "MH steps for correlation matrix, 296\n",
      "multistate life tables, 312\n",
      "\n",
      "multivariate probit model, health and\n",
      "mortality, 307\n",
      "\n",
      "multivariate probit model, ppd\n",
      "simulation, 302\n",
      "\n",
      "multivariate regression model, 275\n",
      "political affiliation/orientation,\n",
      "\n",
      "multivariate probit model, 297\n",
      "rejection sampling, linear density, 85\n",
      "simulation from multivariate normal,\n",
      "\n",
      "284\n",
      "truncated multivariate normal\n",
      "\n",
      "simulation, 285\n",
      "race and health example, 223–228\n",
      "race and mortality example, 206–217\n",
      "random coefficient model, 251\n",
      "random effects, 264\n",
      "random intercept model, 241–242,\n",
      "\n",
      "248–249\n",
      "random variable, 10\n",
      "random walk Metropolis, 109\n",
      "reference prior, x\n",
      "regression modeling, briefly defined, 165\n",
      "rejection sampling, 84–86\n",
      "\n",
      "limitations, 86\n",
      "relative frequency, 13, 16\n",
      "relative risk, 197\n",
      "reparameterization, 134\n",
      "\n",
      "vs. changing proposal, 134\n",
      "\n",
      "sample space, defined, 9\n",
      "sampling distribution, 337\n",
      "sampling methods, logic of, 78\n",
      "scale reduction factor (R̂), 147–153\n",
      "\n",
      "OLS regression example, 175\n",
      "seemingly-unrelated regression, 272\n",
      "simultaneous equation models, 277\n",
      "stacked regression, 273\n",
      "standard errors, 39–41, 338–340\n",
      "state space, 304\n",
      "statistics, process of, 1\n",
      "structural equation models, 277\n",
      "subjectivity, criticism and response, 71\n",
      "support (of distribution), 19\n",
      "symbols, 6\n",
      "symmetry (in proposal density), 109\n",
      "\n",
      "t distribution, see probability distribu-\n",
      "tions\n",
      "\n",
      "t-test, 41\n",
      "\n",
      "\n",
      "\n",
      "Index 359\n",
      "\n",
      "target distribution, 91\n",
      "thinning the chain, 146\n",
      "threshold, 219\n",
      "trace (tr) of a matrix, 332\n",
      "trace plot, 91, 135\n",
      "transformation of variables, see\n",
      "\n",
      "reparameterization\n",
      "transition probability, 311\n",
      "transpose of a matrix, 331\n",
      "truncated normal distribution\n",
      "\n",
      "multivariate simulation, 283–289\n",
      "univariate simulation, 200–206, 221\n",
      "\n",
      "uniform distribution, see probability\n",
      "distributions\n",
      "\n",
      "update, defined, 89\n",
      "\n",
      "variable transformation, see reparame-\n",
      "terization\n",
      "\n",
      "variables vs. parameters in distributions,\n",
      "68\n",
      "\n",
      "variance components, 247\n",
      "variance, formal definition, 18\n",
      "vec operator, 273\n",
      "\n",
      "wages, 241\n",
      "WinBugs, see programming languages\n",
      "\n",
      "and Chapter 9\n",
      "WinBugs programs\n",
      "\n",
      "growth curve model of health, 259\n",
      "Internet use, random coefficient\n",
      "\n",
      "model, 253, 255\n",
      "Internet use, random intercept model,\n",
      "\n",
      "248, 250\n",
      "simple random effects model, 246\n",
      "\n",
      "Wishart distribution, see probability\n",
      "distributions\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalize(s):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"\\n\", \" \")\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        s = s.replace(a, b).replace(a.upper(), b.upper())\n",
    "    return s\n",
    "\n",
    "def convert_pdf_to_text(doc_pdf,id):\n",
    "    from tika import parser\n",
    "    file = r'Introduction to Applied Bayesian Statistics - Scott M. Lynch.pdf'\n",
    "    file_data = parser.from_file(file)\n",
    "    text = file_data['content']\n",
    "    with open((\"archivos_txt/file-text-\" + str(id) + \".txt\"), \"w\") as txtfile:\n",
    "        print(\"String Variable: {}\".format(normalize(text)), file=txtfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '1. [Teoria] Introduccion.pdf',\n",
       " 'app.py',\n",
       " 'archivos_pdf',\n",
       " 'archivos_txt',\n",
       " 'pruebas.ipynb',\n",
       " 'static',\n",
       " 'templates',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Introducci(cid:243)n\n",
      "Dr Carlos L(cid:243)pez de Castilla VÆsquez\n",
      "UniversidadNacionalAgrariaLaMolina\n",
      "2022-1\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La estad(cid:237)stica Bayesiana le debe su nombre al trabajo pionero\n",
      "del reverendo Thomas Bayes titulado: An Essay towards solving\n",
      "a Problem in the Doctrine of Chances publicado en 1764.\n",
      "Elart(cid:237)culofueenviadoalaRealSociedaddeLondresporRichard\n",
      "Price en 1763 quiØn escribi(cid:243): Yo ahora le mando un ensayo\n",
      "que he encontrado entre los papeles de nuestro fallecido amigo\n",
      "Thomas Bayes y el cual, en mi opini(cid:243)n, tiene un gran mØrito y\n",
      "bien merece ser preservado ...\n",
      "Aunque la obra de Thomas Bayes data ya de mÆs de dos siglos\n",
      "la estad(cid:237)stica Bayesiana es relativamente nueva y actualmente\n",
      "ostenta un gran desarrollo aunque no ajena tambiØn a grandes\n",
      "controversias.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La Estad(cid:237)stica es el estudio de la incertidumbre. Una forma de\n",
      "lidiar con ella es pensar en las probabilidades.\n",
      "Algunosejemplos:(cid:190)cuÆleslaprobabilidaddelanzardosdadosy\n",
      "obtener como suma cuatro? (cid:190)cuÆl es la probabilidad que llueva\n",
      "maæana? (cid:190)cuÆl es la probabilidad que el universo siga expan-\n",
      "diØndose para siempre?\n",
      "Existen tres enfoques diferentes para de(cid:28)nir probabilidades.\n",
      "La de(cid:28)nici(cid:243)n clÆsica se basa en la suposici(cid:243)n de que cada resul-\n",
      "tado es igualmente probable.\n",
      "La de(cid:28)nici(cid:243)n frecuentista requiere que se tenga una secuencia\n",
      "hipotØtica de eventos in(cid:28)nitos.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La de(cid:28)nici(cid:243)n Bayesiana se basa en una perspectiva personal,\n",
      "donde la probabilidad es una medida del conocimiento que se\n",
      "tiene sobre un fen(cid:243)meno de interØs.\n",
      "Setratadeunenfoquesubjetivo,peroquepuedefuncionarbien\n",
      "en una base estad(cid:237)stica rigurosa, y que conduce a resultados\n",
      "mucho mÆs intuitivos en comparaci(cid:243)n al enfoque frecuentista.\n",
      "En el ejemplo de la expansi(cid:243)n del universo se podr(cid:237)a considerar\n",
      "la teor(cid:237)a del multiverso, donde existe un nœmero in(cid:28)nito de uni-\n",
      "versos paralelos y preguntarse quØ fracci(cid:243)n tiene universos que\n",
      "se expanden para siempre. Dependiendo de lo que se sepa sobre\n",
      "el universo, se podr(cid:237)an obtener diferentes respuestas.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Teorema de Bayes\n",
      "Sean A y B dos eventos con Pr(B) > 0, entonces:\n",
      "Pr(B|A)Pr(A)\n",
      "Pr(A|B) =\n",
      "Pr(B)\n",
      "Si se tiene k eventos A ,··· ,A , los cuales constituyen una\n",
      "1 k\n",
      "partici(cid:243)n del espacio muestral Ω:\n",
      "Pr(B|A )Pr(A ) Pr(B|A )Pr(A )\n",
      "i i i i\n",
      "Pr(A |B) = =\n",
      "i Pr(B) (cid:80)k Pr(B|A )Pr(A )\n",
      "j=1 j j\n",
      "donde Pr(B) se le conoce como probabilidad total.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Teorema de Bayes\n",
      "Ejemplo 1: Transmici(cid:243)n de seæales\n",
      "Un transmisor estÆ enviando un mensaje mediante un c(cid:243)digo\n",
      "binario con seæales de 0 y 1.\n",
      "Cada seæal transmitida debe pasar por dos relevadores antes de\n",
      "llegar (cid:28)nalmente al receptor.\n",
      "En cada relevador, la probabilidad de que la seæal enviada sea\n",
      "diferentedelaseæalrecibida(inversi(cid:243)ndeseæal)esde0,10.Los\n",
      "relevadores funcionan independientemente uno de otro.\n",
      "El 60% de todas las seæales enviadas desde el emisor son 1.\n",
      "Si un 1 es recibido por el receptor (cid:190)cuÆl es la probabilidad de\n",
      "que haya sido enviado realmente un 1?\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Elobjetivoenlaestad(cid:237)sticaBayesianaesrepresentaryactualizar\n",
      "la incertidumbre asociada a un parÆmetro θ.\n",
      "Un punto importante en la de(cid:28)nici(cid:243)n clÆsica de inferencia es\n",
      "que el parÆmetro θ es considerado como constante.\n",
      "La diferencia fundamental entre la teor(cid:237)a clÆsica y la Bayesiana\n",
      "estÆ en que θ es considerado una variable aleatoria.\n",
      "La incertidumbre inicial sobre θ se representa usando una distri-\n",
      "buci(cid:243)n inicial o distribuci(cid:243)n a priori.\n",
      "El proceso de actualizaci(cid:243)n de la distribuci(cid:243)n a priori se reali-\n",
      "za incorporando la informaci(cid:243)n contenida en los datos, lo que\n",
      "permite hallar la distribuci(cid:243)n posterior para θ.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "El teorema de Bayes, de(cid:28)nido anteriormente usando eventos,\n",
      "puede presentarse en tØrminos de funciones de probabilidad o\n",
      "densidad:\n",
      "f (y|θ)f (θ)\n",
      "f (θ|y) =\n",
      "f (y)\n",
      "La distribuci(cid:243)n marginal para y se puede obtener de la siguiente\n",
      "manera:\n",
      "(cid:40)(cid:80)\n",
      "f (y|θ)f (θ) siθesdiscreta\n",
      "f (y) = θ\n",
      "(cid:82)\n",
      "f (y|θ)f (θ)dθ siθescontinua\n",
      "θ\n",
      "y es llamada la distribuci(cid:243)n predictiva a priori.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "La distribuci(cid:243)n a priori representa lo que es conocido de θ antes\n",
      "de observar los datos y se denota por f (θ).\n",
      "La distribuci(cid:243)n posterior representa lo que se conoce de θ des-\n",
      "puØs de observar los datos y se denota por f (θ|y).\n",
      "La funci(cid:243)n f (y|θ) es la distribuci(cid:243)n muestral e incorpora la\n",
      "informaci(cid:243)n proporcionada por los datos.\n",
      "El nœcleo de la distribuci(cid:243)n posterior es:\n",
      "f (θ|y) ∝ f (y|θ)f (θ)\n",
      "En muchos casos la distribuci(cid:243)n muestral se reemplaza por la\n",
      "funci(cid:243)n de verosimilitud L(θ|y) en el teorema de Bayes.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Ejemplo 2: Modelo binomial\n",
      "Suponga que se tiene una muestra:\n",
      "y ,··· ,y ∼ B(θ)\n",
      "1 n\n",
      "y que a priori θ ∼ BE(α,β). Hallar la distribuci(cid:243)n posterior.\n",
      "Ejemplo 3: Modelo de Poisson\n",
      "Suponga que se tiene una muestra:\n",
      "y ,··· ,y ∼ P(λ)\n",
      "1 n\n",
      "y que a priori λ ∼ G(α,β). Hallar la distribuci(cid:243)n posterior.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Distribuci(cid:243)n predictiva posterior\n",
      "Luego de obtener la distribuci(cid:243)n posterior es posible realizar el\n",
      "proceso de predicci(cid:243)n sobre una cantidad no observable y˜ que\n",
      "se de(cid:28)ne a partir de la distribuci(cid:243)n muestral.\n",
      "Algunas opciones son:\n",
      "m\n",
      "y˜= y y˜= (cid:80)m y y˜= 1 (cid:88)y\n",
      "(cid:101)i i=1(cid:101)i m (cid:101)i\n",
      "i=1\n",
      "La distribuci(cid:243)n predictiva posterior:\n",
      "(cid:40)(cid:80)\n",
      "f (y˜|θ)f (θ|y) siθesdiscreta\n",
      "f (y˜|y) = θ\n",
      "(cid:82)\n",
      "f (y˜|θ)f (θ|y)dθ siθescontinua\n",
      "θ\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Distribuci(cid:243)n predictiva posterior\n",
      "Ejemplo 4: Distribuci(cid:243)n predictiva posterior modelo binomial\n",
      "Considere el modelo binomial del ejemplo 2. Hallar la distri-\n",
      "buci(cid:243)n predictiva posterior para y˜ = y , un nuevo ensayo de\n",
      "(cid:101)i\n",
      "Bernoulli.\n",
      "Ejemplo 5: Distribuci(cid:243)n predictiva posterior modelo de Poisson\n",
      "Considere el modelo de Poisson del ejemplo 3. Hallar la distri-\n",
      "buci(cid:243)n predictiva posterior para y˜ = (cid:80)m y , el nœmero total\n",
      "i=1(cid:101)i\n",
      "de eventos de interØs en una nueva muestra y ,··· ,y .\n",
      "(cid:101)1 (cid:101)m\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "\n",
      "Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Introducci(cid:243)n\n",
      "Dr Carlos L(cid:243)pez de Castilla VÆsquez\n",
      "UniversidadNacionalAgrariaLaMolina\n",
      "2022-1\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La estad(cid:237)stica Bayesiana le debe su nombre al trabajo pionero\n",
      "del reverendo Thomas Bayes titulado: An Essay towards solving\n",
      "a Problem in the Doctrine of Chances publicado en 1764.\n",
      "Elart(cid:237)culofueenviadoalaRealSociedaddeLondresporRichard\n",
      "Price en 1763 quiØn escribi(cid:243): Yo ahora le mando un ensayo\n",
      "que he encontrado entre los papeles de nuestro fallecido amigo\n",
      "Thomas Bayes y el cual, en mi opini(cid:243)n, tiene un gran mØrito y\n",
      "bien merece ser preservado ...\n",
      "Aunque la obra de Thomas Bayes data ya de mÆs de dos siglos\n",
      "la estad(cid:237)stica Bayesiana es relativamente nueva y actualmente\n",
      "ostenta un gran desarrollo aunque no ajena tambiØn a grandes\n",
      "controversias.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La Estad(cid:237)stica es el estudio de la incertidumbre. Una forma de\n",
      "lidiar con ella es pensar en las probabilidades.\n",
      "Algunosejemplos:(cid:190)cuÆleslaprobabilidaddelanzardosdadosy\n",
      "obtener como suma cuatro? (cid:190)cuÆl es la probabilidad que llueva\n",
      "maæana? (cid:190)cuÆl es la probabilidad que el universo siga expan-\n",
      "diØndose para siempre?\n",
      "Existen tres enfoques diferentes para de(cid:28)nir probabilidades.\n",
      "La de(cid:28)nici(cid:243)n clÆsica se basa en la suposici(cid:243)n de que cada resul-\n",
      "tado es igualmente probable.\n",
      "La de(cid:28)nici(cid:243)n frecuentista requiere que se tenga una secuencia\n",
      "hipotØtica de eventos in(cid:28)nitos.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Introducci(cid:243)n\n",
      "La de(cid:28)nici(cid:243)n Bayesiana se basa en una perspectiva personal,\n",
      "donde la probabilidad es una medida del conocimiento que se\n",
      "tiene sobre un fen(cid:243)meno de interØs.\n",
      "Setratadeunenfoquesubjetivo,peroquepuedefuncionarbien\n",
      "en una base estad(cid:237)stica rigurosa, y que conduce a resultados\n",
      "mucho mÆs intuitivos en comparaci(cid:243)n al enfoque frecuentista.\n",
      "En el ejemplo de la expansi(cid:243)n del universo se podr(cid:237)a considerar\n",
      "la teor(cid:237)a del multiverso, donde existe un nœmero in(cid:28)nito de uni-\n",
      "versos paralelos y preguntarse quØ fracci(cid:243)n tiene universos que\n",
      "se expanden para siempre. Dependiendo de lo que se sepa sobre\n",
      "el universo, se podr(cid:237)an obtener diferentes respuestas.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Teorema de Bayes\n",
      "Sean A y B dos eventos con Pr(B) > 0, entonces:\n",
      "Pr(B|A)Pr(A)\n",
      "Pr(A|B) =\n",
      "Pr(B)\n",
      "Si se tiene k eventos A ,··· ,A , los cuales constituyen una\n",
      "1 k\n",
      "partici(cid:243)n del espacio muestral Ω:\n",
      "Pr(B|A )Pr(A ) Pr(B|A )Pr(A )\n",
      "i i i i\n",
      "Pr(A |B) = =\n",
      "i Pr(B) (cid:80)k Pr(B|A )Pr(A )\n",
      "j=1 j j\n",
      "donde Pr(B) se le conoce como probabilidad total.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Introducci(cid:243)n\n",
      "Estad(cid:237)sticaBayesiana TeoremadeBayes\n",
      "Teorema de Bayes\n",
      "Ejemplo 1: Transmici(cid:243)n de seæales\n",
      "Un transmisor estÆ enviando un mensaje mediante un c(cid:243)digo\n",
      "binario con seæales de 0 y 1.\n",
      "Cada seæal transmitida debe pasar por dos relevadores antes de\n",
      "llegar (cid:28)nalmente al receptor.\n",
      "En cada relevador, la probabilidad de que la seæal enviada sea\n",
      "diferentedelaseæalrecibida(inversi(cid:243)ndeseæal)esde0,10.Los\n",
      "relevadores funcionan independientemente uno de otro.\n",
      "El 60% de todas las seæales enviadas desde el emisor son 1.\n",
      "Si un 1 es recibido por el receptor (cid:190)cuÆl es la probabilidad de\n",
      "que haya sido enviado realmente un 1?\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Elobjetivoenlaestad(cid:237)sticaBayesianaesrepresentaryactualizar\n",
      "la incertidumbre asociada a un parÆmetro θ.\n",
      "Un punto importante en la de(cid:28)nici(cid:243)n clÆsica de inferencia es\n",
      "que el parÆmetro θ es considerado como constante.\n",
      "La diferencia fundamental entre la teor(cid:237)a clÆsica y la Bayesiana\n",
      "estÆ en que θ es considerado una variable aleatoria.\n",
      "La incertidumbre inicial sobre θ se representa usando una distri-\n",
      "buci(cid:243)n inicial o distribuci(cid:243)n a priori.\n",
      "El proceso de actualizaci(cid:243)n de la distribuci(cid:243)n a priori se reali-\n",
      "za incorporando la informaci(cid:243)n contenida en los datos, lo que\n",
      "permite hallar la distribuci(cid:243)n posterior para θ.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "El teorema de Bayes, de(cid:28)nido anteriormente usando eventos,\n",
      "puede presentarse en tØrminos de funciones de probabilidad o\n",
      "densidad:\n",
      "f (y|θ)f (θ)\n",
      "f (θ|y) =\n",
      "f (y)\n",
      "La distribuci(cid:243)n marginal para y se puede obtener de la siguiente\n",
      "manera:\n",
      "(cid:40)(cid:80)\n",
      "f (y|θ)f (θ) siθesdiscreta\n",
      "f (y) = θ\n",
      "(cid:82)\n",
      "f (y|θ)f (θ)dθ siθescontinua\n",
      "θ\n",
      "y es llamada la distribuci(cid:243)n predictiva a priori.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "La distribuci(cid:243)n a priori representa lo que es conocido de θ antes\n",
      "de observar los datos y se denota por f (θ).\n",
      "La distribuci(cid:243)n posterior representa lo que se conoce de θ des-\n",
      "puØs de observar los datos y se denota por f (θ|y).\n",
      "La funci(cid:243)n f (y|θ) es la distribuci(cid:243)n muestral e incorpora la\n",
      "informaci(cid:243)n proporcionada por los datos.\n",
      "El nœcleo de la distribuci(cid:243)n posterior es:\n",
      "f (θ|y) ∝ f (y|θ)f (θ)\n",
      "En muchos casos la distribuci(cid:243)n muestral se reemplaza por la\n",
      "funci(cid:243)n de verosimilitud L(θ|y) en el teorema de Bayes.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Estad(cid:237)stica Bayesiana\n",
      "Ejemplo 2: Modelo binomial\n",
      "Suponga que se tiene una muestra:\n",
      "y ,··· ,y ∼ B(θ)\n",
      "1 n\n",
      "y que a priori θ ∼ BE(α,β). Hallar la distribuci(cid:243)n posterior.\n",
      "Ejemplo 3: Modelo de Poisson\n",
      "Suponga que se tiene una muestra:\n",
      "y ,··· ,y ∼ P(λ)\n",
      "1 n\n",
      "y que a priori λ ∼ G(α,β). Hallar la distribuci(cid:243)n posterior.\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Distribuci(cid:243)n predictiva posterior\n",
      "Luego de obtener la distribuci(cid:243)n posterior es posible realizar el\n",
      "proceso de predicci(cid:243)n sobre una cantidad no observable y˜ que\n",
      "se de(cid:28)ne a partir de la distribuci(cid:243)n muestral.\n",
      "Algunas opciones son:\n",
      "m\n",
      "y˜= y y˜= (cid:80)m y y˜= 1 (cid:88)y\n",
      "(cid:101)i i=1(cid:101)i m (cid:101)i\n",
      "i=1\n",
      "La distribuci(cid:243)n predictiva posterior:\n",
      "(cid:40)(cid:80)\n",
      "f (y˜|θ)f (θ|y) siθesdiscreta\n",
      "f (y˜|y) = θ\n",
      "(cid:82)\n",
      "f (y˜|θ)f (θ|y)dθ siθescontinua\n",
      "θ\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n",
      "Introducci(cid:243)n Estad(cid:237)sticaBayesiana\n",
      "Estad(cid:237)sticaBayesiana Distribuci(cid:243)npredictivaposterior\n",
      "Distribuci(cid:243)n predictiva posterior\n",
      "Ejemplo 4: Distribuci(cid:243)n predictiva posterior modelo binomial\n",
      "Considere el modelo binomial del ejemplo 2. Hallar la distri-\n",
      "buci(cid:243)n predictiva posterior para y˜ = y , un nuevo ensayo de\n",
      "(cid:101)i\n",
      "Bernoulli.\n",
      "Ejemplo 5: Distribuci(cid:243)n predictiva posterior modelo de Poisson\n",
      "Considere el modelo de Poisson del ejemplo 3. Hallar la distri-\n",
      "buci(cid:243)n predictiva posterior para y˜ = (cid:80)m y , el nœmero total\n",
      "i=1(cid:101)i\n",
      "de eventos de interØs en una nueva muestra y ,··· ,y .\n",
      "(cid:101)1 (cid:101)m\n",
      "DrCarlosL(cid:243)pezdeCastillaVÆsquez Estad(cid:237)sticaBayesiana\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir():\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.pdf'):\n",
    "        all_text = '' # new line\n",
    "        with pdfplumber.open(file) as pdf:\n",
    "            # page = pdf.pages[0] - comment out or remove line\n",
    "            # text = page.extract_text() - comment out or remove line\n",
    "            for pdf_page in pdf.pages:\n",
    "               single_page_text = pdf_page.extract_text()\n",
    "               print( single_page_text )\n",
    "               # separate each page's text with newline\n",
    "               all_text = all_text + '\\n' + single_page_text\n",
    "            print(all_text)\n",
    "            # print(text) - comment out or remove line "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178b3e80c9db8f54e2acb31d91e54199a8837be0fdaa35cf7429bca94acf3364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
